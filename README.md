# ğŸ•¸ï¸ PolyGraph: Autonomous System for Trace Analysis
*(Formerly named as ASTRA in the sourse code)*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Geometric-red.svg)](https://pytorch-geometric.readthedocs.io/)
[![Framework](https://img.shields.io/badge/Framework-PolyGraph-blueviolet)](https://github.com/your-repo)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> ğŸ’¡ **Naming Convention Note**
>     In case of ambiguity
> ğŸ“Œ **PolyGraph** is the name of our method as presented in the academic paper (previously named **ASTRA** in the code).
>
> ğŸ“Œ **PolyGen** is our data generation engine (referenced in the codebase as **ASTRA-Gen 3.0**).
>
> âš ï¸ *Please note: While the conceptual descriptions below use the new terminology (**PolyGraph/PolyGen**), the source code, file structures, and command lines retain the original `astra` namespace to ensure reproducibility.*

---

## ğŸ“– Abstract

**PolyGraph** is a cutting-edge, end-to-end framework for multi-agent system fault attribution ğŸ•µï¸â€â™‚ï¸. It seamlessly combines **Graph Neural Networks (GNN)** ğŸ•¸ï¸ and **Large Language Models (LLM)** ğŸ¤– to achieve surgical precision in fault localization within complex multi-agent environments.

The system employs a smart **Coarse-to-Fine** ğŸ“‰ two-stage strategy:
1.  ğŸš€ **Stage 1:** Using GNN to rapidly identify top-K candidate agents.
2.  ğŸ”¬ **Stage 2:** Using LLM for fine-grained, reasoning-based analysis.

## âœ¨ Key Features

| Feature | Description |
| :--- | :--- |
| ğŸ² **Dynamic Causal Simulation** | **PolyGen** (ASTRA-Gen 3.0) generates highly realistic multi-agent interaction traces. |
| ğŸ•¸ï¸ **DHCG** | **Dynamic Heterogeneous Causal Graph** captures intricate temporal â³ and causal ğŸ”— relationships. |
| ğŸ§  **PolyGraph-MoE Model** | STGAT-based GNN equipped with a **Mixture of Experts** for robust coarse-grained fault attribution. |
| ğŸ”§ **LLM Fine-tuning** | Specialized **Qwen 8B** model fine-tuned for deep-dive fault analysis. |
| ğŸ¯ **Coarse-to-Fine Eval** | A sophisticated two-stage evaluation system delivering high accuracy. |

## ğŸ“‚ Project Structure

The codebase retains the `astra` package structure as follows:

```
ASTRA_Release/
â”œâ”€â”€ astra/                      # ğŸ“¦ Main source code package
â”‚   â”œâ”€â”€ generation/             # ğŸ­ Stage 1: Data generation
â”‚   â”‚   â””â”€â”€ generator.py        # PolyGen (ASTRA-Gen 3.0) generator
â”‚   â”œâ”€â”€ parsing/                # ğŸ§© Stage 1: Graph parsing
â”‚   â”‚   â””â”€â”€ dhcg_parser/        # DHCG parser implementation
â”‚   â”œâ”€â”€ data/                   # ğŸ”„ Stage 2: Data adapter
â”‚   â”‚   â”œâ”€â”€ adapter.py          # GraphDataConverter
â”‚   â”‚   â””â”€â”€ graph_data.py       # HeteroGraph data structure
â”‚   â”œâ”€â”€ model/                  # ğŸ§  Stage 3: Model architecture
â”‚   â”‚   â”œâ”€â”€ gnn.py              # PolyGraph-MoE (ASTRA-MoE) model
â”‚   â”‚   â”œâ”€â”€ stgat.py            # STGAT implementation
â”‚   â”‚   â””â”€â”€ loss.py             # Loss functions
â”‚   â”œâ”€â”€ training/               # ğŸ‹ï¸ Stage 3 & 4: Training scripts
â”‚   â”‚   â”œâ”€â”€ train_gnn.py        # GNN training script
â”‚   â”‚   â””â”€â”€ prep_llm_data.py    # LLM data preparation
â”‚   â””â”€â”€ evaluation/             # ğŸ“Š Stage 5: Evaluation
â”‚       â”œâ”€â”€ eval_pipeline.py    # Coarse-to-fine evaluation
â”‚       â””â”€â”€ eval_benchmark.py   # Benchmark evaluation
â”œâ”€â”€ scripts/                    # ğŸ› ï¸ Utility scripts
â”‚   â”œâ”€â”€ parse_dataset.py        # Dataset parsing
â”‚   â””â”€â”€ preprocess_external.py  # External dataset preprocessing
â”œâ”€â”€ examples/                   # ğŸ“ Sample data
â”‚   â”œâ”€â”€ golden_sample.json      # âœ… Golden trace example
â”‚   â”œâ”€â”€ fatal_sample.json       # âŒ Fatal trace example
â”‚   â””â”€â”€ healed_sample.json      # ğŸ’Š Healed trace example
â”œâ”€â”€ requirements.txt            # ğŸ“‹ Python dependencies
â””â”€â”€ README.md                   # ğŸ“„ This file
```

## âš™ï¸ Installation

### Prerequisites

*   ğŸ Python >= 3.8
*   ğŸ® CUDA >= 11.8 (for GPU acceleration)
*   ğŸ§  16GB+ RAM recommended
*   ğŸ’¾ 20GB+ disk space for models and data

### Setup Steps

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd ASTRA_Release
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Download pre-trained models (optional):**
    *   ğŸ“¥ **Qwen 8B base model:** Download from HuggingFace or ModelScope.
    *   ğŸ“ Place models in the `models/` directory.

## ğŸš€ Usage

### ğŸ—ï¸ Stage 1: Data Generation (PolyGen) and Graph Parsing

#### Generate PolyGen Dataset
Run the **PolyGen** (ASTRA-Gen 3.0) engine to create synthetic tasks:

```bash
python -m astra.generation.generator \
    --num_tasks 100 \
    --output_dir outputs/astra_v3 \
    --api_base <your-llm-api-endpoint>
```

#### Parse Traces to Graphs
Convert the raw logs into graph structures:

```bash
python scripts/parse_dataset.py \
    --input_dir outputs/astra_v3 \
    --output_dir processed_graphs/astra_v3
```

### ğŸ”„ Stage 2: Data Conversion

The graph data is **automatically converted** during the training phase. ğŸ§™â€â™‚ï¸
The `GraphDataConverter` handles:
*   ğŸ”¹ Node feature extraction and encoding
*   ğŸ”¹ Edge feature extraction
*   ğŸ”¹ HeteroGraph sequence construction

### ğŸ§  Stage 3: GNN Training (PolyGraph-MoE)

Train the coarse-grained expert model:

```bash
python -m astra.training.train_gnn \
    --data_dir processed_graphs/astra_v3 \
    --output_dir checkpoints/astra_moe \
    --epochs 50 \
    --batch_size 8 \
    --device cuda
```

### ğŸ“ Stage 4: LLM Data Preparation and Fine-tuning

#### Prepare LLM Training Data
Filter data using the GNN checkpoint to create focused samples for the LLM:

```bash
python -m astra.training.prep_llm_data \
    --graph_dir processed_graphs/astra_v3 \
    --gnn_checkpoint checkpoints/astra_moe/best_model.pt \
    --output_dir training_data/llm \
    --top_k 4
```

#### Fine-tune LLM
Use your preferred fine-tuning framework (e.g., PEFT) to train the **PolyGraph** reasoning module:

```bash
# Use your preferred LLM fine-tuning framework
# Example with PEFT:
python -m astra.training.finetune_llm \
    --base_model Qwen/Qwen3-8B \
    --data_dir training_data/llm \
    --output_dir adapters/qwen8b_astra
```

### ğŸ“Š Stage 5: Evaluation

#### Coarse-to-Fine Evaluation
Run the full **PolyGraph** pipeline:

```bash
python -m astra.evaluation.eval_pipeline \
    --test_data_dir processed_graphs/test \
    --gnn_checkpoint checkpoints/astra_moe/best_model.pt \
    --llm_adapter adapters/qwen8b_astra \
    --base_model_name Qwen/Qwen3-8B \
    --top_k 4 \
    --device cuda
```

#### Benchmark Evaluation (TracerTraj)

```bash
python -m astra.evaluation.eval_benchmark \
    --test_data_dir processed_graphs/tracertraj \
    --gnn_checkpoint checkpoints/astra_moe/best_model.pt \
    --llm_adapter adapters/qwen8b_astra \
    --base_model_name Qwen/Qwen3-8B \
    --device cuda
```

## ğŸ“ Example Data

The `examples/` directory contains sample data files for quick testing:
*   âœ… `sample_golden.json`: A successful multi-agent interaction trace (no fault).
*   âŒ `sample_fatal.json`: A trace with injected fault.

You can use these to test the parsing and evaluation pipeline. See `examples/README.md` for more details.

## ğŸ§© Key Components

### PolyGraph-MoE Model (ASTRA-MoE)
The core GNN architecture consists of:
*   **MicroStateEncoder**: ğŸ“· Multi-modal node feature encoder.
*   **STGAT**: ğŸ•¸ï¸ Spatio-temporal graph attention network.
*   **TemporalReasoning**: â³ Causal temporal reasoning with RoPE.
*   **MoEHead**: ğŸš¦ Uncertainty-aware expert routing.

### DHCG Parser
The parser extracts the **Dynamic Heterogeneous Causal Graph**:
*   **Nodes** ğŸŸ£: Agents, Tools, Artifacts, Environment.
*   **Edges** â–: Invoke, Return, Reference, Communicate, Affect.
*   **Features** ğŸ“„: Text embeddings, metadata features.

### Coarse-to-Fine Strategy
1.  **Coarse Stage (GNN)** âš¡: Predicts top-K candidate agents.
2.  **Fine Stage (LLM)** ğŸ”: Analyzes candidate logs to identify exact fault agent and step.

## ğŸ“ˆ Performance

**PolyGraph** demonstrates state-of-the-art results:

*   ğŸ† **Agent Accuracy**: ~67.39% on Who&When benchmark, and 77.95% on TracerTraj-Code.
*   ğŸ¯ **Step Accuracy**: ~40.22% on Who&When benchmark, and 31.50% on TracerTraj-Code.
*   ğŸ“‰ **Token Efficiency**: Optimized prompt design significantly reduces LLM token usage.

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.
