"""
ASTRA-MoE æ··åˆæŸå¤±å‡½æ•°å®ç°

å®ç°è®ºæ–‡ Section 3.5 ä¸­å®šä¹‰çš„æ··åˆæŸå¤±å‡½æ•°ï¼š
1. Agent å½’å› æŸå¤± (L_focal) - ä½¿ç”¨ Focal Loss å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
2. æ­¥éª¤é¢„æµ‹æŸå¤± (L_step) - æ—¶é—´æ­¥åˆ†ç±»æŸå¤±
3. ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤± (L_aux) - é˜²æ­¢ MoE åç¼©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple


class FocalLoss(nn.Module):
    """
    Focal Loss ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    
    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)
    
    å…¶ä¸­ï¼š
    - p_t æ˜¯é¢„æµ‹æ¦‚ç‡
    - alpha_t æ˜¯ç±»åˆ«æƒé‡
    - gamma æ˜¯èšç„¦å‚æ•°ï¼ˆgamma > 0 æ—¶ï¼Œéš¾åˆ†ç±»æ ·æœ¬æƒé‡æ›´å¤§ï¼‰
    """
    
    def __init__(self, 
                 alpha: float = 0.25,
                 gamma: float = 2.0,
                 reduction: str = 'mean'):
        """
        Args:
            alpha: ç±»åˆ«æƒé‡å¹³è¡¡å› å­
            gamma: èšç„¦å‚æ•°ï¼Œè¶Šå¤§è¶Šå…³æ³¨éš¾åˆ†ç±»æ ·æœ¬
            reduction: 'mean', 'sum', æˆ– 'none'
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, 
                inputs: torch.Tensor, 
                targets: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            inputs: é¢„æµ‹logits [B, T, N, 2] æˆ– [B, N, 2] æˆ– [B*T*N, 2]
            targets: çœŸå®æ ‡ç­¾ [B, T, N] æˆ– [B, N] æˆ– [B*T*N]ï¼Œå€¼ä¸º 0 æˆ– 1
            mask: æœ‰æ•ˆä½ç½®æ©ç  [B, T, N] æˆ– [B, N] æˆ– [B*T*N]ï¼ŒTrue è¡¨ç¤ºæœ‰æ•ˆä½ç½®
        
        Returns:
            æŸå¤±å€¼
        """
        # å±•å¹³å¤„ç†
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ reshape è€Œä¸æ˜¯ viewï¼Œå› ä¸º tensor å¯èƒ½ä¸è¿ç»­
        if inputs.dim() == 4:
            # [B, T, N, C] -> [B*T*N, C]
            B, T, N, C = inputs.shape
            inputs = inputs.reshape(-1, C)  # [B*T*N, 2]
            targets = targets.reshape(-1)  # [B*T*N]
            if mask is not None:
                mask = mask.reshape(-1)  # [B*T*N]
        elif inputs.dim() == 3:
            # [B, N, C] -> [B*N, C]
            B, N, C = inputs.shape
            inputs = inputs.reshape(-1, C)  # [B*N, 2]
            targets = targets.reshape(-1)  # [B*N]
            if mask is not None:
                mask = mask.reshape(-1)  # [B*N]
        
        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = F.cross_entropy(inputs, targets.long(), reduction='none')  # [B*T*N] æˆ– [B*N]
        
        # è®¡ç®—æ¦‚ç‡
        p = torch.exp(-ce_loss)  # p_t = exp(-ce_loss)
        
        # Focal Loss
        focal_loss = self.alpha * (1 - p) ** self.gamma * ce_loss
        
        # åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            focal_loss = focal_loss * mask.float()
            if self.reduction == 'mean':
                # åªå¯¹æœ‰æ•ˆä½ç½®æ±‚å¹³å‡
                return focal_loss.sum() / (mask.float().sum() + 1e-8)
            elif self.reduction == 'sum':
                return focal_loss.sum()
            else:
                return focal_loss
        else:
            if self.reduction == 'mean':
                return focal_loss.mean()
            elif self.reduction == 'sum':
                return focal_loss.sum()
            else:
                return focal_loss


class ASTRALoss(nn.Module):
    """
    ASTRA-MoE æ··åˆæŸå¤±å‡½æ•°
    
    æ€»æŸå¤± = w1 * L_agent + w2 * L_step + w3 * L_aux
    
    å…¶ä¸­ï¼š
    - L_agent: Agent å½’å› æŸå¤±ï¼ˆFocal Lossï¼‰
    - L_step: æ­¥éª¤é¢„æµ‹æŸå¤±ï¼ˆCrossEntropyï¼‰
    - L_aux: ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±
    """
    
    def __init__(self,
                 w_agent: float = 1.0,
                 w_step: float = 1.0,
                 w_aux: float = 0.01,
                 focal_alpha: float = 0.25,
                 focal_gamma: float = 2.0,
                 aux_alpha: float = 0.01,
                 mask_agent0: bool = True):
        """
        Args:
            w_agent: Agent å½’å› æŸå¤±æƒé‡
            w_step: æ­¥éª¤é¢„æµ‹æŸå¤±æƒé‡
            w_aux: ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
            focal_alpha: Focal Loss çš„ alpha å‚æ•°
            focal_gamma: Focal Loss çš„ gamma å‚æ•°
            aux_alpha: è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡ç³»æ•°
            mask_agent0: æ˜¯å¦åœ¨è®­ç»ƒæ—¶æŠ‘åˆ¶ Agent 0 çš„é¢„æµ‹ï¼ˆæ‰“ç ´æ¨¡å‹åç¼©ï¼‰
        """
        super().__init__()
        self.w_agent = w_agent
        self.w_step = w_step
        self.w_aux = w_aux
        self.aux_alpha = aux_alpha
        self.mask_agent0 = mask_agent0  # ğŸ”¥ æ–°å¢ï¼šå»åæœºåˆ¶
        
        # Agent å½’å› æŸå¤±ï¼ˆFocal Lossï¼‰
        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
        
        # æ­¥éª¤é¢„æµ‹æŸå¤±ï¼ˆCrossEntropyï¼‰
        self.step_loss_fn = nn.CrossEntropyLoss(reduction='mean')
    
    def compute_agent_loss(self,
                           logits: torch.Tensor,
                           y_agent: torch.Tensor,
                           agent_mask: torch.Tensor,
                           seq_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®— Agent å½’å› æŸå¤± (ä¿®å¤ç‰ˆï¼šåŸºäºåˆ†æ•°çš„æ’åºæŸå¤±)
        
        Args:
            logits: [B, T, N, 1] - æ¯ä¸ª Agent çš„æ•…éšœåˆ†æ•° (Logits)
            y_agent: [B, N] - çœŸå®æ ‡ç­¾ (One-hot), 1 è¡¨ç¤ºè¯¥ Agent æ˜¯æ•…éšœæº
            agent_mask: [B, N] - æœ‰æ•ˆ Agent æ©ç 
            seq_mask: [B, T] - åºåˆ—æ©ç ï¼Œç”¨äºæ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„å®é™…æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        """
        B, T, N, C = logits.shape
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¯ä¸ªæ ·æœ¬çš„å®é™…åºåˆ—é•¿åº¦æå– scores
        # é—®é¢˜ï¼šä¹‹å‰ç»Ÿä¸€å– logits[:, -1, :, :] ä¼šå¯¼è‡´çŸ­åºåˆ—æ ·æœ¬å–åˆ°paddingä½ç½®ï¼ˆå…¨é›¶ï¼‰
        # åæœï¼šæ¨¡å‹åœ¨è®­ç»ƒæ—¶æ”¶åˆ°é”™è¯¯çš„æ¢¯åº¦ä¿¡å·ï¼Œå¯¼è‡´æ— æ³•å­¦ä¹ ï¼Œå‡†ç¡®ç‡æ’å®šä¸”ä½
        if seq_mask is not None:
            scores = torch.zeros(B, N, device=logits.device, dtype=logits.dtype)
            for i in range(B):
                valid_steps = seq_mask[i].nonzero(as_tuple=True)[0]
                if valid_steps.numel() > 0:
                    last_step = valid_steps[-1].item()
                    scores[i] = logits[i, last_step, :, 0]
                else:
                    scores[i] = 0.0
        else:
            # Fallbackï¼šå¦‚æœæ²¡æœ‰æä¾› seq_maskï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ï¼ˆä½†ä¼šæœ‰bugï¼‰
            scores = logits[:, -1, :, :] # [B, N, 1]
            scores = scores.squeeze(-1)  # [B, N] - æ¯ä¸ª Agent çš„æ•…éšœå¾—åˆ†
        
        # 2. å¯¹é½ç»´åº¦ (å¤„ç† N ä¸ y_agent ä¸ä¸€è‡´çš„æƒ…å†µ)
        target_N = y_agent.shape[1]
        valid_N = min(N, target_N)
        
        scores = scores[:, :valid_N]      # [B, valid_N]
        targets = y_agent[:, :valid_N]    # [B, valid_N]
        mask = agent_mask[:, :valid_N]    # [B, valid_N]
        
        # 3. æ©ç å¤„ç†ï¼šå°†æ— æ•ˆ Agent çš„åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©· (é˜²æ­¢ softmax é€‰ä¸­)
        # æ³¨æ„ï¼štargets æ˜¯ float ç±»å‹ (0.0 æˆ– 1.0)ï¼Œéœ€è¦è½¬æ¢
        
        # 4. è®¡ç®— Loss
        # è¿™æ˜¯ä¸€ä¸ªå¤šåˆ†ç±»é—®é¢˜ï¼šåœ¨ valid_N ä¸ª Agent ä¸­é€‰å‡ºä¸€ä¸ª
        # æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ CrossEntropyï¼Œä½†éœ€è¦å°† One-hot target è½¬ä¸º Index
        
        # è¿‡æ»¤æ‰æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾çš„æ ·æœ¬ (é˜²æ­¢ NaN)
        has_label = targets.sum(dim=1) > 0
        valid_indices = torch.where(has_label)[0]
        
        if len(valid_indices) == 0:
            return scores.sum() * 0.0
            
        scores_valid = scores[valid_indices] # [B_valid, valid_N]
        targets_valid = targets[valid_indices] # [B_valid, valid_N]
        mask_valid = mask[valid_indices]     # [B_valid, valid_N]
        
        # åº”ç”¨æ©ç åˆ°åˆ†æ•° (æ— æ•ˆèŠ‚ç‚¹å¾—åˆ† -inf)
        scores_masked = scores_valid.clone()
        scores_masked[~mask_valid.bool()] = -1e9
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœå¼€å¯ mask_agent0ï¼Œåˆ™åœ¨è®­ç»ƒåˆæœŸæŠ‘åˆ¶ Agent 0 çš„ Logits
        # è¿™æ˜¯ä¸€ç§å¼ºåŠ›çš„"å»å"æ‰‹æ®µï¼Œæ‰“ç ´æ¨¡å‹åç¼©åˆ° Agent 0
        if self.mask_agent0 and self.training:
            # è·å–çœŸå®æ ‡ç­¾ç´¢å¼•
            target_indices = targets_valid.argmax(dim=1)  # [B_valid]
            is_not_agent0 = (target_indices != 0)
            
            # ä»…å¯¹çœŸå®æ ‡ç­¾ä¸æ˜¯ 0 çš„æ ·æœ¬ï¼ŒæŠ‘åˆ¶ Agent 0 çš„é¢„æµ‹
            # å°† Agent 0 çš„åˆ†æ•°å‡å»ä¸€ä¸ªå¤§å€¼ï¼Œä½¿å…¶ Softmax æ¦‚ç‡å˜å°
            if is_not_agent0.any():
                scores_masked[is_not_agent0, 0] -= 5.0  # æƒ©ç½š Agent 0
        
        # è·å–ç›®æ ‡ç´¢å¼•ï¼ˆå¦‚æœä¸Šé¢å·²ç»è®¡ç®—è¿‡ï¼Œè¿™é‡Œéœ€è¦é‡æ–°è®¡ç®—ï¼‰
        target_indices = targets_valid.argmax(dim=1)  # [B_valid]
        
        # æ ‡å‡† CrossEntropy
        loss = F.cross_entropy(scores_masked, target_indices)
        
        return loss
    
    def compute_step_loss(self,
                         step_logits: torch.Tensor,
                         y_step: torch.Tensor,
                         seq_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—æ­¥éª¤é¢„æµ‹æŸå¤±
        
        ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„ step_logits è®¡ç®— CrossEntropy Loss
        
        Args:
            step_logits: æ¨¡å‹è¾“å‡ºçš„æ­¥éª¤é¢„æµ‹ logits [B, T]
                       æ¯ä¸ªæ—¶é—´æ­¥çš„ logitï¼Œè¡¨ç¤ºè¯¥æ—¶é—´æ­¥æ˜¯æ•…éšœæ­¥çš„æ¦‚ç‡
                       æ— æ•ˆæ—¶é—´æ­¥çš„ logits åº”ä¸º -inf
            y_step: çœŸå®æ•…éšœæ—¶é—´æ­¥ [B]ï¼Œå€¼ä¸º 0 åˆ° T-1 ä¹‹é—´çš„æ•´æ•°ï¼Œ-1 è¡¨ç¤ºæ— æ•ˆ
            seq_mask: åºåˆ—æ©ç  [B, T]ï¼ŒTrue è¡¨ç¤ºæœ‰æ•ˆæ—¶é—´æ­¥
        
        Returns:
            æŸå¤±å€¼ï¼ˆå¦‚æœè®¡ç®—å¤±è´¥æˆ–æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè¿”å›0.0ï¼‰
        """
        B, T = step_logits.shape
        
        # ğŸ”¥ ä¿®æ­£ 3: ç®€åŒ–é€»è¾‘ - ä¿¡ä»» collate_fn çš„è¾“å‡ºï¼ˆæ— æ•ˆæ ‡ç­¾ç”¨ -1ï¼Œæ— æ•ˆ logits ç”¨ -infï¼‰
        # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾ï¼ˆy_step == -1ï¼‰
        valid_mask = (y_step >= 0) & (y_step < T)
        
        if not valid_mask.any():
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè¿”å›å¸¦æ¢¯åº¦çš„é›¶å¼ é‡
            zero_loss = step_logits.sum() * 0.0
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿”å›çš„é›¶å¼ é‡ä¸ä¼šäº§ç”Ÿ NaN
            if torch.isnan(zero_loss) or torch.isinf(zero_loss):
                return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
            return zero_loss
        
        # è·å–æœ‰æ•ˆçš„ logits å’Œæ ‡ç­¾
        valid_step_logits = step_logits[valid_mask]  # [valid_B, T]
        valid_y_step = y_step[valid_mask].long()  # [valid_B]
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯ valid_y_step ä¸­çš„æ‰€æœ‰å€¼éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if (valid_y_step < 0).any() or (valid_y_step >= T).any():
            # å¦‚æœå‘ç°è¶Šç•Œç´¢å¼•ï¼Œæˆªæ–­åˆ°æœ‰æ•ˆèŒƒå›´
            valid_y_step = torch.clamp(valid_y_step, min=0, max=T - 1)
        
        # å¦‚æœæä¾›äº† seq_maskï¼Œåªå¯¹æœ‰æ•ˆæ—¶é—´æ­¥è®¡ç®—æŸå¤±
        if seq_mask is not None:
            # ğŸ”¥ ä¿®æ­£ 3: ç®€åŒ–é€»è¾‘ - ä¿¡ä»» collate_fn çš„è¾“å‡º
            # ç¡®ä¿ seq_mask çš„å½¢çŠ¶ä¸ step_logits ä¸€è‡´
            if seq_mask.shape[1] != T:
                if seq_mask.shape[1] > T:
                    seq_mask = seq_mask[:, :T]
                else:
                    pad_size = T - seq_mask.shape[1]
                    seq_mask = F.pad(seq_mask, (0, pad_size), value=False)
            
            valid_seq_mask = seq_mask[valid_mask]  # [valid_B, T]
            # ç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾ä½ç½®åœ¨ seq_mask ä¸­ä¸º True
            batch_indices = torch.arange(valid_y_step.shape[0], device=valid_y_step.device)
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            safe_y_step = torch.clamp(valid_y_step, min=0, max=T - 1)
            valid_seq_mask[batch_indices, safe_y_step] = True
            
            # å°†æ— æ•ˆæ—¶é—´æ­¥çš„ logits è®¾ä¸º -inf
            masked_logits = torch.where(valid_seq_mask, valid_step_logits, 
                                       torch.tensor(float('-inf'), device=valid_step_logits.device, dtype=valid_step_logits.dtype))
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ masked_logits åœ¨æ ‡ç­¾ä½ç½®æ˜¯å¦å…¨éƒ¨ä¸º -inf
            # å¦‚æœæ‰€æœ‰æœ‰æ•ˆæ ·æœ¬åœ¨æ ‡ç­¾ä½ç½®çš„ logit éƒ½æ˜¯ -infï¼Œä¼šå¯¼è‡´ CrossEntropyLoss äº§ç”Ÿ NaN
            label_logits = masked_logits[batch_indices, safe_y_step]  # [valid_B]
            if (label_logits == float('-inf')).all():
                # æ‰€æœ‰æ ‡ç­¾ä½ç½®çš„ logits éƒ½æ˜¯ -infï¼Œæ— æ³•è®¡ç®—æŸå¤±ï¼Œè¿”å› 0
                print(f"[WARNING] All label logits are -inf in step_loss computation, returning 0.0")
                return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
            
            try:
                step_loss = self.step_loss_fn(masked_logits, safe_y_step)
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è®¡ç®—ç»“æœæ˜¯å¦ä¸º NaN æˆ– Inf
                if torch.isnan(step_loss) or torch.isinf(step_loss):
                    print(f"[WARNING] Step loss is NaN/Inf, returning 0.0. Check model outputs and labels.")
                    return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
            except Exception as e:
                print(f"[ERROR] Step loss computation failed: {e}, returning 0.0")
                return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
        else:
            # ç›´æ¥è®¡ç®— CrossEntropy Loss
            try:
                step_loss = self.step_loss_fn(valid_step_logits, valid_y_step)
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è®¡ç®—ç»“æœæ˜¯å¦ä¸º NaN æˆ– Inf
                if torch.isnan(step_loss) or torch.isinf(step_loss):
                    print(f"[WARNING] Step loss is NaN/Inf, returning 0.0. Check model outputs and labels.")
                    return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
            except Exception as e:
                print(f"[ERROR] Step loss computation failed: {e}, returning 0.0")
                return torch.tensor(0.0, device=step_logits.device, requires_grad=True)
        
        return step_loss
    
    def compute_aux_loss(self,
                        gate_weights: torch.Tensor,
                        load: torch.Tensor,
                        agent_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±
        
        å…¬å¼: L_aux = alpha * N * sum_i(f_i * P_i)
        
        å…¶ä¸­ï¼š
        - f_i æ˜¯ä¸“å®¶ i çš„è´Ÿè½½ï¼ˆloadï¼‰
        - P_i æ˜¯ä¸“å®¶ i çš„å¹³å‡é—¨æ§æƒé‡
        - N æ˜¯ä¸“å®¶æ•°é‡
        
        Args:
            gate_weights: é—¨æ§æƒé‡ [B, T, N, num_experts] æˆ– [B, T, N, num_experts]
            load: ä¸“å®¶è´Ÿè½½ [B, num_experts] æˆ– [num_experts]
            agent_mask: Agent æ©ç  [B, N]ï¼ŒTrue è¡¨ç¤ºæœ‰æ•ˆ Agent
        
        Returns:
            æŸå¤±å€¼
        """
        # å¤„ç† load çš„ç»´åº¦
        if load.dim() == 1:
            # [num_experts] -> [1, num_experts]
            load = load.unsqueeze(0)
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡é—¨æ§æƒé‡
        # gate_weights: [B, T, N, num_experts]
        if gate_weights.dim() == 4:
            B, T, N, num_experts = gate_weights.shape
            
            # åº”ç”¨ agent_maskï¼ˆå¦‚æœæœ‰ï¼‰
            if agent_mask is not None:
                # æ‰©å±• mask åˆ° [B, T, N, 1]
                agent_mask_expanded = agent_mask.unsqueeze(1).unsqueeze(-1).expand(B, T, N, 1)  # [B, T, N, 1]
                # åªå¯¹æœ‰æ•ˆ Agent æ±‚å¹³å‡
                masked_weights = gate_weights * agent_mask_expanded.float()
                # è®¡ç®—æœ‰æ•ˆ Agent æ•°é‡
                valid_agents = agent_mask_expanded.float().sum(dim=(1, 2), keepdim=True)  # [B, 1, 1, 1]
                # å°† valid_agents ä» [B, 1, 1, 1] è½¬æ¢ä¸º [B, 1] ä»¥åŒ¹é… [B, num_experts] çš„ç»´åº¦
                valid_agents_flat = valid_agents.squeeze(-1).squeeze(-1)  # [B, 1]
                P = masked_weights.sum(dim=(1, 2)) / (valid_agents_flat + 1e-8)  # [B, num_experts]
            else:
                P = gate_weights.mean(dim=(1, 2))  # [B, num_experts]
        else:
            # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œç›´æ¥å¹³å‡
            P = gate_weights.mean(dim=tuple(range(gate_weights.dim() - 1)))  # [..., num_experts]
        
        # æ‰©å±• load åˆ°åŒ¹é… P çš„ batch ç»´åº¦
        if load.shape[0] == 1 and P.shape[0] > 1:
            load = load.expand_as(P)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        # L_aux = alpha * N * sum_i(f_i * P_i)
        num_experts = load.shape[-1]
        aux_loss = self.aux_alpha * num_experts * (load * P).sum(dim=-1).mean()
        
        return aux_loss
    
    def forward(self,
                outputs: Dict[str, torch.Tensor],
                targets: Dict[str, torch.Tensor],
                masks: Optional[Dict[str, torch.Tensor]] = None) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±
        
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'logits': [B, T, N, 2] Agent æ•…éšœæ¦‚ç‡ logits
                - 'alpha': [B, T, N, num_classes] Dirichlet åˆ†å¸ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰
                - 'gate_weights': [B, T, N, num_experts] é—¨æ§æƒé‡
                - 'load': [B, num_experts] æˆ– [num_experts] ä¸“å®¶è´Ÿè½½
                - 'step_logits': [B, T] æ­¥éª¤é¢„æµ‹ logitsï¼ˆå¯é€‰ï¼‰
            targets: çœŸå®æ ‡ç­¾å­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'y_agent': [B, N] Agent æ•…éšœæ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰
                - 'y_step': [B] æ•…éšœæ—¶é—´æ­¥ï¼ˆ0 åˆ° T-1 çš„æ•´æ•°ï¼Œ-1 è¡¨ç¤ºæ— æ•ˆï¼‰
            masks: æ©ç å­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'agent_mask': [B, N] Agent æ©ç ï¼ŒTrue è¡¨ç¤ºæœ‰æ•ˆ Agent
                - 'seq_mask': [B, T] åºåˆ—æ©ç ï¼ŒTrue è¡¨ç¤ºæœ‰æ•ˆæ—¶é—´æ­¥
        
        Returns:
            æŸå¤±å­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'total_loss': æ€»æŸå¤±
                - 'agent_loss': Agent å½’å› æŸå¤±
                - 'step_loss': æ­¥éª¤é¢„æµ‹æŸå¤±
                - 'aux_loss': ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±
        """
        # æå–è¾“å‡º
        logits = outputs['logits']  # [B, T, N, 2]
        gate_weights = outputs['gate_weights']  # [B, T, N, num_experts]
        load = outputs['load']  # [B, num_experts] æˆ– [num_experts]
        
        # æå–æ ‡ç­¾
        y_agent = targets['y_agent']  # [B, N]
        y_step = targets['y_step']  # [B]
        
        # æå–æ©ç 
        agent_mask = masks.get('agent_mask', None) if masks else None
        seq_mask = masks.get('seq_mask', None) if masks else None
        
        # 1. Agent å½’å› æŸå¤±
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼ å…¥ seq_mask ä»¥æ­£ç¡®æå–æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥
        agent_loss = self.compute_agent_loss(logits, y_agent, agent_mask, seq_mask)
        
        # 2. æ­¥éª¤é¢„æµ‹æŸå¤±
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœç¼ºå°‘ step_logitsï¼Œç›´æ¥æŠ¥é”™è€Œä¸æ˜¯è¿”å› 0
        if 'step_logits' not in outputs:
            raise RuntimeError(
                "CRITICAL ERROR: 'step_logits' missing in loss input!\n"
                f"  Available keys in outputs: {list(outputs.keys())}\n"
                f"  Expected keys: ['logits', 'alpha', 'gate_weights', 'load', 'step_logits']\n"
                "  This indicates the model's forward() method is not returning 'step_logits'."
            )
        
        # å¦‚æœæ¨¡å‹æä¾›äº† step_logitsï¼Œä½¿ç”¨ compute_step_loss è®¡ç®—æŸå¤±
        step_logits = outputs['step_logits']  # [B, T]
        step_loss = self.compute_step_loss(step_logits, y_step, seq_mask)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯ step_loss æ˜¯å¦ä¸º NaN æˆ–æ— æ•ˆå€¼ï¼Œå¦‚æœæ˜¯åˆ™ç¦ç”¨è¯¥é¡¹
        if torch.isnan(step_loss) or torch.isinf(step_loss) or step_loss.item() == 0.0:
            if torch.isnan(step_loss) or torch.isinf(step_loss):
                print(f"[WARNING] Step loss is NaN/Inf, disabling step loss component in total loss.")
            # å°† step_loss è®¾ä¸º 0ï¼ˆä¸å¸¦æ¢¯åº¦ï¼‰ï¼Œé˜²æ­¢æ±¡æŸ“æ€»æŸå¤±
            step_loss = torch.tensor(0.0, device=agent_loss.device, requires_grad=False)
            # åŒæ—¶å°†æƒé‡è®¾ä¸º 0ï¼Œç¡®ä¿ä¸å½±å“æ€»æŸå¤±
            effective_w_step = 0.0
        else:
            effective_w_step = self.w_step
        
        # 3. ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±
        aux_loss = self.compute_aux_loss(gate_weights, load, agent_mask)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯æ‰€æœ‰æŸå¤±ç»„ä»¶æ˜¯å¦ä¸º NaN
        if torch.isnan(agent_loss) or torch.isinf(agent_loss):
            raise RuntimeError(f"CRITICAL: Agent loss is NaN/Inf! agent_loss={agent_loss}")
        if torch.isnan(aux_loss) or torch.isinf(aux_loss):
            print(f"[WARNING] Aux loss is NaN/Inf, setting to 0.0")
            aux_loss = torch.tensor(0.0, device=agent_loss.device, requires_grad=False)
            effective_w_aux = 0.0
        else:
            effective_w_aux = self.w_aux
        
        # 4. æ€»æŸå¤±ï¼ˆä½¿ç”¨æœ‰æ•ˆçš„æƒé‡ï¼‰
        total_loss = (self.w_agent * agent_loss + 
                     effective_w_step * step_loss + 
                     effective_w_aux * aux_loss)
        
        # ğŸ”¥ æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ total_loss ä¸æ˜¯ NaN
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"[CRITICAL ERROR] Total loss is NaN/Inf!")
            print(f"  agent_loss: {agent_loss.item():.6f}, w_agent: {self.w_agent}")
            print(f"  step_loss: {step_loss.item():.6f}, w_step: {effective_w_step}")
            print(f"  aux_loss: {aux_loss.item():.6f}, w_aux: {effective_w_aux}")
            raise RuntimeError("Total loss computation resulted in NaN/Inf. Check individual loss components.")
        
        return {
            'total_loss': total_loss,
            'agent_loss': agent_loss,
            'step_loss': step_loss,
            'aux_loss': aux_loss
        }


class ASTRAContrastiveLoss(nn.Module):
    """
    ASTRA-CL: Counterfactual Node-Level Contrast Loss
    
    åŸºäºåäº‹å®ï¼ˆCounterfactualï¼‰çš„èŠ‚ç‚¹çº§å¯¹æ¯”å­¦ä¹ ï¼š
    - Positive Pair (æ‹‰è¿‘): Mutated å›¾ä¸­çš„æ­£å¸¸èŠ‚ç‚¹ vs. Healed å›¾ä¸­çš„å¯¹åº”èŠ‚ç‚¹
    - Negative Pair (æ¨è¿œ): Mutated å›¾ä¸­çš„æ•…éšœèŠ‚ç‚¹ vs. Healed å›¾ä¸­çš„å¯¹åº”èŠ‚ç‚¹
    """
    
    def __init__(self, margin: float = 1.0, alpha: float = 0.7):
        """
        Args:
            margin: å¯¹æ¯”æŸå¤±çš„è¾¹ç•Œï¼ˆç”¨äºæ•…éšœèŠ‚ç‚¹çš„æ¨è¿œï¼‰
            alpha: æ•…éšœèŠ‚ç‚¹æŸå¤±çš„æƒé‡ï¼ˆæ­£å¸¸èŠ‚ç‚¹æŸå¤±æƒé‡ä¸º 1-alphaï¼‰
        """
        super().__init__()
        self.margin = margin
        self.alpha = alpha
    
    def forward(self, 
                emb_mut: torch.Tensor, 
                emb_heal: torch.Tensor, 
                mistake_agent_idx: torch.Tensor,
                agent_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—åäº‹å®å¯¹æ¯”æŸå¤±
        
        Args:
            emb_mut: [Batch, Num_Agents, Hidden_Dim] - æ•…éšœå›¾çš„ Agent åµŒå…¥
            emb_heal: [Batch, Num_Agents, Hidden_Dim] - ä¿®å¤å›¾çš„ Agent åµŒå…¥
            mistake_agent_idx: [Batch] - çœŸå®çš„æ•…éšœ Agent ç´¢å¼•ï¼ˆ-1 è¡¨ç¤ºæ— æ•ˆï¼‰
            agent_mask: [Batch, Num_Agents] - Agent æ©ç ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æŸå¤±å€¼
        """
        B, N, D = emb_mut.shape
        
        # ç¡®ä¿ emb_heal çš„å½¢çŠ¶ä¸ emb_mut åŒ¹é…
        if emb_heal.shape != emb_mut.shape:
            # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•å¯¹é½
            B_h, N_h, D_h = emb_heal.shape
            if B_h != B:
                raise ValueError(f"Batch size mismatch: emb_mut={B}, emb_heal={B_h}")
            if D_h != D:
                raise ValueError(f"Hidden dim mismatch: emb_mut={D}, emb_heal={D_h}")
            
            # å¯¹é½ Agent æ•°é‡ï¼ˆå–è¾ƒå°å€¼ï¼‰
            N = min(N, N_h)
            emb_mut = emb_mut[:, :N, :]
            emb_heal = emb_heal[:, :N, :]
            if agent_mask is not None:
                agent_mask = agent_mask[:, :N]
        
        loss = 0.0
        valid_count = 0
        
        for b in range(B):
            idx = mistake_agent_idx[b].item()
            if idx < 0 or idx >= N:
                continue  # è·³è¿‡æ— æ•ˆæ•°æ®
            
            # è·å–å½“å‰æ ·æœ¬çš„æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
            if agent_mask is not None:
                sample_mask = agent_mask[b, :N]  # [N]
            else:
                sample_mask = torch.ones(N, dtype=torch.bool, device=emb_mut.device)
            
            # 1. æ•…éšœèŠ‚ç‚¹å¯¹æ¯” (Negative Pair): è·ç¦»è¶Šå¤§è¶Šå¥½
            # æå–æ•…éšœ Agent åœ¨ä¸¤ä¸ªå›¾ä¸­çš„ Embedding
            h_mut_target = emb_mut[b, idx]  # [D]
            h_heal_target = emb_heal[b, idx]  # [D]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæˆ‘ä»¬å¸Œæœ›å®ƒè¶Šå°è¶Šå¥½ï¼Œå³è·ç¦»è¶Šå¤§è¶Šå¥½ï¼‰
            cos_sim_target = F.cosine_similarity(h_mut_target.unsqueeze(0), h_heal_target.unsqueeze(0), dim=1)  # [1]
            cos_sim_target = cos_sim_target.squeeze(0)  # scalar
            
            # æŸå¤±ï¼šæˆ‘ä»¬å¸Œæœ› cos_sim_target æ¥è¿‘ -1 æˆ– 0ï¼ˆå³ä¸ç›¸ä¼¼ï¼‰
            # ä½¿ç”¨ hinge loss: max(0, margin - distance)ï¼Œä½†è¿™é‡Œæˆ‘ä»¬å¸Œæœ›è·ç¦»å¤§
            # æ‰€ä»¥ä½¿ç”¨: max(0, cos_sim_target - (-margin)) = max(0, cos_sim_target + margin)
            # æˆ–è€…æ›´ç®€å•ï¼šç›´æ¥æƒ©ç½šç›¸ä¼¼åº¦ï¼ˆå¸Œæœ›ç›¸ä¼¼åº¦ä¸ºè´Ÿï¼‰
            loss_target = F.relu(cos_sim_target + self.margin)  # å¦‚æœç›¸ä¼¼åº¦ > -marginï¼Œåˆ™æƒ©ç½š
            
            # 2. æ­£å¸¸èŠ‚ç‚¹å¯¹æ¯” (Positive Pair): è·ç¦»è¶Šå°è¶Šå¥½ï¼ˆä¿æŒç¨³å®šæ€§ï¼‰
            # ç®—å‡ºéæ•…éšœèŠ‚ç‚¹çš„æ©ç 
            normal_mask = sample_mask.clone()
            normal_mask[idx] = False
            
            if normal_mask.any():
                h_mut_others = emb_mut[b, normal_mask]  # [num_normal, D]
                h_heal_others = emb_heal[b, normal_mask]  # [num_normal, D]
                
                # æ­£å¸¸èŠ‚ç‚¹çš„è¡¨ç¤ºåº”è¯¥ä¿æŒä¸€è‡´ï¼ˆç›¸ä¼¼åº¦åº”è¯¥æ¥è¿‘ 1ï¼‰
                cos_sim_others = F.cosine_similarity(h_mut_others, h_heal_others, dim=1)  # [num_normal]
                
                # æŸå¤±ï¼šæˆ‘ä»¬å¸Œæœ› cos_sim_others æ¥è¿‘ 1ï¼ˆå³ç›¸ä¼¼ï¼‰
                # ä½¿ç”¨: 1 - cos_sim_othersï¼ˆè·ç¦»ï¼‰
                loss_others = (1 - cos_sim_others).mean()
            else:
                loss_others = 0.0
            
            # æ€» Loss: æ•…éšœèŠ‚ç‚¹çš„æ’æ–¥ + æ­£å¸¸èŠ‚ç‚¹çš„å¸å¼•
            loss += self.alpha * loss_target + (1 - self.alpha) * loss_others
            valid_count += 1
        
        if valid_count == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¿”å›é›¶æŸå¤±ï¼ˆå¸¦æ¢¯åº¦ï¼‰
            return emb_mut.sum() * 0.0
        
        return loss / valid_count


class SupConLoss(nn.Module):
    """
    æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤± (Supervised Contrastive Loss)
    è®©åŒä¸€ç±»æ•…éšœçš„æ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´èšé›†ï¼Œä¸åŒç±»çš„æ’æ–¥ã€‚
    """
    def __init__(self, temperature=0.07):
        super(SupConLoss, self).__init__()
        self.temperature = temperature

    def forward(self, features, labels):
        """
        Args:
            features: [batch_size, feature_dim] (æ¨¡å‹çš„ä¸­é—´å±‚è¾“å‡º)
            labels: [batch_size] (æ•…éšœç±»å‹ label, e.g., Tool Error=0, Logic Error=1...)
        """
        device = features.device
        batch_size = features.shape[0]
        
        # ç‰¹å¾å½’ä¸€åŒ–
        features = F.normalize(features, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # æ©ç ï¼šè‡ªå·±ä¸èƒ½å’Œè‡ªå·±å¯¹æ¯”
        mask = torch.eye(batch_size, dtype=torch.bool).to(device)
        
        # æ ‡ç­¾ç›¸åŒçš„ä¸ºæ­£æ ·æœ¬ (Positive Mask)
        labels = labels.view(-1, 1)
        if labels.shape[0] != batch_size:
            raise ValueError('Num of labels does not match num of features')
        mask_positive = torch.eq(labels, labels.T).float().to(device)
        # ç§»é™¤å¯¹è§’çº¿
        mask_positive = mask_positive * (~mask).float()
        
        # è®¡ç®— Logits
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šä½¿ç”¨ InfoNCE å˜ä½“
        exp_logits = torch.exp(similarity_matrix) * (~mask).float()
        log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ­£æ ·æœ¬å¯¹
        # å¦‚æœ batch é‡Œæ²¡æœ‰æ­£æ ·æœ¬å¯¹ï¼ˆå³ mask_positive å…¨ä¸º 0ï¼‰ï¼Œè¿”å› 0 Loss
        num_positive_pairs = mask_positive.sum().item()
        if num_positive_pairs == 0:
            # æ²¡æœ‰æ­£æ ·æœ¬å¯¹ï¼Œå¯¹æ¯”å­¦ä¹ æ— æ•ˆï¼Œè¿”å› 0 Loss
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        mean_log_prob_pos = (mask_positive * log_prob).sum(1) / (mask_positive.sum(1) + 1e-8)
        loss = - mean_log_prob_pos
        return loss.mean()

