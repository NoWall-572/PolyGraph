"""
åŠ¨æ€å¼‚æ„å› æœå›¾ (DHCG) è§£æå™¨ - GPUç‰ˆæœ¬
ç”¨äºè§£æ Who&When æ•°æ®é›†çš„ JSON æ—¥å¿—å¹¶æ„å»ºåŠ¨æ€å¼‚æ„å›¾
æ”¯æŒå•ä¸ªæ–‡ä»¶å’Œç›®å½•æ‰¹é‡å¤„ç†
ä¼˜å…ˆä½¿ç”¨GPUåŠ é€Ÿï¼Œå¦‚æœGPUä¸å¯ç”¨åˆ™è‡ªåŠ¨å›é€€åˆ°CPU
"""

import json
import re
import sys
import os
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from collections import defaultdict

# å°è¯•å¯¼å…¥torchå¹¶æ£€æŸ¥CUDAå¯ç”¨æ€§
try:
    import torch
    import warnings
    # æŠ‘åˆ¶sm_120å…¼å®¹æ€§è­¦å‘Šï¼ˆå¦‚æœGPUè®¡ç®—æµ‹è¯•æˆåŠŸï¼Œè­¦å‘Šå¯ä»¥å¿½ç•¥ï¼‰
    warnings.filterwarnings('ignore', category=UserWarning, module='torch.cuda')
    
    CUDA_AVAILABLE = torch.cuda.is_available()
    BLACKWELL_DETECTED = False
    SM120_UNSUPPORTED = False
    
    if CUDA_AVAILABLE:
        print(f"[GPUæ¨¡å¼] æ£€æµ‹åˆ°CUDAå¯ç”¨ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        print(f"[GPUæ¨¡å¼] GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"[GPUæ¨¡å¼] CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"[GPUæ¨¡å¼] PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥GPUè®¡ç®—èƒ½åŠ›
        capability = torch.cuda.get_device_capability(0)
        if capability[0] >= 12:
            BLACKWELL_DETECTED = True
            print(f"[GPUæ¨¡å¼] æ£€æµ‹åˆ°Blackwellæ¶æ„ (sm_{capability[0]}{capability[1]})")
            # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒsm_120
            pytorch_version = torch.__version__
            version_parts = pytorch_version.split('.')
            major, minor = int(version_parts[0]), int(version_parts[1])
            # PyTorch 2.6+ æˆ– nightly ç‰ˆæœ¬æ‰æ”¯æŒ sm_120
            if major < 2 or (major == 2 and minor < 6):
                if 'dev' not in pytorch_version and 'nightly' not in pytorch_version.lower():
                    SM120_UNSUPPORTED = True
                    print(f"[è­¦å‘Š] å½“å‰PyTorchç‰ˆæœ¬ ({pytorch_version}) ä¸æ”¯æŒsm_120æ¶æ„")
                    print(f"[æç¤º] éœ€è¦PyTorch 2.6+ æˆ– Nightlyç‰ˆæœ¬")
        
        # æµ‹è¯•GPUæ˜¯å¦çœŸçš„å¯ç”¨ï¼ˆå³ä½¿æœ‰è­¦å‘Šï¼‰
        try:
            test_tensor = torch.randn(2, 2).cuda()
            _ = test_tensor @ test_tensor
            print(f"[GPUæ¨¡å¼] GPUè®¡ç®—æµ‹è¯•: âœ“ æˆåŠŸ")
            DEVICE = 'cuda'
        except RuntimeError as e:
            error_msg = str(e).lower()
            if 'no kernel image' in error_msg or 'kernel image is available' in error_msg:
                if BLACKWELL_DETECTED:
                    print(f"[é”™è¯¯] GPUè®¡ç®—æµ‹è¯•å¤±è´¥: PyTorchä¸æ”¯æŒsm_120æ¶æ„")
                    print(f"[è§£å†³æ–¹æ¡ˆ] è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ”¯æŒsm_120çš„PyTorchç‰ˆæœ¬:")
                    print(f"  python install_pytorch_sm120.py")
                    print(f"  æˆ–æ‰‹åŠ¨å®‰è£…: pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121")
                    SM120_UNSUPPORTED = True
                else:
                    print(f"[é”™è¯¯] GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            else:
                print(f"[è­¦å‘Š] GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            print("[å›é€€] å°†ä½¿ç”¨CPUæ¨¡å¼")
            CUDA_AVAILABLE = False
            DEVICE = 'cpu'
        except Exception as e:
            print(f"[è­¦å‘Š] GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            print("[å›é€€] å°†ä½¿ç”¨CPUæ¨¡å¼")
            CUDA_AVAILABLE = False
            DEVICE = 'cpu'
    else:
        print("[è­¦å‘Š] CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        print("[æç¤º] å¦‚éœ€ä½¿ç”¨GPUï¼Œè¯·æ£€æŸ¥:")
        print("  1. PyTorchæ˜¯å¦å®‰è£…äº†CUDAç‰ˆæœ¬")
        print("  2. GPUé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("  3. CUDAå·¥å…·åŒ…æ˜¯å¦ä¸PyTorchç‰ˆæœ¬åŒ¹é…")
        DEVICE = 'cpu'
except ImportError:
    print("[é”™è¯¯] PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
    CUDA_AVAILABLE = False
    DEVICE = 'cpu'
    BLACKWELL_DETECTED = False
    SM120_UNSUPPORTED = False

from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np

# å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_model = None
_tokenizer = None


def get_embedding_model(use_8bit: bool = True, force_cpu: bool = False):
    """
    è·å–æˆ–åˆå§‹åŒ– Qwen-8B åµŒå…¥æ¨¡å‹
    
    Args:
        use_8bit: æ˜¯å¦ä½¿ç”¨8-bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œé»˜è®¤Trueï¼‰
        force_cpu: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰
    """
    global _model, _tokenizer
    if _model is None:
        try:
            # ğŸ”¥ ä½¿ç”¨ Qwen-8B æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹åï¼‰
            model_name = os.getenv("QWEN_MODEL_PATH", "models/Qwen3-8B/qwen/Qwen3-8B")
            if not os.path.exists(model_name):
                # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
                model_name = "models/Qwen3-8B/qwen/Qwen3-8B"
            
            # ç¡®å®šè®¾å¤‡
            if force_cpu:
                device = 'cpu'
                print(f"[å¼ºåˆ¶CPUæ¨¡å¼] æ­£åœ¨åŠ è½½ Qwen-8B åµŒå…¥æ¨¡å‹: {model_name}...")
            elif torch.cuda.is_available() and not force_cpu:
                device = 'cuda'
                print(f"[GPUæ¨¡å¼] æ­£åœ¨åŠ è½½ Qwen-8B åµŒå…¥æ¨¡å‹: {model_name}...")
            else:
                device = 'cpu'
                print(f"[CPUæ¨¡å¼] GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUåŠ è½½æ¨¡å‹: {model_name}...")
            
            _tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶CPUæ¨¡å¼æ—¶ï¼Œç¡®ä¿ä¸ä½¿ç”¨device_map="auto"ï¼ˆä¼šå°è¯•GPUï¼‰
            if force_cpu or device == 'cpu':
                # CPUæ¨¡å¼ï¼šç›´æ¥åŠ è½½åˆ°CPUï¼Œä¸ä½¿ç”¨device_map
                print(f"[{device.upper()}æ¨¡å¼] å¼ºåˆ¶CPUæ¨¡å¼ï¼Œä¸ä½¿ç”¨GPU...")
                print(f"[æç¤º] æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰...")
                _model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float32)
                _model = _model.to('cpu')
                _model.eval()
                print(f"[{device.upper()}æ¨¡å¼] âœ“ æ¨¡å‹æƒé‡åŠ è½½å®Œæˆï¼")
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨8-bité‡åŒ–æˆ–CPUæ¨¡å¼èŠ‚çœæ˜¾å­˜
            elif device == 'cuda' and use_8bit:
                try:
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                        llm_int8_threshold=6.0
                    )
                    print(f"[GPUæ¨¡å¼] ä½¿ç”¨8-bité‡åŒ–åŠ è½½æ¨¡å‹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰...")
                    _model = AutoModel.from_pretrained(
                        model_name,
                        trust_remote_code=True,
                        device_map="auto",
                        quantization_config=quantization_config
                    )
                    print(f"[GPUæ¨¡å¼] âœ“ 8-bité‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ")
                except ImportError:
                    print(f"[è­¦å‘Š] BitsAndBytesæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨FP16æ¨¡å¼...")
                    _model = AutoModel.from_pretrained(
                        model_name,
                        trust_remote_code=True,
                        device_map="auto",
                        torch_dtype=torch.float16
                    )
                    print(f"[GPUæ¨¡å¼] âœ“ FP16æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"[è­¦å‘Š] 8-bité‡åŒ–åŠ è½½å¤±è´¥: {e}")
                    print(f"[å›é€€] å°è¯•ä½¿ç”¨FP16æ¨¡å¼...")
                    _model = AutoModel.from_pretrained(
                        model_name,
                        trust_remote_code=True,
                        device_map="auto",
                        torch_dtype=torch.float16
                    )
            else:
                # å…¶ä»–æƒ…å†µï¼ˆä¸åº”è¯¥æ‰§è¡Œåˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨ä¿ç•™ï¼‰
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šGPUæ¨¡å¼ä½†ä¸ä½¿ç”¨8-bité‡åŒ–æ—¶ï¼Œä½¿ç”¨FP16
                print(f"[GPUæ¨¡å¼] ä½¿ç”¨FP16åŠ è½½æ¨¡å‹...")
                _model = AutoModel.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    device_map="auto",
                    torch_dtype=torch.float16
                )
            
            # ğŸ”¥ æ³¨æ„ï¼šCPUæ¨¡å¼çš„eval()åœ¨ä¸Šé¢å·²ç»è°ƒç”¨äº†ï¼Œè¿™é‡Œåªå¤„ç†GPUæ¨¡å¼
            if not (force_cpu or device == 'cpu'):
                _model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            # æµ‹è¯•ç¼–ç ï¼ˆéªŒè¯æ¨¡å‹å¯ç”¨æ€§ï¼‰
            print(f"[{device.upper()}æ¨¡å¼] æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼ˆè®¡ç®—ç¬¬ä¸€ä¸ªembeddingï¼‰...")
            if force_cpu or device == 'cpu':
                print(f"[æç¤º] CPUæ¨¡å¼ä¸‹embeddingè®¡ç®—å¯èƒ½è¾ƒæ…¢ï¼ˆ10-30ç§’ï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            with torch.no_grad():
                test_text = "test"
                if device == 'cuda':
                    inputs = _tokenizer(test_text, return_tensors="pt", truncation=True, max_length=512).to(device)
                else:
                    inputs = _tokenizer(test_text, return_tensors="pt", truncation=True, max_length=512)
                
                outputs = _model(**inputs)
                # ä½¿ç”¨ mean pooling: å¯¹æ‰€æœ‰ token çš„ hidden states å–å¹³å‡
                if hasattr(outputs, 'last_hidden_state'):
                    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                else:
                    # æœ‰äº›æ¨¡å‹è¿”å›çš„æ˜¯tuple
                    embedding = outputs[0].mean(dim=1).cpu().numpy()
                
                print(f"[{device.upper()}æ¨¡å¼] âœ“ Qwen-8B æ¨¡å‹æˆåŠŸåˆå§‹åŒ–ï¼ŒåµŒå…¥ç»´åº¦: {embedding.shape[1]}")
            
        except Exception as e:
            print(f"[é”™è¯¯] Qwen-8B æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # å¦‚æœGPUå¤±è´¥ï¼Œå°è¯•CPU
            if device == 'cuda' and not force_cpu:
                print(f"[å›é€€] å°è¯•ä½¿ç”¨ CPU æ¨¡å¼...")
                try:
                    device = 'cpu'
                    _tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                    _model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
                    _model.eval()
                    print(f"[CPUæ¨¡å¼] âœ“ Qwen-8B æ¨¡å‹æˆåŠŸåˆå§‹åŒ–ï¼ˆå›é€€æ¨¡å¼ï¼‰")
                except Exception as e2:
                    print(f"[ä¸¥é‡é”™è¯¯] CPU åˆå§‹åŒ–ä¹Ÿå¤±è´¥: {e2}")
                    raise
            else:
                raise
    
    return _model, _tokenizer

def encode_text(text: str, use_8bit: bool = None, force_cpu: bool = None) -> np.ndarray:
    """
    ä½¿ç”¨ Qwen-8B ç¼–ç æ–‡æœ¬ï¼ˆMean Poolingï¼‰
    
    Args:
        text: è¦ç¼–ç çš„æ–‡æœ¬
        use_8bit: æ˜¯å¦ä½¿ç”¨8-bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–æˆ–é»˜è®¤True
        force_cpu: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ã€‚å¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–æˆ–é»˜è®¤False
    """
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    if use_8bit is None:
        use_8bit = os.getenv("USE_8BIT", "true").lower() in ["true", "1", "yes"]
    if force_cpu is None:
        force_cpu = os.getenv("FORCE_CPU", "").lower() in ["true", "1", "yes"]
    
    model, tokenizer = get_embedding_model(use_8bit=use_8bit, force_cpu=force_cpu)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆå…¼å®¹8-bité‡åŒ–æ¨¡å‹ï¼‰
    device = None
    try:
        # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦æœ‰hf_device_mapï¼ˆ8-bité‡åŒ–æ¨¡å‹çš„è®¾å¤‡æ˜ å°„ï¼‰
        if hasattr(model, 'hf_device_map') and model.hf_device_map:
            # 8-bité‡åŒ–æ¨¡å‹ï¼Œè®¾å¤‡æ˜ å°„å¯èƒ½åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Š
            # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯embeddingå±‚æ‰€åœ¨çš„è®¾å¤‡ï¼‰
            try:
                device = next(model.parameters()).device
            except:
                # å¦‚æœæ— æ³•è·å–ï¼Œå°è¯•ä»embed_tokensè·å–
                if hasattr(model, 'embed_tokens'):
                    device = next(model.embed_tokens.parameters()).device
                else:
                    device = torch.device('cuda' if torch.cuda.is_available() and not force_cpu else 'cpu')
        # æ–¹æ³•2: ç›´æ¥è·å–deviceå±æ€§
        elif hasattr(model, 'device'):
            device = model.device
        # æ–¹æ³•3: ä»ç¬¬ä¸€ä¸ªå‚æ•°è·å–è®¾å¤‡
        else:
            try:
                device = next(model.parameters()).device
            except:
                # å¦‚æœæ— æ³•è·å–ï¼Œå°è¯•ä»embed_tokensè·å–
                if hasattr(model, 'embed_tokens'):
                    device = next(model.embed_tokens.parameters()).device
                else:
                    device = torch.device('cuda' if torch.cuda.is_available() and not force_cpu else 'cpu')
    except:
        # é»˜è®¤ä½¿ç”¨CUDAï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸æ˜¯å¼ºåˆ¶CPUï¼‰
        device = torch.device('cuda' if torch.cuda.is_available() and not force_cpu else 'cpu')
    
    # ğŸ”¥ æ·»åŠ è¿›åº¦æç¤ºï¼ˆCPUæ¨¡å¼å¾ˆæ…¢ï¼‰
    if force_cpu or (device is not None and device.type == 'cpu'):
        # CPUæ¨¡å¼ï¼šé™é»˜å¤„ç†ï¼Œä¸æ‰“å°ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
        pass
    
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        # å¯¹äº8-bité‡åŒ–æ¨¡å‹ï¼Œembed_tokenså¯èƒ½åœ¨CUDAä¸Šï¼Œå¿…é¡»å°†inputsç§»åˆ°ç›¸åŒè®¾å¤‡
        if device is not None and device.type == 'cuda':
            # GPUæ¨¡å¼ï¼šå°†è¾“å…¥ç§»åˆ°GPU
            inputs = {k: v.to(device) for k, v in inputs.items()}
        # CPUæ¨¡å¼ï¼šinputså·²ç»åœ¨CPUä¸Šï¼Œä¸éœ€è¦ç§»åŠ¨
        
        # ğŸ”¥ CPUæ¨¡å¼ä¸‹è®¡ç®—embeddingä¼šå¾ˆæ…¢ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’åˆ°å‡ åç§’ï¼‰
        outputs = model(**inputs)
        
        # Mean pooling: å¯¹æ‰€æœ‰ token çš„ hidden states å–å¹³å‡
        if hasattr(outputs, 'last_hidden_state'):
            embedding = outputs.last_hidden_state.mean(dim=1)
        else:
            # æœ‰äº›æ¨¡å‹è¿”å›çš„æ˜¯tuple
            embedding = outputs[0].mean(dim=1)
        
        # ç¡®ä¿ç»“æœåœ¨CPUä¸Š
        if isinstance(embedding, torch.Tensor):
            embedding = embedding.cpu().numpy()
    
    return embedding[0] if embedding.ndim > 1 else embedding  # è¿”å› [dim] å½¢çŠ¶çš„æ•°ç»„


class Node:
    """å›¾èŠ‚ç‚¹ç±»"""
    def __init__(self, node_id: str, node_type: str, creation_time: int, artifact_type: Optional[str] = None):
        self.id: str = node_id
        self.type: str = node_type  # e.g., "Agent", "Tool", "Artifact", "Environment"
        self.created_at: int = creation_time
        # features[t] å­˜å‚¨èŠ‚ç‚¹åœ¨æ—¶é—´æ­¥ t çš„åŠ¨æ€ç‰¹å¾
        self.features: Dict[int, Dict[str, Any]] = {}
        # artifact_type: ä»…å¯¹ArtifactèŠ‚ç‚¹æœ‰æ•ˆï¼Œå€¼ä¸º"file"æˆ–"url"
        self.artifact_type: Optional[str] = artifact_type
    
    def __repr__(self):
        artifact_info = f", artifact_type='{self.artifact_type}'" if self.artifact_type else ""
        return f"Node(id='{self.id}', type='{self.type}', created_at={self.created_at}{artifact_info})"


class Edge:
    """å›¾è¾¹ç±»"""
    def __init__(self, source_id: str, target_id: str, edge_type: str, timestamp: int, features: Dict[str, Any]):
        self.source: str = source_id
        self.target: str = target_id
        self.type: str = edge_type  # e.g., "Communicate", "Invoke", "Return", "Reference", "Affect"
        self.timestamp: int = timestamp
        self.features: Dict[str, Any] = features
    
    def __repr__(self):
        features_str = f", Features: {self.features}" if self.features else ""
        return f"Edge(t={self.timestamp}): {self.source} -> {self.target} (Type: {self.type}{features_str})"


class DynamicGraph:
    """åŠ¨æ€å¼‚æ„å›¾ç±»"""
    def __init__(self, question: str, ground_truth: Dict[str, Any]):
        self.question: str = question
        self.ground_truth: Dict[str, Any] = ground_truth
        self.nodes: Dict[str, Node] = {}  # Key: node_id
        self.edges: List[Edge] = []
    
    def add_node(self, node: Node):
        """æ·»åŠ èŠ‚ç‚¹åˆ°å›¾ä¸­"""
        if node.id not in self.nodes:
            self.nodes[node.id] = node
    
    def add_edge(self, edge: Edge):
        """æ·»åŠ è¾¹åˆ°å›¾ä¸­"""
        self.edges.append(edge)
    
    def __repr__(self):
        question_preview = self.question[:30] + "..." if len(self.question) > 30 else self.question
        return (f"DynamicGraph(\n"
                f"  Nodes: {len(self.nodes)},\n"
                f"  Edges: {len(self.edges)},\n"
                f"  Question: '{question_preview}'\n)")


def GetOrCreateNode(name: str, node_type: str, t: int, node_registry: Dict[str, Node], artifact_type: Optional[str] = None) -> Node:
    """
    è·å–æˆ–åˆ›å»ºèŠ‚ç‚¹
    
    Args:
        name: èŠ‚ç‚¹åç§°
        node_type: èŠ‚ç‚¹ç±»å‹
        t: æ—¶é—´æ­¥
        node_registry: èŠ‚ç‚¹æ³¨å†Œè¡¨
        artifact_type: ä»…å¯¹ArtifactèŠ‚ç‚¹æœ‰æ•ˆï¼Œå€¼ä¸º"file"æˆ–"url"
    
    Returns:
        Nodeå¯¹è±¡
    """
    if name not in node_registry:
        # å¦‚æœæ˜¯ArtifactèŠ‚ç‚¹ï¼Œæ ¹æ®nameåˆ¤æ–­artifact_type
        if node_type == "Artifact" and artifact_type is None:
            if "http" in name.lower():
                artifact_type = "url"
            else:
                artifact_type = "file"
        node = Node(name, node_type, t, artifact_type=artifact_type)
        node_registry[name] = node
    return node_registry[name]


def DetermineNodeType(actor: str, system_prompt: Dict[str, Any], event: Dict[str, Any]) -> str:
    """
    ç¡®å®šèŠ‚ç‚¹çš„ç±»å‹
    
    Args:
        actor: äº‹ä»¶å‘èµ·è€…åç§°
        system_prompt: ç³»ç»Ÿæç¤ºå­—å…¸
        event: äº‹ä»¶å­—å…¸
    
    Returns:
        èŠ‚ç‚¹ç±»å‹å­—ç¬¦ä¸²
    """
    # å¦‚æœactoræ˜¯Computer_terminalï¼Œè¿”å›"Tool"
    if actor == "Computer_terminal":
        return "Tool"
    
    # å¦‚æœeventä¸­åŒ…å«exitcodeå­—æ®µï¼Œæˆ–è€…contentä¸­åŒ…å«"exitcode:"ï¼Œè¿”å›"Tool"
    if "exitcode" in event:
        return "Tool"
    content = event.get('content', '')
    if re.search(r'exitcode:\s*\d+', content, re.IGNORECASE):
        return "Tool"
    
    # å¦‚æœactoråœ¨system_promptçš„é”®ä¸­ï¼Œè¿”å›"Agent"
    if system_prompt and actor in system_prompt:
        return "Agent"
    
    # å¦‚æœactorçš„åå­—åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼ï¼Œè¿”å›"Agent"
    if re.search(r'(Expert|Assistant|Orchestrator|Surfer|Planner)', actor):
        return "Agent"
    
    # é»˜è®¤è¿”å›"Agent"
    return "Agent"


def FindCallerAgent(tool_node: Node, t: int, history: List[Dict[str, Any]], 
                   system_prompt: Dict[str, Any]) -> str:
    """
    æŸ¥æ‰¾è°ƒç”¨å·¥å…·çš„Agentï¼ˆå¢å¼ºç‰ˆï¼šç›´æ¥ä½¿ç”¨historyè€Œéedgesï¼‰
    
    Args:
        tool_node: å·¥å…·èŠ‚ç‚¹
        t: å½“å‰æ—¶é—´æ­¥
        history: å†å²äº‹ä»¶åˆ—è¡¨
        system_prompt: ç³»ç»Ÿæç¤ºå­—å…¸ï¼ˆç”¨äºDetermineNodeTypeï¼‰
    
    Returns:
        è°ƒç”¨è€…Agentçš„åç§°ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›"Broadcast"
    """
    # ä»æ—¶é—´æ­¥ t-1 å¼€å§‹å‘ä¸Šå›æº¯
    for tau in range(t - 1, -1, -1):
        if tau < len(history):
            prev_event = history[tau]
            prev_actor = prev_event.get('name') or prev_event.get('role', '')
            prev_content = prev_event.get('content', '')
            
            # æ£€æŸ¥prev_actoræ˜¯å¦æ˜¯Agentç±»å‹
            actor_type = DetermineNodeType(prev_actor, system_prompt, prev_event)
            if actor_type == "Agent":
                # æ£€æŸ¥prev_contentæ˜¯å¦åŒ…å«ä»£ç å—ï¼ˆè¡¨ç¤ºè°ƒç”¨äº†å·¥å…·ï¼‰
                code_block_pattern = r'```(?:python|sh|bash|javascript|js|java|cpp|c\+\+|c|go|rust|sql|html|css|xml|json|yaml|yml|markdown|md|text|plaintext)[\s\S]*?```'
                if re.search(code_block_pattern, prev_content, re.IGNORECASE):
                    return prev_actor
    
    return "Broadcast"


def ParseEdges(source_node: Node, content: str, t: int, history: List[Dict[str, Any]], 
               node_registry: Dict[str, Node], event: Dict[str, Any], system_prompt: Dict[str, Any],
               mention_counter: Dict[str, int]) -> List[Tuple[str, str, str, Dict[str, Any]]]:
    """
    è§£æè¾¹ï¼Œè¿”å›äº¤äº’å…ƒç»„åˆ—è¡¨ (target_name, target_type, edge_type, edge_features)
    
    Args:
        source_node: æºèŠ‚ç‚¹
        content: äº‹ä»¶å†…å®¹
        t: å½“å‰æ—¶é—´æ­¥
        history: å†å²äº‹ä»¶åˆ—è¡¨
        node_registry: èŠ‚ç‚¹æ³¨å†Œè¡¨
        event: å½“å‰äº‹ä»¶å­—å…¸
        system_prompt: ç³»ç»Ÿæç¤ºå­—å…¸ï¼ˆç”¨äºFindCallerAgentï¼‰
        mention_counter: å¼•ç”¨è®¡æ•°å™¨ï¼ˆç”¨äºç»Ÿè®¡Artifactè¢«å¼•ç”¨æ¬¡æ•°ï¼‰
    
    Returns:
        äº¤äº’å…ƒç»„åˆ—è¡¨
    """
    interactions = []
    
    # 1. Invoke: å¦‚æœsource_node.type == "Agent" å¹¶ä¸”contentåŒ…å«ä»£ç å—
    if source_node.type == "Agent":
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»£ç å— (```python...``` æˆ– ```sh...```)
        code_block_pattern = r'```(?:python|sh|bash|javascript|js|java|cpp|c\+\+|c|go|rust|sql|html|css|xml|json|yaml|yml|markdown|md|text|plaintext)[\s\S]*?```'
        if re.search(code_block_pattern, content, re.IGNORECASE):
            # Invokeè¾¹çš„æ„å›¾å›ºå®šä¸º"Command"
            interactions.append(("Computer_terminal", "Tool", "Invoke", {"intent": "Command"}))
    
    # 2. Return: å¦‚æœsource_node.type == "Tool" å¹¶ä¸”eventä¸­æœ‰exitcode
    if source_node.type == "Tool":
        exitcode = None
        # é¦–å…ˆæ£€æŸ¥eventä¸­æ˜¯å¦æœ‰exitcodeå­—æ®µ
        if "exitcode" in event:
            exitcode = event.get("exitcode")
        else:
            # ä»contentä¸­æå–exitcode
            exitcode_match = re.search(r'exitcode:\s*(\d+)', content, re.IGNORECASE)
            if exitcode_match:
                exitcode = exitcode_match.group(1)
        
        if exitcode is not None:
            # å¤„ç†exitcodeå€¼
            if isinstance(exitcode, str):
                exitcode_match = re.search(r'(\d+)', exitcode)
                if exitcode_match:
                    exitcode = int(exitcode_match.group(1))
                else:
                    exitcode = 0
            elif not isinstance(exitcode, int):
                exitcode = 0
            
            status = "success" if exitcode == 0 else "failure"
            # Returnè¾¹çš„æ„å›¾å›ºå®šä¸º"Inform"
            caller_agent = FindCallerAgent(source_node, t, history, system_prompt)
            interactions.append((caller_agent, "Agent", "Return", {"status": status, "intent": "Inform"}))
    
    # 3. Reference: æå–æ–‡ä»¶è·¯å¾„å’ŒURLï¼ˆå»é‡å’Œè§„èŒƒåŒ–ï¼‰
    # æ­£åˆ™è¡¨è¾¾å¼: (\.\./[\w/.-]+|https?://[\w/.-]+|filename:\s*[\w.-]+)
    reference_pattern = r'(\.\./[\w/.-]+|https?://[\w/.-]+|filename:\s*[\w.-]+)'
    references = re.findall(reference_pattern, content)
    # å»é‡ï¼šå°†åˆ—è¡¨è½¬æ¢ä¸ºsetå†è½¬å›list
    references = list(set(references))
    
    for ref in references:
        # è§„èŒƒåŒ–ï¼šæ¸…ç†å¼•ç”¨å­—ç¬¦ä¸²ï¼Œå»é™¤æœ«å°¾æ ‡ç‚¹ç¬¦å·
        ref_clean = ref.strip().rstrip('.,;:')
        if ref_clean.startswith("filename:"):
            ref_clean = ref_clean.replace("filename:", "").strip().rstrip('.,;:')
        
        # æ›´æ–°mention_counter
        mention_counter[ref_clean] += 1
        
        # Referenceè¾¹çš„æ„å›¾å›ºå®šä¸º"Inform"
        interactions.append((ref_clean, "Artifact", "Reference", {"intent": "Inform"}))
    
    # 4. Communicateï¼ˆéœ€è¦åˆ¤æ–­æ„å›¾ï¼‰
    # ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ @\w+ æŸ¥æ‰¾ç›´æ¥@æåŠ
    mention_pattern = r'@(\w+)'
    mentions = re.findall(mention_pattern, content)
    
    # åˆ¤æ–­Communicateè¾¹çš„æ„å›¾
    intent = "Inform"  # é»˜è®¤æ„å›¾
    if mentions:
        # å¦‚æœcontentåŒ…å«é—®å·æˆ–@å¼€å¤´ï¼Œæ„å›¾ä¸º"Query"
        if '?' in content or content.strip().startswith('@'):
            intent = "Query"
        for mention in mentions:
            interactions.append((mention, "Agent", "Communicate", {"intent": intent}))
    else:
        # å¦‚æœæ²¡æœ‰æåŠï¼Œä¸”contentæ˜¯è‡ªç„¶è¯­è¨€ï¼Œä¸”t > 0ï¼Œä¸”history[t-1]çš„actorä¸æ˜¯è‡ªå·±
        if t > 0 and t - 1 < len(history):
            prev_event = history[t - 1]
            prev_actor = prev_event.get('name') or prev_event.get('role', '')
            if prev_actor != source_node.id:
                # æ£€æŸ¥contentæ˜¯å¦æ˜¯è‡ªç„¶è¯­è¨€ï¼ˆä¸åŒ…å«ä»£ç å—ï¼‰
                if not re.search(r'```[\s\S]*?```', content):
                    # åˆ¤æ–­æ˜¯å¦æ˜¯å›å¤ï¼šå¦‚æœåŒ…å«é—®å·ï¼Œæ„å›¾ä¸º"Query"ï¼Œå¦åˆ™ä¸º"Inform"
                    if '?' in content:
                        intent = "Query"
                    else:
                        intent = "Inform"
                    interactions.append((prev_actor, "Agent", "Communicate", {"intent": intent}))
                else:
                    intent = "Broadcast"
                    interactions.append(("Broadcast", "Environment", "Communicate", {"intent": intent}))
            else:
                intent = "Broadcast"
                interactions.append(("Broadcast", "Environment", "Communicate", {"intent": intent}))
        else:
            intent = "Broadcast"
            interactions.append(("Broadcast", "Environment", "Communicate", {"intent": intent}))
    
    # 5. Affect: æŸ¥æ‰¾ç¯å¢ƒé”™è¯¯å…³é”®è¯
    # æ³¨æ„ï¼šAffectè¾¹çš„æ–¹å‘æ˜¯ä»EnvæŒ‡å‘source_node
    # è¿™é‡Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œä¸»å‡½æ•°ä¼šå¤„ç†
    error_pattern = r'(Timeout|OOM|Permission Denied|Network Error)'
    error_match = re.search(error_pattern, content, re.IGNORECASE)
    if error_match:
        # æå–åŒ¹é…åˆ°çš„é”™è¯¯ç±»å‹ï¼ˆè½¬ä¸ºå°å†™ï¼‰
        event_type = error_match.group(1).lower()
        # Affectè¾¹çš„æ„å›¾å›ºå®šä¸º"Reject"
        interactions.append(("__AFFECT__", "Environment", "Affect", {"intent": "Reject", "event_type": event_type}))
    
    return interactions


def ExtractNodeFeatures(source_node: Node, event: Dict[str, Any], t: int, history: List[Dict[str, Any]],
                        mention_counter: Dict[str, int], env_event_type: Optional[str] = None) -> Dict[str, Any]:
    """
    æå–èŠ‚ç‚¹ç‰¹å¾
    
    Args:
        source_node: æºèŠ‚ç‚¹
        event: å½“å‰äº‹ä»¶å­—å…¸
        t: å½“å‰æ—¶é—´æ­¥
        history: å†å²äº‹ä»¶åˆ—è¡¨
        mention_counter: å¼•ç”¨è®¡æ•°å™¨ï¼ˆç”¨äºArtifactèŠ‚ç‚¹çš„mention_countç‰¹å¾ï¼‰
        env_event_type: ç¯å¢ƒäº‹ä»¶ç±»å‹ï¼ˆç”¨äºEnvironmentèŠ‚ç‚¹çš„env_event_typeç‰¹å¾ï¼‰
    
    Returns:
        ç‰¹å¾å­—å…¸
    """
    features = {}
    content = event.get('content', '')
    
    # 1. content_embedding: ä½¿ç”¨ Qwen-8B ç¼–ç ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
    # ä½¿ç”¨ encode_text å‡½æ•°è¿›è¡Œç¼–ç 
    embedding = encode_text(content).tolist()
    features['content_embedding'] = embedding
    # ğŸ”¥ å­˜å‚¨åŸå§‹æ–‡æœ¬ï¼Œä¾› Ollama embedding ä½¿ç”¨
    features['content_text'] = content
    
    # 2. Toolç‰¹å¾
    if source_node.type == "Tool":
        exitcode = None
        # é¦–å…ˆæ£€æŸ¥eventä¸­æ˜¯å¦æœ‰exitcodeå­—æ®µ
        if "exitcode" in event:
            exitcode = event.get("exitcode")
        else:
            # ä»contentä¸­æå–exitcode
            exitcode_match = re.search(r'exitcode:\s*(\d+)', content, re.IGNORECASE)
            if exitcode_match:
                exitcode = exitcode_match.group(1)
        
        if exitcode is not None:
            # å¤„ç†exitcodeå€¼
            if isinstance(exitcode, str):
                exitcode_match = re.search(r'(\d+)', exitcode)
                if exitcode_match:
                    exitcode = int(exitcode_match.group(1))
                else:
                    exitcode = 0
            elif not isinstance(exitcode, int):
                exitcode = 0
            features['exitcode_status'] = "success" if exitcode == 0 else "failure"
        else:
            features['exitcode_status'] = "unknown"
    
    # 3. Agentç‰¹å¾
    if source_node.type == "Agent":
        # is_terminate: contentä¸­æ˜¯å¦åŒ…å«"TERMINATE"
        features['is_terminate'] = "TERMINATE" in content.upper()
        
        # plan_signal: contentä¸­æ˜¯å¦åŒ…å«"Plan"æˆ–"Step"
        features['plan_signal'] = bool(re.search(r'\b(Plan|Step)\b', content, re.IGNORECASE))
        
        # active_ratio: è®¡ç®—source_node.idåœ¨historyçš„0åˆ°tæ­¥ä¸­å‡ºç°çš„é¢‘ç‡
        count = 0
        for i in range(t + 1):
            if i < len(history):
                event_i = history[i]
                actor_name = event_i.get('name') or event_i.get('role', '')
                if actor_name == source_node.id:
                    count += 1
        features['active_ratio'] = count / (t + 1) if t >= 0 else 0.0
    
    # 4. Artifactç‰¹å¾
    if source_node.type == "Artifact":
        # artifact_type: ä»èŠ‚ç‚¹çš„é™æ€å±æ€§ä¸­è¯»å–
        if source_node.artifact_type:
            features['artifact_type'] = source_node.artifact_type
        else:
            # å¦‚æœèŠ‚ç‚¹æ²¡æœ‰artifact_typeå±æ€§ï¼Œæ ¹æ®IDåˆ¤æ–­
            if "http" in source_node.id.lower():
                features['artifact_type'] = "url"
            else:
                features['artifact_type'] = "file"
        
        # mention_count: ä»mention_counterä¸­è¯»å–å½“å‰è®¡æ•°
        features['mention_count'] = mention_counter.get(source_node.id, 0)
    
    # 5. Environmentç‰¹å¾
    if source_node.type == "Environment":
        # env_event_type: å¦‚æœæä¾›äº†env_event_typeå‚æ•°ï¼Œæ·»åŠ åˆ°ç‰¹å¾ä¸­
        if env_event_type:
            features['env_event_type'] = env_event_type
    
    return features


def MainParser(json_data: Dict[str, Any]) -> DynamicGraph:
    """
    ä¸»è§£æå‡½æ•°
    
    Args:
        json_data: JSONæ•°æ®å­—å…¸
    
    Returns:
        DynamicGraphå¯¹è±¡
    """
    # 1. åˆå§‹åŒ–
    question = json_data.get('question', '')
    ground_truth = {
        'mistake_agent': json_data.get('mistake_agent', ''),
        'mistake_step': json_data.get('mistake_step', ''),
        'mistake_reason': json_data.get('mistake_reason', ''),
        'ground_truth': json_data.get('ground_truth', '')
    }
    graph = DynamicGraph(question, ground_truth)
    node_registry = graph.nodes
    
    # é¢„åˆ›å»ºå…¨å±€èŠ‚ç‚¹
    GetOrCreateNode("Broadcast", "Environment", -1, node_registry)
    GetOrCreateNode("Env", "Environment", -1, node_registry)
    
    history = json_data.get('history', [])
    system_prompt = json_data.get('system_prompt', {})
    
    # åˆå§‹åŒ–mention_counterï¼ˆç”¨äºç»Ÿè®¡Artifactè¢«å¼•ç”¨æ¬¡æ•°ï¼‰
    mention_counter = defaultdict(int)
    
    # 2. éå†å†å²äº‹ä»¶
    for t in range(len(history)):
        event = history[t]
        actor_name = event.get('name') or event.get('role', '')
        
        # 3. è¯†åˆ«å¹¶è·å–æºèŠ‚ç‚¹
        actor_type = DetermineNodeType(actor_name, system_prompt, event)
        source_node = GetOrCreateNode(actor_name, actor_type, t, node_registry)
        
        # 4. è§£æå†…å®¹ä»¥åˆ›å»ºè¾¹å’Œç›®æ ‡èŠ‚ç‚¹
        content = event.get('content', '')
        interactions = ParseEdges(source_node, content, t, history, node_registry, event, system_prompt, mention_counter)
        
        # ç”¨äºå­˜å‚¨å½“å‰æ—¶é—´æ­¥çš„env_event_typeï¼ˆå¦‚æœæœ‰Affectè¾¹ï¼‰
        current_env_event_type = None
        
        for target_name, target_type, edge_type, edge_features in interactions:
            # ç‰¹æ®Šå¤„ç†Affectè¾¹ï¼šæ–¹å‘æ˜¯ä»EnvæŒ‡å‘source_node
            if edge_type == "Affect" and target_name == "__AFFECT__":
                # Affectè¾¹æ˜¯ä»EnvæŒ‡å‘source_node
                env_node = GetOrCreateNode("Env", "Environment", -1, node_registry)
                # ä»edge_featuresä¸­æå–event_typeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "event_type" in edge_features:
                    current_env_event_type = edge_features["event_type"]
                edge = Edge(env_node.id, source_node.id, edge_type, t, edge_features)
                graph.add_edge(edge)
            else:
                # å¯¹äºArtifactèŠ‚ç‚¹ï¼Œéœ€è¦ä¼ é€’artifact_typeå‚æ•°
                artifact_type = None
                if target_type == "Artifact":
                    # æ ¹æ®target_nameåˆ¤æ–­artifact_type
                    if "http" in target_name.lower():
                        artifact_type = "url"
                    else:
                        artifact_type = "file"
                target_node = GetOrCreateNode(target_name, target_type, t, node_registry, artifact_type=artifact_type)
                edge = Edge(source_node.id, target_node.id, edge_type, t, edge_features)
                graph.add_edge(edge)
        
        # 5. æå–å¹¶å­˜å‚¨å½“å‰èŠ‚ç‚¹çš„åŠ¨æ€ç‰¹å¾
        # å¦‚æœæ˜¯EnvèŠ‚ç‚¹ä¸”æœ‰Affectè¾¹ï¼Œä¼ é€’env_event_type
        env_event_type_for_features = current_env_event_type if source_node.id == "Env" and current_env_event_type else None
        node_features = ExtractNodeFeatures(source_node, event, t, history, mention_counter, env_event_type_for_features)
        source_node.features[t] = node_features
    
    return graph


def process_single_file(json_file: Path, verbose: bool = True, save_result: bool = False, output_dir: Optional[Path] = None, source_dir_name: Optional[str] = None) -> Optional[DynamicGraph]:
    """
    å¤„ç†å•ä¸ªJSONæ–‡ä»¶
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        save_result: æ˜¯å¦ä¿å­˜ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        source_dir_name: æºç›®å½•åç§°ï¼ˆç”¨äºåŒºåˆ†ä¸åŒç›®å½•çš„åŒåæ–‡ä»¶ï¼‰
    
    Returns:
        DynamicGraphå¯¹è±¡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    try:
        if verbose:
            print(f"Processing: {json_file.name}...")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not json_file.exists():
            if verbose:
                print(f"  âœ— Error: File '{json_file}' does not exist.")
            return None

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆè€Œä¸æ˜¯ç›®å½•ï¼‰
        if not json_file.is_file():
            if verbose:
                print(f"  âœ— Error: '{json_file}' is not a file.")
            return None
        
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        # è°ƒç”¨MainParseræ„å»ºå›¾
        graph = MainParser(json_data)
        
        if verbose:
            print(f"  âœ“ Success: {len(graph.nodes)} nodes, {len(graph.edges)} edges")
        
        # ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        if save_result and output_dir:
            output_dir.mkdir(parents=True, exist_ok=True)
            # å¦‚æœæä¾›äº†æºç›®å½•åï¼Œåœ¨æ–‡ä»¶åä¸­åŒ…å«å®ƒä»¥é¿å…ä¸åŒç›®å½•çš„åŒåæ–‡ä»¶å†²çª
            if source_dir_name:
                # æ¸…ç†ç›®å½•åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                safe_dir_name = source_dir_name.replace('/', '_').replace('\\', '_').replace('&', '_')
                output_file = output_dir / f"{safe_dir_name}_{json_file.stem}_graph.json"
            else:
                output_file = output_dir / f"{json_file.stem}_graph.json"
            
            # å°†å›¾è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            graph_dict = {
                'question': graph.question,
                'ground_truth': graph.ground_truth,
                'nodes': {},
                'edges': []
            }
            
            # åºåˆ—åŒ–èŠ‚ç‚¹
            for node_id, node in graph.nodes.items():
                graph_dict['nodes'][node_id] = {
                    'id': node.id,
                    'type': node.type,
                    'created_at': node.created_at,
                    'features': {str(t): features for t, features in node.features.items()}
                }
            
            # åºåˆ—åŒ–è¾¹
            for edge in graph.edges:
                graph_dict['edges'].append({
                    'source': edge.source,
                    'target': edge.target,
                    'type': edge.type,
                    'timestamp': edge.timestamp,
                    'features': edge.features
                })
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(graph_dict, f, ensure_ascii=False, indent=2)
            
            if verbose:
                print(f"  âœ“ Saved to: {output_file}")
        
        return graph
        
    except FileNotFoundError:
        if verbose:
            print(f"  âœ— Error: File '{json_file}' not found.")
        return None
    except json.JSONDecodeError as e:
        if verbose:
            print(f"  âœ— Error: Invalid JSON format in '{json_file}': {e}")
        return None
    except Exception as e:
        if verbose:
            print(f"  âœ— Error: {type(e).__name__}: {e}")
        return None


def process_directory(directory: Path, verbose: bool = True, save_result: bool = False, output_dir: Optional[Path] = None) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    
    Args:
        directory: ç›®å½•è·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        save_result: æ˜¯å¦ä¿å­˜ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡å­—å…¸
    """
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = sorted(directory.glob("*.json"))
    
    if not json_files:
        print(f"No JSON files found in directory: {directory}")
        return {'total': 0, 'success': 0, 'failed': 0, 'files': []}
    
    print(f"\nFound {len(json_files)} JSON files in: {directory}")
    print("=" * 60)
    
    results = {
        'total': len(json_files),
        'success': 0,
        'failed': 0,
        'files': []
    }
    
    # è·å–ç›®å½•åç§°ï¼ˆç”¨äºåŒºåˆ†ä¸åŒç›®å½•çš„åŒåæ–‡ä»¶ï¼‰
    source_dir_name = directory.name
    
    for i, json_file in enumerate(json_files, 1):
        if verbose:
            print(f"\n[{i}/{len(json_files)}] ", end="")
        
        graph = process_single_file(json_file, verbose=verbose, save_result=save_result, output_dir=output_dir, source_dir_name=source_dir_name)
        
        if graph is not None:
            results['success'] += 1
            results['files'].append({
                'file': str(json_file),
                'status': 'success',
                'nodes': len(graph.nodes),
                'edges': len(graph.edges)
            })
        else:
            results['failed'] += 1
            results['files'].append({
                'file': str(json_file),
                'status': 'failed'
            })
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print(f"Processing Complete!")
    print(f"  Total files: {results['total']}")
    print(f"  Successful: {results['success']}")
    print(f"  Failed: {results['failed']}")
    print("=" * 60)
    
    return results


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='è§£æWho&When JSONæ—¥å¿—å¹¶æ„å»ºåŠ¨æ€å¼‚æ„å›¾ï¼ˆGPUåŠ é€Ÿç‰ˆæœ¬ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¤„ç†å•ä¸ªæ–‡ä»¶
  python parser_gpu.py "Who&When/Algorithm-Generated/1.json"
  
  # æ‰¹é‡å¤„ç†æ•´ä¸ªç›®å½•
  python parser_gpu.py "Who&When/Algorithm-Generated"
  python parser_gpu.py "Who&When/Hand-Crafted"
  
  # æ‰¹é‡å¤„ç†å¹¶ä¿å­˜ç»“æœ
  python parser_gpu.py "Who&When/Algorithm-Generated" --save --output results/
        """
    )
    parser.add_argument('input_path', type=str, help='è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--save', action='store_true', help='ä¿å­˜è§£æç»“æœåˆ°JSONæ–‡ä»¶')
    parser.add_argument('--output', type=str, default='outputs', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputsï¼‰')
    parser.add_argument('--quiet', action='store_true', help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæ€»ç»“ä¿¡æ¯')
    
    args = parser.parse_args()
    
    input_path = Path(args.input_path)
    
    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
    if not input_path.exists():
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆdhcg_parserï¼‰
        script_dir = Path(__file__).parent
        # å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾ï¼ˆä¸Šä¸€çº§ç›®å½•ï¼‰
        project_root = script_dir.parent
        alternative_path = project_root / args.input_path
        
        if alternative_path.exists():
            input_path = alternative_path
            if not args.quiet:
                print(f"Note: Using path from project root: {input_path}")
        else:
            print(f"Error: Path '{args.input_path}' does not exist.")
            print(f"  Tried: {Path(args.input_path).absolute()}")
            print(f"  Also tried: {alternative_path.absolute()}")
            sys.exit(1)
    
    # åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    if input_path.is_file():
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        if not input_path.suffix == '.json':
            print(f"Error: '{input_path}' is not a JSON file.")
            sys.exit(1)
        
        output_dir = Path(args.output) if args.save else None
        # å¯¹äºå•ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨çˆ¶ç›®å½•åä½œä¸ºæ ‡è¯†ï¼ˆå¦‚æœçˆ¶ç›®å½•åæœ‰æ„ä¹‰ï¼‰
        source_dir_name = input_path.parent.name if input_path.parent.name else None
        graph = process_single_file(input_path, verbose=not args.quiet, save_result=args.save, output_dir=output_dir, source_dir_name=source_dir_name)
        
        if graph is None:
            sys.exit(1)
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœä¸æ˜¯é™é»˜æ¨¡å¼ï¼‰
        if not args.quiet:
            print("\n--- Graph Summary ---")
            print(graph)
            print(f"\n--- Question ---")
            print(f"{graph.question}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print("\n--- Statistics ---")
            node_types = {}
            for node in graph.nodes.values():
                node_types[node.type] = node_types.get(node.type, 0) + 1
            print(f"Node types: {node_types}")
            
            edge_types = {}
            for edge in graph.edges:
                edge_types[edge.type] = edge_types.get(edge.type, 0) + 1
            print(f"Edge types: {edge_types}")
    
    elif input_path.is_dir():
        # æ‰¹é‡å¤„ç†ç›®å½•
        output_dir = Path(args.output) if args.save else None
        results = process_directory(input_path, verbose=not args.quiet, save_result=args.save, output_dir=output_dir)
        
        if results['failed'] > 0:
            sys.exit(1)
    
    else:
        print(f"Error: '{input_path}' is neither a file nor a directory.")
        sys.exit(1)


if __name__ == "__main__":
    main()

