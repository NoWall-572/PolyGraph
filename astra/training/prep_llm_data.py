"""
å°†å›¾æ•°æ®è½¬æ¢ä¸ºé˜¶æ®µäºŒï¼ˆLLMå¾®è°ƒï¼‰æ‰€éœ€çš„æ ¼å¼

é˜¶æ®µäºŒçš„ç›®æ ‡ï¼š
- GNN å·²ç»é¢„æµ‹å‡º Top-K å€™é€‰ Agent
- LLM éœ€è¦æ ¹æ®è¿™äº›å€™é€‰ Agent çš„æ—¥å¿—ï¼Œåˆ†æå‡ºæ•…éšœåŸå› 

æ•°æ®æ ¼å¼ï¼š
- instruction: åŒ…å« Top-K å€™é€‰ Agent çš„æ—¥å¿—å†…å®¹
- output: æ•…éšœåŸå› åˆ†æï¼ˆåŸºäºæ—¥å¿—å†…å®¹ï¼‰
"""

import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
import argparse
from tqdm import tqdm
import random


def extract_agent_logs(nodes: Dict[str, Any], agent_ids: List[str], history: Optional[List[Dict[str, Any]]] = None) -> Dict[str, str]:
    """
    æå–æŒ‡å®š Agent çš„æ—¥å¿—å†…å®¹
    
    Args:
        nodes: èŠ‚ç‚¹æ•°æ®å­—å…¸
        agent_ids: Agent ID åˆ—è¡¨
        history: å†å²äº‹ä»¶åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºæå–åŸå§‹å†…å®¹ï¼‰
    
    Returns:
        Dict[agent_id, log_content]
    """
    agent_logs = {}
    
    for agent_id in agent_ids:
        if agent_id in nodes:
            node_data = nodes[agent_id]
            log_content = ""
            
            # æ–¹æ³•1: ä» history ä¸­æå–è¯¥ Agent çš„æ‰€æœ‰äº‹ä»¶å†…å®¹ï¼ˆä¿ç•™ç»å¯¹Step IDï¼‰
            if history:
                agent_events = []
                for event_idx, event in enumerate(history):
                    # æ£€æŸ¥äº‹ä»¶æ˜¯å¦å±äºè¯¥ Agent
                    event_agent = event.get('name') or event.get('role') or event.get('sender', '')
                    if event_agent == agent_id or (isinstance(event_agent, str) and agent_id in event_agent):
                        content = event.get('content', '')
                        # å°è¯•ä»eventä¸­æå–Step IDï¼ˆç»å¯¹IDï¼‰
                        step_id = event.get('step', event.get('step_id', event.get('timestamp', event_idx)))
                        
                        if content:
                            # å¤„ç†ä¸åŒç±»å‹çš„content
                            if isinstance(content, str):
                                agent_events.append((step_id, content))
                            elif isinstance(content, dict):
                                # å°è¯•ä»å­—å…¸ä¸­æå–æ–‡æœ¬
                                text = content.get('text', '') or content.get('content', '') or str(content)
                                if text:
                                    agent_events.append((step_id, text))
                            else:
                                agent_events.append((step_id, str(content)))
                
                if agent_events:
                    # åˆå¹¶æ‰€æœ‰äº‹ä»¶å†…å®¹ï¼Œä½¿ç”¨ç»å¯¹Step ID
                    for step_id, event_content in agent_events:
                        if log_content:
                            log_content += f"\n[Step {step_id}]: {event_content[:500]}"
                        else:
                            log_content = f"[Step {step_id}]: {event_content[:500]}"
            
            # æ–¹æ³•2: å¦‚æœ history ä¸­æ²¡æœ‰ï¼Œå°è¯•ä» features ä¸­æå–ï¼ˆä½¿ç”¨ç»å¯¹Step IDï¼‰
            if not log_content:
                features = node_data.get('features', {})
                if isinstance(features, dict):
                    # æŒ‰æ—¶é—´æ­¥æ’åºï¼ˆtæ˜¯ç»å¯¹Step IDï¼‰
                    sorted_timesteps = sorted(features.keys(), key=lambda x: int(x) if str(x).isdigit() else 0)
                    
                    for t in sorted_timesteps:
                        feat = features[t]
                        if isinstance(feat, dict):
                            # å°è¯•å¤šç§å¯èƒ½çš„é”®å
                            content_text = (
                                feat.get('content_text', '') or 
                                feat.get('content', '') or
                                feat.get('text', '')
                            )
                            if content_text and content_text.strip():
                                # ğŸ”¥ ä½¿ç”¨ç»å¯¹Step IDï¼ˆtï¼‰ï¼Œè€Œä¸æ˜¯ç›¸å¯¹ç´¢å¼•
                                if log_content:
                                    log_content += f"\n[Step {t}]: {content_text[:500]}"
                                else:
                                    log_content = f"[Step {t}]: {content_text[:500]}"
            
            if log_content:
                agent_logs[agent_id] = log_content
            else:
                # å¦‚æœæ²¡æœ‰æ—¥å¿—ï¼Œå°è¯•ä»èŠ‚ç‚¹ç±»å‹å’Œå…¶ä»–ä¿¡æ¯æ„å»ºæè¿°
                agent_type = node_data.get('type', 'Agent')
                node_info = f"Agent {agent_id} ({agent_type})"
                features = node_data.get('features', {})
                if features:
                    node_info += f"ï¼Œæœ‰ {len(features)} ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾"
                agent_logs[agent_id] = f"{node_info}ï¼šæ— è¯¦ç»†æ—¥å¿—å†…å®¹"
    
    return agent_logs


def format_instruction_for_stage2(
    ground_truth: Dict[str, Any],
    nodes: Dict[str, Any],
    history: Optional[List[Dict[str, Any]]] = None,
    top_k: int = 3,
    include_negative: bool = True
) -> Dict[str, str]:
    """
    æ ¼å¼åŒ–é˜¶æ®µäºŒçš„æŒ‡ä»¤æ•°æ®
    
    é˜¶æ®µäºŒåœºæ™¯ï¼š
    1. GNN å·²ç»é¢„æµ‹å‡º Top-K å€™é€‰ Agentï¼ˆåŒ…å«çœŸå‡¶ï¼‰
    2. éœ€è¦ä»è¿™äº›å€™é€‰ Agent çš„æ—¥å¿—ä¸­åˆ†æå‡ºæ•…éšœåŸå› 
    
    Args:
        ground_truth: çœŸå®æ ‡ç­¾
        nodes: èŠ‚ç‚¹æ•°æ®
        top_k: Top-K å€™é€‰æ•°é‡
        include_negative: æ˜¯å¦åŒ…å«è´Ÿæ ·æœ¬ï¼ˆéæ•…éšœAgentï¼‰
    
    Returns:
        {"instruction": ..., "output": ...}
    """
    mistake_agent = ground_truth.get('mistake_agent', '')
    mistake_step = ground_truth.get('mistake_step', '')
    mistake_reason = ground_truth.get('mistake_reason', '')
    
    # å¤„ç† None å€¼ï¼ˆHealed æƒ…å†µï¼‰
    if mistake_agent is None or mistake_agent == '' or mistake_agent.lower() == 'none':
        # Healed æƒ…å†µï¼šç”Ÿæˆè´Ÿæ ·æœ¬
        if not include_negative:
            return None
        
        # éšæœºé€‰æ‹©å‡ ä¸ª Agent ä½œä¸ºå€™é€‰
        all_agents = [node_id for node_id, node_data in nodes.items() 
                     if node_data.get('type') == 'Agent']
        if len(all_agents) < top_k:
            return None
        
        candidate_agents = random.sample(all_agents, min(top_k, len(all_agents)))
        agent_logs = extract_agent_logs(nodes, candidate_agents, history)
        
        instruction = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤šAgentç³»ç»Ÿçš„è¿è¡Œåœºæ™¯ã€‚GNNæ¨¡å‹å·²ç»è¯†åˆ«å‡ºä»¥ä¸‹ {len(candidate_agents)} ä¸ªå€™é€‰Agentå¯èƒ½å­˜åœ¨å¼‚å¸¸ï¼š

"""
        for agent_id, log_content in agent_logs.items():
            instruction += f"**å€™é€‰Agent {agent_id}çš„æ—¥å¿—ï¼š**\n{log_content}\n\n"
        
        instruction += """è¯·åˆ†æè¿™äº›å€™é€‰Agentçš„æ—¥å¿—ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æ•…éšœã€‚å¦‚æœå­˜åœ¨æ•…éšœï¼Œè¯·è¯´æ˜ï¼š
1. å“ªä¸ªAgentæ˜¯æ•…éšœæº
2. æ•…éšœå‘ç”Ÿåœ¨å“ªä¸ªæ—¶é—´æ­¥
3. æ•…éšœçš„å…·ä½“åŸå› 

å¦‚æœç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œè¯·è¯´æ˜æ²¡æœ‰å‘ç°æ•…éšœã€‚"""
        
        output = "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ²¡æœ‰å‘ç°æ•…éšœã€‚è¿™æ˜¯ä¸€ä¸ªæˆåŠŸæ‰§è¡Œçš„åœºæ™¯ã€‚"
        
        return {
            "instruction": instruction,
            "output": output
        }
    
    # æ•…éšœæƒ…å†µï¼šæ„å»º Top-K å€™é€‰åˆ—è¡¨ï¼ˆåŒ…å«çœŸå‡¶ï¼‰
    all_agents = [node_id for node_id, node_data in nodes.items() 
                 if node_data.get('type') == 'Agent']
    
    if mistake_agent not in all_agents:
        # å¦‚æœçœŸå‡¶ä¸åœ¨èŠ‚ç‚¹åˆ—è¡¨ä¸­ï¼Œè·³è¿‡
        return None
    
    # æ„å»ºå€™é€‰åˆ—è¡¨ï¼šçœŸå‡¶ + å…¶ä»–éšæœºAgent
    other_agents = [a for a in all_agents if a != mistake_agent]
    if len(other_agents) < top_k - 1:
        # Agent æ•°é‡ä¸è¶³
        return None
    
    # éšæœºé€‰æ‹©å…¶ä»– Agent ä½œä¸ºå¹²æ‰°é¡¹
    negative_agents = random.sample(other_agents, top_k - 1)
    candidate_agents = [mistake_agent] + negative_agents
    # éšæœºæ‰“ä¹±é¡ºåºï¼Œæ¨¡æ‹Ÿ GNN é¢„æµ‹çš„ä¸ç¡®å®šæ€§
    random.shuffle(candidate_agents)
    
    # æå–å€™é€‰ Agent çš„æ—¥å¿—
    agent_logs = extract_agent_logs(nodes, candidate_agents, history)
    
    # æ„å»ºæŒ‡ä»¤
    instruction = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤šAgentç³»ç»Ÿçš„è¿è¡Œåœºæ™¯ã€‚GNNæ¨¡å‹å·²ç»è¯†åˆ«å‡ºä»¥ä¸‹ {len(candidate_agents)} ä¸ªå€™é€‰Agentå¯èƒ½å­˜åœ¨å¼‚å¸¸ï¼š

"""
    for i, agent_id in enumerate(candidate_agents, 1):
        log_content = agent_logs.get(agent_id, f"Agent {agent_id}: æ— æ—¥å¿—")
        instruction += f"**å€™é€‰Agent {i}: {agent_id}**\n{log_content}\n\n"
    
    instruction += """è¯·åˆ†æè¿™äº›å€™é€‰Agentçš„æ—¥å¿—ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æ•…éšœã€‚

**é‡è¦**ï¼šè¯·å…ˆè¿›è¡Œæ¨ç†åˆ†æï¼ˆä½¿ç”¨<think>æ ‡ç­¾ï¼‰ï¼Œç„¶åç»™å‡ºç»“è®ºã€‚

å¦‚æœå­˜åœ¨æ•…éšœï¼Œè¯·è¯´æ˜ï¼š
1. å“ªä¸ªAgentæ˜¯æ•…éšœæº
2. æ•…éšœå‘ç”Ÿåœ¨å“ªä¸ªæ—¶é—´æ­¥
3. æ•…éšœçš„å…·ä½“åŸå› 

å¦‚æœç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œè¯·è¯´æ˜æ²¡æœ‰å‘ç°æ•…éšœã€‚"""
    
    # ğŸ”¥ æ„å»ºCoTï¼ˆChain of Thoughtï¼‰è¾“å‡ºæ ¼å¼
    # æ ¼å¼ï¼š<think>æ¨ç†è¿‡ç¨‹</think>\næ•…éšœæºAgent: ...
    
    # 1. ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼ˆåŸºäºæ—¥å¿—å†…å®¹ï¼‰
    reasoning_parts = []
    
    if mistake_agent and mistake_agent in agent_logs:
        mistake_log = agent_logs[mistake_agent]
        
        # æå–å…³é”®ä¿¡æ¯ç”¨äºæ¨ç†
        # æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦åŒ…å«é”™è¯¯å…³é”®è¯
        error_keywords = ['error', 'fail', 'exception', 'wrong', 'incorrect', 'invalid', 'not found', 'timeout']
        has_error = any(keyword in mistake_log.lower() for keyword in error_keywords)
        
        if has_error:
            reasoning_parts.append(f"æ£€æŸ¥å€™é€‰Agentçš„æ—¥å¿—ï¼Œå‘ç° {mistake_agent} çš„æ—¥å¿—ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯")
        
        # åˆ†ææ—¶é—´æ­¥
        if mistake_step:
            # å°è¯•ä»æ—¥å¿—ä¸­æå–Stepä¿¡æ¯
            step_pattern = r'\[Step\s+(\d+)\]'
            steps_in_log = re.findall(step_pattern, mistake_log)
            if steps_in_log:
                reasoning_parts.append(f"åœ¨ {mistake_agent} çš„æ—¥å¿—ä¸­ï¼ŒStep {mistake_step} å‡ºç°äº†å¼‚å¸¸")
            else:
                reasoning_parts.append(f"æ ¹æ®æ—¥å¿—åˆ†æï¼Œæ•…éšœå‘ç”Ÿåœ¨ Step {mistake_step}")
        
        # åˆ†æå› æœé“¾ï¼ˆå¦‚æœæ˜¯éªŒè¯ç±»AgentæŠ¥é”™ï¼Œå¯èƒ½æ˜¯ä¸Šæ¸¸é—®é¢˜ï¼‰
        if 'verification' in mistake_agent.lower() or 'validation' in mistake_agent.lower():
            reasoning_parts.append(f"{mistake_agent} æ˜¯éªŒè¯ç±»Agentï¼Œå®ƒæŠ¥é”™é€šå¸¸æ„å‘³ç€ä¸Šæ¸¸Agentç”Ÿæˆäº†é”™è¯¯æ•°æ®")
            # æ£€æŸ¥å€™é€‰åˆ—è¡¨ä¸­æ˜¯å¦æœ‰æ•°æ®ç”Ÿæˆç±»Agent
            data_agents = [a for a in candidate_agents if 'data' in a.lower() or 'web' in a.lower() or 'coder' in a.lower()]
            if data_agents:
                reasoning_parts.append(f"æ£€æŸ¥ä¸Šæ¸¸Agentï¼š{', '.join(data_agents)}ï¼Œå‘ç° {mistake_agent} æ˜¯æ ¹å› ")
        else:
            reasoning_parts.append(f"åˆ†æ {mistake_agent} çš„æ—¥å¿—ï¼Œç¡®è®¤å®ƒæ˜¯æ•…éšœæº")
    else:
        reasoning_parts.append("åˆ†ææ‰€æœ‰å€™é€‰Agentçš„æ—¥å¿—ï¼Œæœªå‘ç°æ˜æ˜¾æ•…éšœ")
    
    # 2. æ„å»ºæœ€ç»ˆè¾“å‡ºï¼ˆCoTæ ¼å¼ï¼‰
    if mistake_agent:
        # æ•…éšœæƒ…å†µï¼šåŒ…å«æ¨ç†è¿‡ç¨‹
        reasoning = " ".join(reasoning_parts) if reasoning_parts else f"æ ¹æ®æ—¥å¿—åˆ†æï¼Œ{mistake_agent} æ˜¯æ•…éšœæº"
        
        output_parts = [f"<think>{reasoning}</think>"]
        output_parts.append(f"æ•…éšœæºAgent: {mistake_agent}")
        
        if mistake_step:
            output_parts.append(f"æ•…éšœæ—¶é—´æ­¥: {mistake_step}")
        
        if mistake_reason:
            output_parts.append(f"æ•…éšœåŸå› : {mistake_reason}")
        else:
            mistake_log = agent_logs.get(mistake_agent, "")
            if mistake_log:
                output_parts.append(f"æ•…éšœåŸå› : æ ¹æ®Agent {mistake_agent}çš„æ—¥å¿—åˆ†æï¼Œè¯¥Agentåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°äº†å¼‚å¸¸è¡Œä¸º")
        
        output = "\n".join(output_parts)
    else:
        # æ­£å¸¸æƒ…å†µ
        reasoning = "åˆ†ææ‰€æœ‰å€™é€‰Agentçš„æ—¥å¿—ï¼Œæœªå‘ç°é”™è¯¯ä¿¡æ¯æˆ–å¼‚å¸¸è¡Œä¸ºï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸"
        output = f"<think>{reasoning}</think>\nç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ²¡æœ‰å‘ç°æ•…éšœã€‚è¿™æ˜¯ä¸€ä¸ªæˆåŠŸæ‰§è¡Œçš„åœºæ™¯ã€‚"
    
    return {
        "instruction": instruction,
        "output": output
    }


def convert_directory_stage2(
    input_dir: str,
    output_file: str,
    top_k: int = 3,
    include_negative: bool = True,
    max_files: int = None,
    recursive: bool = False,
    resume: bool = True
):
    """
    æ‰¹é‡è½¬æ¢ç›®å½•ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶ä¸ºé˜¶æ®µäºŒæ ¼å¼
    
    Args:
        input_dir: è¾“å…¥ç›®å½•
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        top_k: Top-K å€™é€‰æ•°é‡
        include_negative: æ˜¯å¦åŒ…å«è´Ÿæ ·æœ¬
        max_files: æœ€å¤§æ–‡ä»¶æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        recursive: æ˜¯å¦é€’å½’æœç´¢å­ç›®å½•
        resume: æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼‰
    """
    input_path = Path(input_dir)
    output_path = Path(output_file)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ğŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
    processed_files = set()
    converted_data = []
    
    if resume and output_path.exists():
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                converted_data = existing_data
                print(f"ğŸ“‹ å‘ç°å·²æœ‰è¾“å‡ºæ–‡ä»¶: {output_path}")
                print(f"   å·²æœ‰æ•°æ®: {len(converted_data)} æ¡")
                
                # ä»å·²æœ‰æ•°æ®ä¸­æå–å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆé€šè¿‡instructionå†…å®¹æ¨æ–­ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡è¯†ï¼Œå› ä¸ºæ¯ä¸ªå›¾æ–‡ä»¶å¯¹åº”ä¸€æ¡æ•°æ®
                # ä½†æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œæˆ–æ–‡ä»¶å
                print(f"   âš ï¸  æ³¨æ„ï¼šæ–­ç‚¹ç»­ä¼ åŸºäºæ–‡ä»¶æ‰«æï¼Œå°†è·³è¿‡å·²è½¬æ¢çš„å›¾æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å·²æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            print(f"   å°†é‡æ–°å¼€å§‹è½¬æ¢")
            converted_data = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶
    if recursive:
        json_files = list(input_path.rglob("*_graph.json"))
    else:
        json_files = list(input_path.glob("*_graph.json"))
    
    # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶ï¼Œä»¥åŠenhancedæ–‡ä»¶
    json_files = [f for f in json_files 
                  if not f.name.startswith('.') 
                  and 'enhanced' not in f.name.lower()]
    
    if max_files:
        json_files = json_files[:max_files]
    
    # ğŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šå¦‚æœå·²æœ‰æ•°æ®ï¼Œå°è¯•è¯†åˆ«å·²å¤„ç†çš„æ–‡ä»¶
    if resume and converted_data:
        # æ–¹æ³•ï¼šé€šè¿‡æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ä¸­çš„å›¾æ–‡ä»¶æ•°é‡æ¥æ¨æ–­
        # å¦‚æœå·²æœ‰Næ¡æ•°æ®ï¼Œå‡è®¾å‰Nä¸ªæ–‡ä»¶å·²å¤„ç†ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰
        # æ›´å®‰å…¨çš„æ–¹æ³•ï¼šè®°å½•å·²å¤„ç†çš„æ–‡ä»¶å
        print(f"   ğŸ’¡ å°†è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶")
    
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªå›¾æ–‡ä»¶ï¼ˆJSONï¼‰")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ğŸ“‹ Top-K: {top_k}")
    print(f"ğŸ“‹ åŒ…å«è´Ÿæ ·æœ¬: {include_negative}")
    print(f"ğŸ“‹ æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    print()
    
    # ğŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šè®¡ç®—éœ€è¦å¤„ç†çš„æ–‡ä»¶
    if resume and converted_data:
        # å¦‚æœå·²æœ‰æ•°æ®ï¼Œå‡è®¾å‰Nä¸ªæ–‡ä»¶å·²å¤„ç†ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰
        sorted_files = sorted(json_files, key=lambda x: x.name)
        already_processed_count = len(converted_data)
        if already_processed_count < len(sorted_files):
            # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
            files_to_process = sorted_files[already_processed_count:]
            print(f"ğŸ“‹ å·²å¤„ç†: {already_processed_count} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“‹ å¾…å¤„ç†: {len(files_to_process)} ä¸ªæ–‡ä»¶")
        else:
            files_to_process = []
            print(f"âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
    else:
        files_to_process = json_files
    
    failed_count = 0
    skipped_count = 0
    new_converted_count = 0
    
    # è½¬æ¢æ¯ä¸ªæ–‡ä»¶
    for json_file in tqdm(files_to_process, desc="è½¬æ¢ä¸­"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                graph_data = json.load(f)
            
            # è½¬æ¢ä¸ºé˜¶æ®µäºŒæ ¼å¼
            converted = format_instruction_for_stage2(
                ground_truth=graph_data.get('ground_truth', {}),
                nodes=graph_data.get('nodes', {}),
                history=graph_data.get('history', None),
                top_k=top_k,
                include_negative=include_negative
            )
            
            if converted is None:
                skipped_count += 1
                continue
            
            converted_data.append(converted)
            new_converted_count += 1
            
        except Exception as e:
            failed_count += 1
            if failed_count <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                print(f"âš ï¸  è½¬æ¢å¤±è´¥ {json_file.name}: {e}")
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜è½¬æ¢ç»“æœ...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è½¬æ¢å®Œæˆï¼")
    print(f"   æ€»è®¡: {len(converted_data)} æ¡")
    if resume and new_converted_count > 0:
        print(f"   æœ¬æ¬¡æ–°å¢: {new_converted_count} æ¡")
    print(f"   è·³è¿‡: {skipped_count} æ¡")
    print(f"   å¤±è´¥: {failed_count} æ¡")
    print(f"   è¾“å‡º: {output_path}")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if converted_data:
        print(f"\nğŸ“„ æ•°æ®ç¤ºä¾‹ï¼ˆå‰2æ¡ï¼‰:")
        for i, item in enumerate(converted_data[:2], 1):
            print(f"\n--- ç¤ºä¾‹ {i} ---")
            print(f"instruction (å‰200å­—ç¬¦): {item['instruction'][:200]}...")
            print(f"output: {item['output']}")


def main():
    parser = argparse.ArgumentParser(description="å°†å›¾æ•°æ®è½¬æ¢ä¸ºé˜¶æ®µäºŒï¼ˆLLMå¾®è°ƒï¼‰æ ¼å¼")
    parser.add_argument("--input_dir", type=str, default="processed_graphs/graphs_astra_v3_backup",
                       help="è¾“å…¥ç›®å½•ï¼ˆåŒ…å«å›¾æ•°æ® JSON æ–‡ä»¶ï¼‰")
    parser.add_argument("--output_file", type=str, default="data/qwen3_stage2_finetune_data.json",
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--top_k", type=int, default=3,
                       help="Top-K å€™é€‰æ•°é‡ï¼ˆé»˜è®¤3ï¼‰")
    parser.add_argument("--include_negative", action="store_true", default=True,
                       help="æ˜¯å¦åŒ…å«è´Ÿæ ·æœ¬ï¼ˆHealedæƒ…å†µï¼‰")
    parser.add_argument("--max_files", type=int, default=None,
                       help="æœ€å¤§è½¬æ¢æ–‡ä»¶æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--recursive", action="store_true",
                       help="é€’å½’æœç´¢å­ç›®å½•ä¸­çš„ JSON æ–‡ä»¶")
    parser.add_argument("--no-resume", action="store_true",
                       help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°å¼€å§‹è½¬æ¢ï¼‰")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    input_path = Path(args.input_dir)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        print(f"   è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    # è½¬æ¢
    convert_directory_stage2(
        input_dir=args.input_dir,
        output_file=args.output_file,
        top_k=args.top_k,
        include_negative=args.include_negative,
        max_files=args.max_files,
        recursive=args.recursive,
        resume=not args.no_resume  # é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­ä¼ 
    )
    
    print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
    print(f"   1. æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶: {args.output_file}")
    print(f"   2. å¼€å§‹é˜¶æ®µäºŒå¾®è°ƒ:")
    print(f"      python finetune_qwen3_4b.py --model_name models\\Qwen1.5-4B-Chat --data_path {args.output_file} --no_quantization --batch_size 2")


if __name__ == "__main__":
    main()









