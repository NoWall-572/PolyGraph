"""
ASTRA-MoE è®­ç»ƒè„šæœ¬

å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. WhoWhenDataset - æ•°æ®é›†ç±»ï¼ˆå¤„ç†å˜é•¿åºåˆ—å’ŒAgentï¼‰
2. è‡ªå®šä¹‰ collate_fn - æ‰¹å¤„ç†å¯¹é½
3. è®­ç»ƒä¸»å¾ªç¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from tqdm import tqdm
import os
import sys
from datetime import datetime

from astra.data.adapter import GraphDataConverter, reconstruct_graph_from_json
from astra.model.gnn import ASTRAMoE
from astra.model.loss import ASTRALoss, SupConLoss, ASTRAContrastiveLoss
from astra.data.graph_data import HeteroGraph
import random

# --- å¼ºåˆ¶ä¿®å¤ GPU ç¯å¢ƒå˜é‡ ---
if os.environ.get("CUDA_VISIBLE_DEVICES") == "":
    print("âš ï¸ æ£€æµ‹åˆ° CUDA_VISIBLE_DEVICES ä¸ºç©ºï¼Œæ­£åœ¨å¼ºåˆ¶æ¸…é™¤ä»¥æ¢å¤ GPU...")
    del os.environ["CUDA_VISIBLE_DEVICES"]
print("\n" + "="*60)
print("ğŸ” æ·±åº¦ç¯å¢ƒè¯Šæ–­ (Deep Diagnostic)")
print("="*60)
print(f"Python è·¯å¾„: {sys.executable}")
print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½® (Not Set)')}")
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA æ˜¯å¦ç¼–è¯‘: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"å½“å‰è®¾å¤‡ç´¢å¼•: {torch.cuda.current_device()}")
    print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")
else:
    print("âŒ torch.cuda.is_available() è¿”å› False")
print("="*60 + "\n")
# ========================


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—ç®¡ç†å™¨ï¼šå°†è¯¦ç»†æ—¥å¿—ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç»ˆç«¯åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯"""
    def __init__(self, log_dir: Path):
        self.log_dir = log_dir
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"training_log_{timestamp}.txt"
        self.file_handle = open(self.log_file, 'w', encoding='utf-8')
        
    def log(self, message: str, to_terminal: bool = False):
        """è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶ï¼Œå¯é€‰æ˜¯å¦åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯"""
        # å§‹ç»ˆå†™å…¥æ–‡ä»¶
        self.file_handle.write(message + '\n')
        self.file_handle.flush()
        
        # æ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è¾“å‡ºåˆ°ç»ˆç«¯
        if to_terminal:
            print(message, flush=True)
    
    def log_epoch_metrics(self, epoch: int, total_epochs: int, 
                          train_metrics: Dict[str, float], 
                          val_metrics: Dict[str, float],
                          lr: float = None):
        """è®°å½• epoch è¯„ä¼°æŒ‡æ ‡ï¼ˆç»ˆç«¯æ˜¾ç¤ºç®€æ´ç‰ˆï¼Œæ–‡ä»¶ä¿å­˜è¯¦ç»†ç‰ˆï¼‰"""
        # ç»ˆç«¯è¾“å‡ºï¼šç®€æ´æ ¼å¼ï¼Œåªæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        terminal_msg = f"\n{'='*80}"
        terminal_msg += f"\nEpoch {epoch+1}/{total_epochs}"
        if lr is not None:
            terminal_msg += f" | LR: {lr:.2e}"
        terminal_msg += f"\n{'='*80}"
        terminal_msg += f"\nè®­ç»ƒé›† - Loss: {train_metrics['loss']:.6f}"
        terminal_msg += f"\n         Agent Acc: {train_metrics['agent_accuracy']:.4f} ({train_metrics['agent_accuracy']*100:.2f}%)"
        terminal_msg += f"\n         Step Acc:  {train_metrics['step_accuracy']:.4f} ({train_metrics['step_accuracy']*100:.2f}%)"
        if 'agent_loss' in train_metrics:
            terminal_msg += f"\n         Agent Loss: {train_metrics['agent_loss']:.6f}"
        if 'step_loss' in train_metrics:
            terminal_msg += f"\n         Step Loss:  {train_metrics['step_loss']:.6f}"
        if 'cl_loss' in train_metrics:
            terminal_msg += f"\n         CL Loss:    {train_metrics['cl_loss']:.6f}"
        if 'rl_loss' in train_metrics:
            terminal_msg += f"\n         RL Loss:    {train_metrics['rl_loss']:.6f}"
        terminal_msg += f"\néªŒè¯é›† - Loss: {val_metrics['loss']:.6f}"
        terminal_msg += f"\n         Agent Acc: {val_metrics['agent_accuracy']:.4f} ({val_metrics['agent_accuracy']*100:.2f}%)"
        terminal_msg += f"\n         Step Acc:  {val_metrics['step_accuracy']:.4f} ({val_metrics['step_accuracy']*100:.2f}%)"
        terminal_msg += f"\n{'='*80}\n"
        
        # æ–‡ä»¶è¾“å‡ºï¼šè¯¦ç»†æ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰æŒ‡æ ‡ï¼‰
        file_msg = f"\n{'='*80}"
        file_msg += f"\nEpoch {epoch+1}/{total_epochs}"
        if lr is not None:
            file_msg += f" | LR: {lr:.2e}"
        file_msg += f"\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        file_msg += f"\n{'='*80}"
        file_msg += f"\nè®­ç»ƒé›†æŒ‡æ ‡:"
        for key, value in sorted(train_metrics.items()):
            file_msg += f"\n  {key}: {value:.6f}"
        file_msg += f"\néªŒè¯é›†æŒ‡æ ‡:"
        for key, value in sorted(val_metrics.items()):
            file_msg += f"\n  {key}: {value:.6f}"
        file_msg += f"\n{'='*80}\n"
        
        # è¾“å‡ºåˆ°ç»ˆç«¯ï¼ˆç®€æ´ç‰ˆï¼‰
        print(terminal_msg, flush=True)
        # å†™å…¥æ–‡ä»¶ï¼ˆè¯¦ç»†ç‰ˆï¼‰
        self.file_handle.write(file_msg)
        self.file_handle.flush()
    
    def close(self):
        """å…³é—­æ—¥å¿—æ–‡ä»¶"""
        if self.file_handle:
            self.file_handle.close()
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æ–‡ä»¶è¢«å…³é—­"""
        self.close()


def seed_everything(seed: int = 42):
    """
    å›ºå®šæ‰€æœ‰éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°

    Args:
        seed: éšæœºç§å­å€¼
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # ç¡®ä¿ CUDA æ“ä½œçš„ç¡®å®šæ€§ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


class WhoWhenDataset(Dataset):
    """
    å¤šæ™ºèƒ½ä½“æ•…éšœå½’å› æ•°æ®é›† (ä¼˜åŒ–ç‰ˆ)
    1. ä¿®å¤ JSON Extra data é”™è¯¯
    2. é‡‡ç”¨åˆ†ç‰‡ç¼“å­˜ç­–ç•¥ (é¿å…ç”Ÿæˆ 40GB çš„å·¨å‹ .pt æ–‡ä»¶)
    """

    def __init__(self,
                 data_dir: str = "outputs",
                 max_seq_len: int = 160,
                 max_agents: int = 50,
                 processed_dir: Optional[str] = None,
                 force_reprocess: bool = False,
                 enable_pairing: bool = True):
        self.data_dir = Path(data_dir)
        self.max_seq_len = max_seq_len
        self.max_agents = max_agents
        self.processed_dir = Path(processed_dir) if processed_dir else None
        self.force_reprocess = force_reprocess
        self.enable_pairing = enable_pairing

        # æŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶
        graph_files = list(self.data_dir.rglob("*_graph.json"))
        new_format_files = list(self.data_dir.rglob("*.json"))
        all_files = set(graph_files + new_format_files)
        self.json_files = sorted(list(all_files))

        if not self.json_files:
            raise ValueError(f"åœ¨ {data_dir} åŠå…¶å­ç›®å½•ä¸­æœªæ‰¾åˆ° JSON æ–‡ä»¶")

        print(f"æ‰¾åˆ° {len(self.json_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        # æ•°æ®è½¬æ¢å™¨
        self.converter = GraphDataConverter(node_feat_dim=8192, edge_feat_dim=32)  # ğŸ”¥ Qwen3-8B: 4096 (åµŒå…¥) + 4096 (å…ƒæ•°æ®)

        # å†…å­˜ä¸­çš„æ•°æ®åˆ—è¡¨ (ä»…å­˜å‚¨å·²åŠ è½½çš„æ•°æ®ç´¢å¼•ï¼Œä¸ºäº†èŠ‚çœå†…å­˜)
        # å®é™…çš„å¤§ Tensor å»ºè®®åœ¨ __getitem__ æ—¶åŠ è½½ï¼Œæˆ–è€…å¦‚æœå†…å­˜å¤Ÿå¤§(64G+)ä¹Ÿå¯ä»¥å­˜å†…å­˜
        # è¿™é‡Œä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬å°†æ•°æ®ç¼“å­˜åœ¨å†…å­˜ä¸­ (self.data_cache)ï¼Œä½†é€šè¿‡åˆ†ç‰‡å†™å…¥ç£ç›˜é˜²æ­¢ crash
        self.data_cache = [None] * len(self.json_files)
        
        # åˆå§‹åŒ–å¤„ç†
        self._init_processing()
        
        # é…å¯¹é€»è¾‘
        if self.enable_pairing:
            self._pair_files()

    def _init_processing(self):
        """åˆå§‹åŒ–å¤„ç†æµç¨‹ï¼šåŠ è½½è½¬æ¢å™¨ï¼Œå¹¶è¿›è¡Œåˆ†ç‰‡å¤„ç†"""
        
        # 1. å°è¯•åŠ è½½æˆ–æ‹Ÿåˆ Converter
        global_converter_path = Path("processed_data/converter_state.pt")
        
        if global_converter_path.exists():
            print(f"\nğŸ”¥ [Dataset] å‘ç°å…¨å±€ Converter: {global_converter_path}")
            self.converter = torch.load(global_converter_path, weights_only=False)
            print("   âœ… å…¨å±€ Converter åŠ è½½æˆåŠŸ")
        else:
            print("\nâš ï¸ [Dataset] æœªæ‰¾åˆ°å…¨å±€ Converterï¼Œæ­£åœ¨ç°åœºæ‹Ÿåˆ...")
            all_graphs = []
            for json_file in tqdm(self.json_files, desc="æ‹Ÿåˆ Converter"):
                graph = self._safe_load_json(json_file) # ä½¿ç”¨å®‰å…¨åŠ è½½
                if graph: all_graphs.append(graph)
            self.converter.fit(all_graphs)
            # ä¿å­˜æ‹Ÿåˆå¥½çš„ converter
            if self.processed_dir:
                self.processed_dir.mkdir(parents=True, exist_ok=True)
                torch.save(self.converter, self.processed_dir / "converter_state.pt")

        # 2. åˆ†ç‰‡å¤„ç†æ•°æ® (ä¸å†ç”Ÿæˆå·¨å‹ .pt æ–‡ä»¶)
        print(f"\nğŸš€ å¼€å§‹æ•°æ®åŠ è½½ä¸ç¼“å­˜ (åˆ†ç‰‡æ¨¡å¼)...")
        # åˆ›å»ºç¼“å­˜å­ç›®å½•
        cache_dir = self.processed_dir / "cache" if self.processed_dir else Path("processed_data/cache")
        cache_dir.mkdir(parents=True, exist_ok=True)
            
        success_count = 0
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        import sys
        for idx, json_file in enumerate(tqdm(self.json_files, desc="Processing")):
            # ğŸ”¥ æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼ˆæ¯100ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡ï¼Œ37%é™„è¿‘è¯¦ç»†æ‰“å°ï¼‰
            if idx > 0 and idx % 100 == 0:
                print(f"\n[è¿›åº¦] å·²å¤„ç† {idx}/{len(self.json_files)} ä¸ªæ–‡ä»¶ ({idx*100//len(self.json_files)}%)", flush=True)
                print(f"  æˆåŠŸ: {success_count}, å¤±è´¥: {idx - success_count}", flush=True)
            # 37%é™„è¿‘ï¼ˆ3500-3650ï¼‰ç‰¹åˆ«å…³æ³¨
            if 3500 <= idx <= 3650 and idx % 10 == 0:
                print(f"\n[37%åŒºåŸŸ] å¤„ç†æ–‡ä»¶ {idx}/{len(self.json_files)}: {json_file.name}", flush=True)
                sys.stdout.flush()
            
            # è®¡ç®—è¯¥æ–‡ä»¶çš„ç¼“å­˜è·¯å¾„: processed_data/cache/{filename}.pt
            cache_name = f"{json_file.stem}.pt"
            cache_path = cache_dir / cache_name
            
            # ğŸ”¥ å…³é”®è°ƒè¯•ï¼š3600é™„è¿‘å¼ºåˆ¶æ‰“å°ï¼ˆåœ¨cache_pathå®šä¹‰åï¼‰
            if 3595 <= idx <= 3605:
                print(f"\n[å…³é”®è°ƒè¯•] idx={idx}, file={json_file.name}, cache_path={cache_path}, exists={cache_path.exists()}", flush=True)
                sys.stdout.flush()
            
            # A. å°è¯•ä»åˆ†ç‰‡ç¼“å­˜åŠ è½½
            if cache_path.exists() and not self.force_reprocess:
                try:
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸åŠ è½½åˆ°å†…å­˜ï¼Œåªå­˜å‚¨ç¼“å­˜è·¯å¾„ï¼ˆèŠ‚çœå†…å­˜ï¼‰
                    # æ•°æ®å°†åœ¨ __getitem__ æ—¶æŒ‰éœ€åŠ è½½
                    self.data_cache[idx] = cache_path  # å­˜å‚¨è·¯å¾„è€Œä¸æ˜¯æ•°æ®
                    success_count += 1
                    continue
                except Exception as e:
                    # ğŸ”¥ ä¿®å¤ï¼šè®°å½•ç¼“å­˜åŠ è½½å¤±è´¥çš„åŸå› ï¼ˆä½†ä¸ä¸­æ–­ï¼‰
                    if idx < 10 or (3500 <= idx <= 3650):
                        print(f"  âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥ {json_file.name}: {type(e).__name__}: {str(e)[:100]}", flush=True)
                    pass # åŠ è½½å¤±è´¥åˆ™é‡æ–°å¤„ç†

            # B. é‡æ–°å¤„ç†
            try:
                # 1. åŠ è½½ JSON
                graph = self._safe_load_json(json_file)
                if not graph: 
                    if idx < 5:  # åªæ‰“å°å‰5ä¸ªå¤±è´¥çš„æ–‡ä»¶
                        print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ï¼ˆåŠ è½½å¤±è´¥ï¼‰: {json_file.name}")
                    continue

                # 2. è½¬æ¢ï¼ˆåœ¨è½¬æ¢å‰å†æ¬¡éªŒè¯ _fitted çŠ¶æ€ï¼‰
                if not hasattr(self.converter, '_fitted') or not self.converter._fitted:
                    raise RuntimeError(f"Converter æœªæ‹Ÿåˆï¼è¯·åœ¨åŠ è½½åè°ƒç”¨ fit() æ–¹æ³•ã€‚_fitted={getattr(self.converter, '_fitted', 'N/A')}")
                graph_list, labels = self.converter.convert(graph)
                
                # æ£€æŸ¥è½¬æ¢ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if not graph_list or len(graph_list) == 0:
                    if idx < 5:
                        print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ï¼ˆè½¬æ¢åä¸ºç©ºï¼‰: {json_file.name}")
                    continue
                
                sample_data = {
                    'graph_list': graph_list,
                    'labels': labels,
                    'source_file': str(json_file)
                }
                
                # 3. ç«‹å³å†™å…¥åˆ†ç‰‡ç¼“å­˜ (é˜²æ­¢ç¨‹åºå´©æºƒå¯¼è‡´æ•°æ®ä¸¢å¤±)
                torch.save(sample_data, cache_path)
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸å­˜å…¥å†…å­˜ï¼Œåªå­˜å‚¨ç¼“å­˜è·¯å¾„ï¼ˆèŠ‚çœå†…å­˜ï¼‰
                # æ•°æ®å°†åœ¨ __getitem__ æ—¶æŒ‰éœ€åŠ è½½
                self.data_cache[idx] = cache_path  # å­˜å‚¨è·¯å¾„è€Œä¸æ˜¯æ•°æ®
                success_count += 1
                    
            except Exception as e:
                # ğŸ”¥ ä¿®å¤ï¼šæ‰“å°é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                error_msg = str(e)
                error_type = type(e).__name__
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼š37%é™„è¿‘ï¼ˆ3600å·¦å³ï¼‰çš„æ–‡ä»¶é”™è¯¯è¦è¯¦ç»†æ‰“å°
                should_print_detail = idx < 10 or (3500 <= idx <= 3650)
                
                if should_print_detail:
                    print(f"\nâŒ [æ–‡ä»¶ {idx}/{len(self.json_files)}] å¤„ç†æ–‡ä»¶å¤±è´¥: {json_file.name}")
                    print(f"   é”™è¯¯ç±»å‹: {error_type}")
                    print(f"   é”™è¯¯ä¿¡æ¯: {error_msg[:500]}")
                    # å¦‚æœæ˜¯å…³é”®é”™è¯¯ï¼Œæ‰“å°å®Œæ•´å †æ ˆ
                    if idx < 3 or (3500 <= idx <= 3650):
                        import traceback
                        print(f"   å †æ ˆè·Ÿè¸ª:")
                        traceback.print_exc()
                elif idx == 10:
                    print(f"\n   ... (åç»­é”™è¯¯å°†ä¸å†æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œä½†37%é™„è¿‘ä¼šè¯¦ç»†æ˜¾ç¤º)")
                elif idx == 3651:
                    print(f"\n   ... (37%åŒºåŸŸæ£€æŸ¥å®Œæˆ)")
                
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
                continue

        # æ¸…ç† None å€¼ (å¤„ç†å¤±è´¥çš„æ ·æœ¬)
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ç¼“å­˜è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœå­˜å‚¨çš„æ˜¯è·¯å¾„ï¼‰
        from pathlib import Path as PathType
        self.valid_indices = []
        for i, d in enumerate(self.data_cache):
            if d is not None:
                # å¦‚æœæ˜¯Pathå¯¹è±¡ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if isinstance(d, (Path, PathType)):
                    if d.exists():
                        self.valid_indices.append(i)
                    else:
                        # ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
                        self.data_cache[i] = None
                else:
                    # æ˜¯æ•°æ®å¯¹è±¡ï¼Œç›´æ¥æ·»åŠ 
                    self.valid_indices.append(i)
        
        # ğŸ”¥ æ·»åŠ è¯¦ç»†ç»Ÿè®¡
        failed_count = len(self.json_files) - len(self.valid_indices)
        if failed_count > 0:
            print(f"\nâš ï¸  è­¦å‘Š: {failed_count} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
            if failed_count == len(self.json_files):
                print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼")
                print(f"   å¯èƒ½çš„åŸå› :")
                print(f"   1. æ•°æ®æ ¼å¼é—®é¢˜")
                print(f"   2. å†…å­˜ä¸è¶³")
                print(f"   3. Converter æœªæ­£ç¡®æ‹Ÿåˆ")
                print(f"   è¯·æ£€æŸ¥å‰å‡ ä¸ªæ–‡ä»¶çš„é”™è¯¯ä¿¡æ¯")
        
        # ğŸ”¥ æ·»åŠ è¯¦ç»†ç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"æ•°æ®åŠ è½½å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ–‡ä»¶æ•°: {len(self.json_files)}")
        print(f"æˆåŠŸå¤„ç†: {len(self.valid_indices)}")
        failed_count = len(self.json_files) - len(self.valid_indices)
        print(f"å¤„ç†å¤±è´¥: {failed_count}")
        
        if failed_count > 0:
            print(f"\nâš ï¸  è­¦å‘Š: {failed_count} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
            if failed_count == len(self.json_files):
                print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ï¼")
                print(f"   å¯èƒ½çš„åŸå› :")
                print(f"   1. æ•°æ®æ ¼å¼é—®é¢˜")
                print(f"   2. å†…å­˜ä¸è¶³")
                print(f"   3. Converter æœªæ­£ç¡®æ‹Ÿåˆ")
                print(f"   è¯·æ£€æŸ¥å‰å‡ ä¸ªæ–‡ä»¶çš„é”™è¯¯ä¿¡æ¯")
        else:
            print(f"âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†æˆåŠŸï¼")
        
        print(f"{'='*60}\n")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®è®¤æ•°æ®åŠ è½½å®Œæˆ
        if len(self.valid_indices) == 0:
            raise RuntimeError("âŒ ä¸¥é‡é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬ï¼æ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†å¤±è´¥ã€‚")
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼æœ‰æ•ˆæ ·æœ¬æ•°: {len(self.valid_indices)}", flush=True)
        print(f"   å·²å¤„ç†æ–‡ä»¶æ•°: {success_count}/{len(self.json_files)}", flush=True)
        print(f"   å°†ä½¿ç”¨è¿™äº›æ ·æœ¬è¿›è¡Œè®­ç»ƒ...", flush=True)
        import sys
        sys.stdout.flush()
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†äº†
        if success_count < len(self.json_files) * 0.9:
            print(f"âš ï¸  è­¦å‘Šï¼šåªå¤„ç†äº† {success_count}/{len(self.json_files)} ä¸ªæ–‡ä»¶ï¼ˆ{success_count*100//len(self.json_files)}%ï¼‰", flush=True)
            print(f"   å¦‚æœç¨‹åºåœ¨æ•°æ®åŠ è½½é˜¶æ®µåœæ­¢ï¼Œå¯èƒ½æ˜¯å†…å­˜ä¸è¶³æˆ–æ–‡ä»¶å¤„ç†å¤±è´¥", flush=True)
        else:
            print(f"âœ… å·²å¤„ç† {success_count}/{len(self.json_files)} ä¸ªæ–‡ä»¶ï¼ˆ{success_count*100//len(self.json_files)}%ï¼‰", flush=True)

    def _safe_load_json(self, json_path: Path):
        """ğŸ”¥ ä¿®å¤ 'Extra data' é”™è¯¯çš„å¥å£® JSON åŠ è½½å™¨"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            try:
                # å°è¯•æ ‡å‡†åŠ è½½
                data = json.loads(content)
            except json.JSONDecodeError:
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœåŒ…å«é¢å¤–æ•°æ®ï¼Œåªè¯»å–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡
                decoder = json.JSONDecoder()
                data, _ = decoder.raw_decode(content)
            
            return reconstruct_graph_from_json(data)
        except Exception as e:
            # print(f"åŠ è½½å¤±è´¥ {json_path.name}: {e}")
            return None

    def _pair_files(self):
        """[æœ€ç»ˆä¿®å¤ç‰ˆ] è‡ªåŠ¨é…å¯¹ Mutated å’Œ Healed æ–‡ä»¶"""
        pairs = []
        # å»ºç«‹ filename -> cache_index çš„æ˜ å°„
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜ å°„æœ‰æ•ˆçš„ã€å·²åŠ è½½çš„æ•°æ®
        name_to_idx = {}
        for list_idx in self.valid_indices:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†Pathå¯¹è±¡ï¼ˆç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼‰
            cache_item = self.data_cache[list_idx]
            
            # å¦‚æœæ˜¯Pathå¯¹è±¡ï¼Œä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®
            if isinstance(cache_item, Path):
                try:
                    data = torch.load(cache_item, weights_only=False)
                    source_file = data.get('source_file', str(cache_item))
                except Exception as e:
                    print(f"âš ï¸  é…å¯¹æ—¶åŠ è½½ç¼“å­˜å¤±è´¥ {cache_item}: {e}", flush=True)
                    continue
            else:
                # å¦‚æœå·²ç»æ˜¯æ•°æ®å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                source_file = cache_item.get('source_file', '')
            
            file_path = Path(source_file)
            name_to_idx[file_path.name] = list_idx

        # æ‰¾å‡ºæ‰€æœ‰çš„ fatal æ–‡ä»¶
        fatal_files = [f for f in self.json_files if "_fatal_" in f.name]
        
        count_paired = 0
        count_unpaired = 0
        
        for mut_path in fatal_files:
            mut_name = mut_path.name
            
            # å¦‚æœè¿™ä¸ª fatal æ–‡ä»¶æœ¬èº«æ²¡åŠ è½½æˆåŠŸï¼Œè·³è¿‡
            if mut_name not in name_to_idx:
                continue
                
            mut_idx = name_to_idx[mut_name]
            pos_idx = None
            
            # ç­–ç•¥ï¼šç›´æ¥å­—ç¬¦ä¸²æ›¿æ¢æŸ¥æ‰¾
            healed_name = mut_name.replace("_fatal_", "_healed_")
            golden_name = mut_name.replace("_fatal_", "_golden_")
            
            if healed_name in name_to_idx:
                pos_idx = name_to_idx[healed_name]
            elif golden_name in name_to_idx:
                pos_idx = name_to_idx[golden_name]
            
            if pos_idx is not None:
                pairs.append((mut_idx, pos_idx))
                count_paired += 1
            else:
                pairs.append((mut_idx, None))
                count_unpaired += 1

        print("="*60)
        print(f"ğŸ”¥ ASTRA-Gen æ•°æ®é…å¯¹ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸé…å¯¹: {count_paired}")
        print(f"   âš ï¸ æ— é…å¯¹: {count_unpaired}")
        print("="*60)
        
        self.pairs = pairs

    def __len__(self):
        if self.enable_pairing and hasattr(self, 'pairs') and self.pairs:
            return len(self.pairs)
        return len(self.valid_indices)

    def __getitem__(self, idx):
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä»ç¼“å­˜è·¯å¾„åŠ è½½æ•°æ®ï¼ˆå¦‚æœå­˜å‚¨çš„æ˜¯è·¯å¾„ï¼‰
        from pathlib import Path as PathType
        def _load_data(cache_item):
            if cache_item is None:
                return None
            # å¦‚æœæ˜¯Pathå¯¹è±¡ï¼Œä»æ–‡ä»¶åŠ è½½
            if isinstance(cache_item, (Path, PathType)):
                try:
                    return torch.load(cache_item, weights_only=False)
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥ {cache_item}: {e}", flush=True)
                    return None
            # å¦‚æœå·²ç»æ˜¯æ•°æ®ï¼Œç›´æ¥è¿”å›
            return cache_item
        
        if self.enable_pairing and hasattr(self, 'pairs') and self.pairs:
            mut_idx, healed_idx = self.pairs[idx]
            
            # ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼ˆå¯èƒ½æ˜¯è·¯å¾„æˆ–æ•°æ®ï¼‰
            data_mut = _load_data(self.data_cache[mut_idx])
            if data_mut is None:
                raise RuntimeError(f"æ— æ³•åŠ è½½mutatedæ•°æ®: index={mut_idx}")
            
            data_healed = None
            if healed_idx is not None:
                # ä»ç¼“å­˜åŠ è½½healedæ•°æ®
                data_healed_raw = _load_data(self.data_cache[healed_idx])
                if data_healed_raw is not None:
                    # æµ…æ‹·è´é¿å…ä¿®æ”¹åŸå§‹ç¼“å­˜
                    import copy
                    data_healed = copy.deepcopy(data_healed_raw)
                    data_healed['labels'] = {
                        'y_agent': -100,
                        'y_step': -100,
                        'mistake_agent_name': '',
                        'mistake_step_str': ''
                    }
            
            return {'mutated': data_mut, 'healed': data_healed}
        else:
            # å•æ ·æœ¬æ¨¡å¼
            real_idx = self.valid_indices[idx]
            data = _load_data(self.data_cache[real_idx])
            if data is None:
                raise RuntimeError(f"æ— æ³•åŠ è½½æ•°æ®: index={real_idx}")
            return data


def collate_fn(batch: List[Dict[str, Any]],
                max_seq_len: int = 160,  # Updated: test data max length is 130, set to 160 with margin
                max_agents: int = 10,
                is_paired: bool = False) -> Dict[str, torch.Tensor]:
    """
    è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°

    å°†å˜é•¿çš„å›¾åºåˆ—å’Œå˜é•¿çš„ Agent æ•°é‡å¯¹é½åˆ°å›ºå®šç»´åº¦

    Args:
        batch: æ‰¹æ¬¡æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ Dataset.__getitem__ çš„è¿”å›å€¼
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        max_agents: æœ€å¤§ Agent æ•°é‡
        is_paired: æ˜¯å¦ä¸ºé…å¯¹æ•°æ®æ¨¡å¼

    Returns:
        æ‰¹å¤„ç†åçš„æ•°æ®å­—å…¸ï¼ŒåŒ…å«ï¼š
            - 'graph_list': List[List[HeteroGraph]] åŸå§‹å›¾åˆ—è¡¨ï¼ˆç”¨äºæ¨¡å‹è¾“å…¥ï¼‰
            - 'y_agent': [B, max_agents] Agent æ•…éšœæ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰
            - 'y_step': [B] æ•…éšœæ—¶é—´æ­¥
            - 'agent_mask': [B, max_agents] Agent æ©ç 
            - 'seq_mask': [B, max_seq_len] åºåˆ—æ©ç 
            - (å¦‚æœ is_paired=True) 'healed_graph_list': List[List[HeteroGraph]] Healed å›¾åˆ—è¡¨
            - (å¦‚æœ is_paired=True) 'healed_y_agent': [B, max_agents] Healed æ ‡ç­¾ï¼ˆå…¨ä¸º -100ï¼‰
            - (å¦‚æœ is_paired=True) 'healed_y_step': [B] Healed æ—¶é—´æ­¥ï¼ˆå…¨ä¸º -100ï¼‰
            - (å¦‚æœ is_paired=True) 'healed_agent_mask': [B, max_agents] Healed Agent æ©ç 
            - (å¦‚æœ is_paired=True) 'healed_seq_mask': [B, max_seq_len] Healed åºåˆ—æ©ç 
    """
    batch_size = len(batch)
    
    # å¦‚æœæ˜¯é…å¯¹æ¨¡å¼ï¼Œéœ€è¦åˆ†åˆ«å¤„ç† mutated å’Œ healed
    if is_paired:
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥batchä¸­çš„æ ·æœ¬æ˜¯å¦çœŸçš„æ˜¯é…å¯¹æ ¼å¼
        if batch and isinstance(batch[0], dict) and 'mutated' in batch[0]:
            # æå– mutated å’Œ healed æ•°æ®
            mutated_batch = [item['mutated'] for item in batch]
            healed_batch = [item['healed'] for item in batch if item.get('healed') is not None]
        else:
            # å¦‚æœæ²¡æœ‰é…å¯¹æ•°æ®ï¼Œé™çº§ä¸ºå•æ ·æœ¬æ¨¡å¼
            return _collate_single(batch, max_seq_len, max_agents)
        
        # å¤„ç† mutated æ•°æ®ï¼ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼‰
        mutated_collated = _collate_single(mutated_batch, max_seq_len, max_agents)
        
        # å¤„ç† healed æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if healed_batch:
            healed_collated = _collate_single(healed_batch, max_seq_len, max_agents)
            # åˆå¹¶ç»“æœ
            result = mutated_collated.copy()
            result['healed_graph_list'] = healed_collated['graph_list']
            result['healed_y_agent'] = healed_collated['y_agent']
            result['healed_y_step'] = healed_collated['y_step']
            result['healed_agent_mask'] = healed_collated['agent_mask']
            result['healed_seq_mask'] = healed_collated['seq_mask']
            return result
        else:
            # æ²¡æœ‰ healed æ•°æ®ï¼Œåªè¿”å› mutated
            return mutated_collated
    else:
        # å•æ ·æœ¬æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        return _collate_single(batch, max_seq_len, max_agents)


def _collate_single(batch: List[Dict[str, Any]],
                    max_seq_len: int = 160,  # Updated: test data max length is 130, set to 160 with margin
                    max_agents: int = 10) -> Dict[str, torch.Tensor]:
    """
    å•æ ·æœ¬æ‰¹å¤„ç†å‡½æ•°ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    """
    batch_size = len(batch)

    # ğŸ”¥ ä¿®æ­£ 4: ç§»é™¤ max_seq_len çš„åŠ¨æ€æ‰©å±•é€»è¾‘
    # max_seq_len å¿…é¡»æ˜¯ä¸€ä¸ªå›ºå®šå€¼ï¼Œä¸èƒ½ä¾èµ–æ‰¹æ¬¡ä¸­çš„æœ€å¤§æ ‡ç­¾
    # å¦‚æœæ ‡ç­¾è¶Šç•Œï¼Œåˆ™ç®—ä½œ -1 (æ— æ•ˆ)

    # åˆå§‹åŒ–è¾“å‡ºï¼ˆä½¿ç”¨å›ºå®šçš„ max_seq_lenï¼‰
    y_agent_batch = torch.zeros(batch_size, max_agents, dtype=torch.long)
    y_step_batch = torch.zeros(batch_size, dtype=torch.long)
    agent_mask_batch = torch.zeros(batch_size, max_agents, dtype=torch.bool)
    seq_mask_batch = torch.zeros(batch_size, max_seq_len, dtype=torch.bool)

    # å­˜å‚¨åŸå§‹å›¾åˆ—è¡¨ï¼ˆç”¨äºæ¨¡å‹è¾“å…¥ï¼‰
    graph_lists = []

    for i, sample in enumerate(batch):
        graph_list = sample['graph_list']
        labels = sample['labels']

        # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åŸå§‹ graph_list
        # Debug prints removed - è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°æ–‡ä»¶

        # è·å–å®é™…åºåˆ—é•¿åº¦
        actual_seq_len = len(graph_list)
        seq_len = min(actual_seq_len, max_seq_len)

        # è®¾ç½®åºåˆ—æ©ç 
        seq_mask_batch[i, :seq_len] = True

        # å¦‚æœåºåˆ—è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œæˆªæ–­
        if actual_seq_len > max_seq_len:
            graph_list = graph_list[:max_seq_len]
            # Debug prints removed
        # å¦‚æœåºåˆ—ä¸è¶³ï¼Œä¿æŒåŸæ ·ï¼ˆæ¨¡å‹ä¼šå¤„ç†ï¼‰

        # Debug prints removed

        graph_lists.append(graph_list)

        # å¤„ç† Agent æ ‡ç­¾
        y_agent_idx = labels.get('y_agent', -1)
        
        # ğŸ”¥ è¯Šæ–­ï¼šæ‰“å° Hand-Crafted æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        filename = sample.get('filename', '')
        if 'Hand-Crafted' in filename:
            import os
            hc_debug_file = os.path.join('checkpoints_large', 'hc_collate_debug.txt')
            os.makedirs('checkpoints_large', exist_ok=True)
            
            # è·å– Agent èŠ‚ç‚¹æ•°é‡
            num_agents = 0
            if graph_list and hasattr(graph_list[0], 'node_id_to_idx') and graph_list[0].node_id_to_idx:
                num_agents = sum(1 for (node_type, _) in graph_list[0].node_id_to_idx.values() 
                               if node_type == 'Agent')
            
            with open(hc_debug_file, 'a', encoding='utf-8') as f:
                f.write(f"\n[Collate Debug] {filename}\n")
                f.write(f"  labels keys: {list(labels.keys())}\n")
                f.write(f"  y_agent_idx: {y_agent_idx}\n")
                f.write(f"  mistake_agent_name: {labels.get('mistake_agent_name', 'N/A')}\n")
                f.write(f"  num_agents in graph: {num_agents}\n")
                f.write(f"  max_agents: {max_agents}\n")
                f.write(f"  condition (y_agent_idx >= 0 and y_agent_idx < max_agents): {y_agent_idx >= 0 and y_agent_idx < max_agents}\n")
                f.write(f"  Will set y_agent_batch[{i}, {y_agent_idx}] = 1: {y_agent_idx >= 0 and y_agent_idx < max_agents}\n")
        
        if y_agent_idx >= 0 and y_agent_idx < max_agents:
            y_agent_batch[i, y_agent_idx] = 1  # äºŒåˆ†ç±»ï¼š1 è¡¨ç¤ºæ•…éšœ
            
            # ğŸ”¥ è¯Šæ–­ï¼šéªŒè¯è®¾ç½®æ˜¯å¦æˆåŠŸ
            if 'Hand-Crafted' in filename:
                with open(hc_debug_file, 'a', encoding='utf-8') as f:
                    f.write(f"  âœ… Successfully set y_agent_batch[{i}, {y_agent_idx}] = 1\n")
                    f.write(f"  y_agent_batch sum: {y_agent_batch[i].sum().item()}\n")

        # ğŸ”¥ ä¿®æ­£ 6: Agent Mask åº”è¯¥åŸºäºæ•´ä¸ªåºåˆ—ä¸­å‡ºç°çš„æ‰€æœ‰ Agent
        # ç­–ç•¥1: ä» node_id_to_idx ä¸­ç»Ÿè®¡ Agent ç±»å‹çš„èŠ‚ç‚¹æ•°é‡ï¼ˆæœ€å‡†ç¡®ï¼‰
        num_agents = 0
        if graph_list:
            first_graph = graph_list[0]
            # ä¼˜å…ˆä½¿ç”¨ node_id_to_idx ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(first_graph, 'node_id_to_idx') and first_graph.node_id_to_idx is not None:
                # ç»Ÿè®¡æ‰€æœ‰ Agent ç±»å‹çš„èŠ‚ç‚¹æ•°é‡
                agent_count_from_mapping = sum(
                    1 for (node_type, _) in first_graph.node_id_to_idx.values() 
                    if node_type == 'Agent'
                )
                if agent_count_from_mapping > 0:
                    num_agents = agent_count_from_mapping
            else:
                # ç­–ç•¥2: éå†æ•´ä¸ªåºåˆ—ï¼Œæ‰¾åˆ°æœ€å¤§çš„ Agent èŠ‚ç‚¹æ•°é‡
                max_agent_count = 0
                for graph in graph_list:
                    if 'Agent' in graph.node_features:
                        agent_count = graph.node_features['Agent'].shape[0]
                        max_agent_count = max(max_agent_count, agent_count)
                if max_agent_count > 0:
                    num_agents = max_agent_count
                else:
                    # ç­–ç•¥3: å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ max_agents ä½œä¸ºåå¤‡
                    # ä½†éœ€è¦ç¡®ä¿ y_agent_batch ä¸­ä¸º 1 çš„ Agent å¯¹åº”çš„åˆ—å¿…é¡»æ˜¯ True
                    num_agents = max_agents
            
            # é™åˆ¶åœ¨ max_agents èŒƒå›´å†…
            num_agents = min(num_agents, max_agents)
            
            # è®¾ç½® agent_maskï¼šè‡³å°‘æ ‡è®°æ‰€æœ‰æœ‰æ•ˆçš„ Agent
            if num_agents > 0:
                agent_mask_batch[i, :num_agents] = True
            
            # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ y_agent_batch ä¸­ä¸º 1 çš„ Agent å¯¹åº”çš„åˆ—å¿…é¡»æ˜¯ True
            # å³ä½¿è¯¥ Agent çš„ç´¢å¼•è¶…å‡ºäº† num_agentsï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
            if y_agent_idx >= 0 and y_agent_idx < max_agents:
                agent_mask_batch[i, y_agent_idx] = True

        # ğŸ”¥ ä¿®æ­£ 5: ç¡®ä¿ y_step åœ¨æœ‰æ•ˆåºåˆ—èŒƒå›´å†…æ‰æœ‰æ•ˆ
        y_step = labels.get('y_step', -1)
        
        # æ£€æŸ¥ y_step æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼š
        # 1. å¿…é¡» >= 0ï¼ˆéè´Ÿæ•°ï¼‰
        # 2. å¿…é¡» < actual_seq_lenï¼ˆä¸èƒ½è¶…å‡ºåŸå§‹åºåˆ—é•¿åº¦ï¼‰
        # 3. å¿…é¡» < max_seq_lenï¼ˆä¸èƒ½è¶…å‡ºæœ€å¤§å…è®¸é•¿åº¦ï¼‰
        # 4. å¦‚æœåºåˆ—è¢«æˆªæ–­ï¼Œå¿…é¡» < seq_lenï¼ˆä¸èƒ½è¶…å‡ºæˆªæ–­åçš„é•¿åº¦ï¼‰
        if y_step >= 0:
            # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆåºåˆ—èŒƒå›´å†…
            if y_step >= actual_seq_len:
                # æ ‡ç­¾è¶…å‡ºåŸå§‹åºåˆ—é•¿åº¦ï¼Œæ— æ•ˆ
                y_step = -1
            elif y_step >= max_seq_len:
                # æ ‡ç­¾è¶…å‡ºæœ€å¤§å…è®¸é•¿åº¦ï¼Œæ— æ•ˆ
                y_step = -1
            elif actual_seq_len > max_seq_len and y_step >= seq_len:
                # åºåˆ—è¢«æˆªæ–­ï¼Œä¸”æ ‡ç­¾è¶…å‡ºæˆªæ–­åçš„é•¿åº¦ï¼Œæ— æ•ˆ
                y_step = -1
        
        # è®¾ç½®æ ‡ç­¾ï¼ˆ-1 è¡¨ç¤ºæ— æ•ˆæ ‡ç­¾ï¼‰
        y_step_batch[i] = y_step

    return {
        'graph_list': graph_lists,  # List[List[HeteroGraph]]
        'y_agent': y_agent_batch,  # [B, max_agents]
        'y_step': y_step_batch,  # [B]
        'agent_mask': agent_mask_batch,  # [B, max_agents]
        'seq_mask': seq_mask_batch,  # [B, max_seq_len]
    }


def compute_metrics(outputs: Dict[str, torch.Tensor],
                    targets: Dict[str, torch.Tensor],
                    masks: Dict[str, torch.Tensor]) -> Dict[str, float]:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡ (ä¿®å¤ç‰ˆï¼šé€‚é… num_classes=1 çš„æ‰“åˆ†æ¨¡å¼)
    
    ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¯ä¸ªæ ·æœ¬çš„å®é™…åºåˆ—é•¿åº¦æå– scoresï¼Œè€Œä¸æ˜¯ç»Ÿä¸€å– [:, -1, :]
    """
    logits = outputs['logits']  # [B, T, N, 1]
    y_agent = targets['y_agent']  # [B, max_agents]
    agent_mask = masks['agent_mask']  # [B, max_agents]
    seq_mask = masks['seq_mask']  # [B, T] - ğŸ”¥ å…³é”®ï¼šç”¨äºæ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„å®é™…æœ€åä¸€ä¸ªæ—¶é—´æ­¥
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¯ä¸ªæ ·æœ¬çš„å®é™…åºåˆ—é•¿åº¦æå– scores
    # é—®é¢˜ï¼šä¹‹å‰ä½¿ç”¨ logits[:, -1, :, 0]ç»Ÿä¸€å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä½†ä¸åŒæ ·æœ¬çš„å®é™…åºåˆ—é•¿åº¦ä¸åŒ
    # ç»“æœï¼šçŸ­åºåˆ—æ ·æœ¬å–åˆ°äº†paddingä½ç½®ï¼ˆå…¨é›¶ï¼‰ï¼Œå¯¼è‡´ 15/16 æ ·æœ¬scoresä¸º0ï¼Œæ¨¡å‹åç¼©
    B, T, N, _ = logits.shape
    scores = torch.zeros(B, N, device=logits.device, dtype=logits.dtype)
    
    for i in range(B):
        # æ‰¾åˆ°ç¬¬ i ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥
        valid_steps = seq_mask[i].nonzero(as_tuple=True)[0]  # æœ‰æ•ˆæ—¶é—´æ­¥çš„ç´¢å¼•
        if valid_steps.numel() > 0:
            last_step = valid_steps[-1].item()  # æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥
            scores[i] = logits[i, last_step, :, 0]  # âœ… ä»æ­£ç¡®çš„æ—¶é—´æ­¥æå–
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ—¶é—´æ­¥ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨å…¨é›¶
            scores[i] = 0.0
    
    # å¯¹é½ç»´åº¦
    target_N = y_agent.shape[1]
    valid_N = min(N, target_N)
    
    scores = scores[:, :valid_N]
    y_agent_aligned = y_agent[:, :valid_N]
    mask = agent_mask[:, :valid_N]
    
    # å±è”½æ— æ•ˆèŠ‚ç‚¹ (å°†æ— æ•ˆAgentçš„åˆ†æ•°è®¾ä¸ºæå°)
    scores_masked = scores.clone()
    if mask.shape == scores.shape:
        scores_masked[~mask.bool()] = -1e9
    
    # é¢„æµ‹ï¼šåˆ†æ•°æœ€é«˜çš„ Agent
    pred_idx = scores_masked.argmax(dim=1)  # [B]
    true_idx = y_agent_aligned.argmax(dim=1)  # [B]
    
    # ä»…è®¡ç®—æœ‰æœ‰æ•ˆæ ‡ç­¾çš„æ ·æœ¬
    has_label = y_agent_aligned.sum(dim=1) > 0
    if has_label.sum() > 0:
        correct = (pred_idx[has_label] == true_idx[has_label]).float()
        agent_acc = correct.mean().item()
    else:
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè¿”å› 0.0
        # è¿™é€šå¸¸å‘ç”Ÿåœ¨ Hand-Crafted æ•°æ®ä¸­ï¼Œmistake_agent æ— æ³•åŒ¹é…åˆ°å›¾ä¸­çš„èŠ‚ç‚¹
        agent_acc = 0.0

    # Step Accuracy (ä¿æŒä¸å˜)
    step_acc = 0.0
    if 'step_logits' in outputs:
        step_logits = outputs['step_logits']
        y_step = targets['y_step']
        T_step = step_logits.shape[1]
        valid_step_mask = (y_step >= 0) & (y_step < T_step)
        if valid_step_mask.any():
            pred_step = step_logits.argmax(dim=1)
            step_acc = (pred_step[valid_step_mask] == y_step[valid_step_mask]).float().mean().item()

    return {
        'agent_accuracy': agent_acc,
        'step_accuracy': step_acc
    }


def train_epoch(model: nn.Module,
               dataloader: DataLoader,
               loss_fn: ASTRALoss,
               optimizer: torch.optim.Optimizer,
               device: torch.device,
               epoch: int,
               logger: Optional[TrainingLogger] = None,
               w_sup: float = 1.0,
               w_cl: float = 0.1,
               w_rl: float = 0.0,
               gradient_accumulation_steps: int = 1) -> Dict[str, float]:
    """è®­ç»ƒä¸€ä¸ª epochï¼ˆæ··åˆæ¨¡å¼ï¼šç›‘ç£å­¦ä¹  + å¯¹æ¯”å­¦ä¹  + MAPPOï¼‰"""
    model.train()
    
    # åˆå§‹åŒ–å¯¹æ¯”æŸå¤±
    contrastive_criterion = SupConLoss(temperature=0.07).to(device)
    astra_cl_criterion = ASTRAContrastiveLoss(margin=1.0, alpha=0.7).to(device)
    
    # æƒé‡è¶…å‚æ•°ï¼ˆä»å‡½æ•°å‚æ•°ä¼ å…¥ï¼‰
    W_SUP = w_sup   # ç›‘ç£æŸå¤±æƒé‡
    W_CL = w_cl     # ASTRA-CL å¯¹æ¯”æŸå¤±æƒé‡ï¼ˆå»ºè®®ä» 0.1 å¼€å§‹ï¼Œå¿…é¡» > 0 æ‰èƒ½å¯ç”¨å¯¹æ¯”å­¦ä¹ ï¼‰
    W_RL = w_rl     # â›” æš‚æ—¶ç¦ç”¨ RLï¼Œç›´åˆ°ç›‘ç£å­¦ä¹ ç¨³å®š
    
    # åˆå§‹åŒ– ASTRA-CL å¯¹æ¯”æŸå¤±
    astra_cl_criterion = ASTRAContrastiveLoss(margin=1.0, alpha=0.7).to(device)
    
    # ğŸ”¥ ä¿®æ”¹ï¼šæš‚æ—¶å®Œå…¨ç¦ç”¨ RLï¼Œä¸å†ä½¿ç”¨ Warm-up ç­–ç•¥
    # åŸå› ï¼šæ ¹æ® IMPLEMENTATION_PLAN.mdï¼Œéœ€è¦æš‚æ—¶ç¦ç”¨ RL ä»¥é¿å…ä¸ç¨³å®š
    if epoch == 0:
        rl_msg = f"\n[é…ç½®] å¼ºåŒ–å­¦ä¹ å·²ç¦ç”¨ (W_RL=0.0)\n  åŸå› : æš‚æ—¶ç¦ç”¨ RLï¼Œç›´åˆ°ç›‘ç£å­¦ä¹ ç¨³å®š\n  ç­–ç•¥: ä¸“æ³¨äºç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ \n"
        if logger:
            logger.log(rl_msg, to_terminal=False)
        else:
            print(rl_msg)
    
    total_loss = 0.0
    total_agent_loss = 0.0
    total_step_loss = 0.0
    total_aux_loss = 0.0
    total_cl_loss = 0.0
    total_rl_loss = 0.0

    all_metrics = {'agent_accuracy': [], 'step_accuracy': []}

    # ç®€åŒ–è¿›åº¦æ¡è¾“å‡ºï¼Œä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}", leave=False, ncols=80)
    for batch_idx, batch in enumerate(pbar):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        graph_lists = batch['graph_list']  # List[List[HeteroGraph]]
        y_agent = batch['y_agent'].to(device)  # [B, max_agents]
        y_step = batch['y_step'].to(device)  # [B]
        agent_mask = batch['agent_mask'].to(device)  # [B, max_agents]
        seq_mask = batch['seq_mask'].to(device)  # [B, max_seq_len]
        
        # ğŸ”¥ ASTRA-CL: æ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹æ•°æ®
        has_healed = 'healed_graph_list' in batch and batch['healed_graph_list'] is not None
        
        # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°ç¬¬ä¸€ä¸ª batch çš„é…å¯¹çŠ¶æ€
        if batch_idx == 0:
            debug_msg = f"\n[DEBUG] Batch {batch_idx}: has_healed = {has_healed}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Batch keys: {list(batch.keys())}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            if has_healed:
                debug_msg = f"  healed_graph_list type: {type(batch['healed_graph_list'])}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                debug_msg = f"  healed_graph_list length: {len(batch['healed_graph_list']) if batch['healed_graph_list'] else 0}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
            debug_msg = f"  Loss weights: W_SUP={W_SUP}, W_CL={W_CL}, W_RL={W_RL}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)

        # å‰å‘ä¼ æ’­
        # æ³¨æ„ï¼šæ¨¡å‹æœŸæœ› List[HeteroGraph]ï¼Œä½†æ‰¹å¤„ç†ä¸­æ¯ä¸ªæ ·æœ¬æ˜¯ List[HeteroGraph]
        # æˆ‘ä»¬éœ€è¦é€ä¸ªå¤„ç†æ¯ä¸ªæ ·æœ¬ï¼Œæˆ–è€…ä¿®æ”¹æ¨¡å‹ä»¥æ”¯æŒæ‰¹å¤„ç†
        # è¿™é‡Œå…ˆä½¿ç”¨é€ä¸ªå¤„ç†çš„æ–¹å¼

        batch_outputs = []
        for i, graph_list in enumerate(graph_lists):
            # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯ä¸ªæ ·æœ¬çš„ graph_list é•¿åº¦
            # Debug prints removed - è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°æ–‡ä»¶
            
            # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            graph_list_device = [graph.to(device) for graph in graph_list]
            
            # ğŸ”¥ å†æ¬¡æ£€æŸ¥ç§»åŠ¨åˆ°è®¾å¤‡åçš„é•¿åº¦
            # Debug prints removed
            
            output = model(graph_list_device)
            batch_outputs.append(output)

        # åˆå¹¶æ‰¹å¤„ç†è¾“å‡º
        # ç”±äºä¸åŒæ ·æœ¬å¯èƒ½æœ‰ä¸åŒçš„åºåˆ—é•¿åº¦å’Œ Agent æ•°é‡ï¼Œéœ€è¦æ‰¾åˆ°æœ€å¤§å€¼å¹¶ padding
        B = len(graph_lists)

        # æ‰¾åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§åºåˆ—é•¿åº¦å’Œ Agent æ•°é‡
        max_T = max(out['logits'].shape[0] for out in batch_outputs)
        max_N = max(out['logits'].shape[1] for out in batch_outputs)
        num_classes = batch_outputs[0]['logits'].shape[2]
        num_experts = batch_outputs[0]['gate_weights'].shape[2]

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨åˆ›å»ºæ©ç ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥ y_step æ˜¯å¦éœ€è¦æ‰©å±• max_T
        # è¿™ç¡®ä¿ output_seq_mask èƒ½å¤Ÿè¦†ç›–æ‰€æœ‰æœ‰æ•ˆçš„ y_step ä½ç½®
        y_step_cpu = batch['y_step']  # è¿˜åœ¨ CPU ä¸Š
        max_y_step = y_step_cpu.max().item() if y_step_cpu.numel() > 0 and y_step_cpu.max() >= 0 else -1
        if max_y_step >= 0 and max_y_step >= max_T:
            # éœ€è¦æ‰©å±• max_T ä»¥åŒ…å«è¶Šç•Œçš„ y_step
            max_T = max_y_step + 1

        # åˆå§‹åŒ–æ‰¹å¤„ç†å¼ é‡ï¼ˆä½¿ç”¨æ‰©å±•åçš„ max_Tï¼‰
        logits_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['logits'].dtype)
        alpha_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['alpha'].dtype)
        gate_weights_batch = torch.zeros(B, max_T, max_N, num_experts, device=device, dtype=batch_outputs[0]['gate_weights'].dtype)

        # åˆ›å»ºè¾“å‡ºæ©ç ï¼ˆç”¨äºæŸå¤±è®¡ç®—æ—¶å¿½ç•¥ paddingï¼‰
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ‰©å±•åçš„ max_T åˆ›å»ºæ©ç 
        output_seq_mask = torch.zeros(B, max_T, dtype=torch.bool, device=device)
        output_agent_mask = torch.zeros(B, max_T, max_N, dtype=torch.bool, device=device)

        # å¡«å……æ¯ä¸ªæ ·æœ¬çš„è¾“å‡º
        for i, out in enumerate(batch_outputs):
            T_i = out['logits'].shape[0]
            N_i = out['logits'].shape[1]

            # å¤åˆ¶å®é™…æ•°æ®
            logits_batch[i, :T_i, :N_i, :] = out['logits']
            alpha_batch[i, :T_i, :N_i, :] = out['alpha']
            gate_weights_batch[i, :T_i, :N_i, :] = out['gate_weights']

            # è®¾ç½®æ©ç ï¼šæ ‡è®°æ¨¡å‹è¾“å‡ºçš„æœ‰æ•ˆæ—¶é—´æ­¥
            output_seq_mask[i, :T_i] = True
            output_agent_mask[i, :T_i, :N_i] = True
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ y_step æ‰€åœ¨çš„ä½ç½®ä¹Ÿè¢«æ ‡è®°ä¸ºæœ‰æ•ˆ
            # å³ä½¿ y_step è¶…å‡ºäº†æ¨¡å‹è¾“å‡ºé•¿åº¦ï¼ˆç”±äº max_T æ‰©å±•ï¼‰ï¼Œä¹Ÿè¦æ ‡è®°ä¸ºæœ‰æ•ˆ
            y_step_i = y_step[i].item() if y_step.numel() > i else -1
            if y_step_i >= 0 and y_step_i < max_T:
                output_seq_mask[i, y_step_i] = True  # å¼ºåˆ¶æ¿€æ´»æ ‡ç­¾ä½ç½®çš„æ©ç 

        # load éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆå¯èƒ½æ˜¯ [num_experts] æˆ– [T, num_experts]ï¼‰
        load_list = [out['load'] for out in batch_outputs]
        if load_list[0].dim() == 1:
            # [num_experts] -> [B, num_experts]
            load_batch = torch.stack(load_list, dim=0)
        elif load_list[0].dim() == 2:
            # [T, num_experts] -> [B, max_T, num_experts]
            max_T_load = max(load.shape[0] for load in load_list)
            load_batch = torch.zeros(B, max_T_load, num_experts, device=device, dtype=load_list[0].dtype)
            for i, load in enumerate(load_list):
                T_load = load.shape[0]
                load_batch[i, :T_load, :] = load
        else:
            load_batch = torch.stack(load_list, dim=0)

        # global_feat å’Œ state_value å¤„ç†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        has_global_feat = all('global_feat' in out for out in batch_outputs)
        has_state_value = all('state_value' in out for out in batch_outputs)
        
        if has_global_feat:
            global_feat_list = [out['global_feat'] for out in batch_outputs]
            # global_feat å½¢çŠ¶æ˜¯ [1, d_model] æˆ– [d_model]ï¼Œéœ€è¦å †å æˆ [B, d_model]
            global_feat_batch = torch.stack([feat.squeeze(0) if feat.dim() > 1 and feat.shape[0] == 1 else feat 
                                            for feat in global_feat_list], dim=0)  # [B, d_model]
        else:
            global_feat_batch = None
        
        if has_state_value:
            state_value_list = [out['state_value'] for out in batch_outputs]
            # state_value å½¢çŠ¶æ˜¯ [1, 1] æˆ– [1]ï¼Œéœ€è¦å †å æˆ [B, 1]
            state_value_batch = torch.stack([val.squeeze() if val.dim() > 0 else val.unsqueeze(0) 
                                            for val in state_value_list], dim=0)  # [B, 1]
            if state_value_batch.dim() == 1:
                state_value_batch = state_value_batch.unsqueeze(-1)  # [B, 1]
        else:
            state_value_batch = None
        
        # step_logits å¤„ç†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ‰€æœ‰è¾“å‡ºæ˜¯å¦éƒ½æœ‰ step_logitsï¼Œå¹¶ç»™å‡ºæ˜ç¡®è­¦å‘Š
        has_step_logits = all('step_logits' in out for out in batch_outputs)
        
        # ğŸ”¥ å¢å¼ºè°ƒè¯•ï¼šåœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°è¯¦ç»†ä¿¡æ¯
        if batch_idx == 0 and epoch == 0:
            debug_msg = f"\n[DEBUG] Batch Alignment - Checking step_logits:"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Batch size: {B}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Has step_logits in all outputs: {has_step_logits}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            for i, out in enumerate(batch_outputs):
                keys = list(out.keys())
                has_sl = 'step_logits' in out
                debug_msg = f"  Sample {i}: keys={keys}, has_step_logits={has_sl}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                if has_sl:
                    sl_shape = out['step_logits'].shape
                    debug_msg = f"    step_logits shape: {sl_shape}"
                    if logger:
                        logger.log(debug_msg, to_terminal=True)
                    else:
                        print(debug_msg)
        
        if has_step_logits:
            step_logits_list = [out['step_logits'] for out in batch_outputs]
            # step_logits å½¢çŠ¶æ˜¯ [T]ï¼Œéœ€è¦å¯¹é½åˆ°æ‰©å±•åçš„ max_Tï¼ˆå·²ç»åœ¨ä¸Šé¢æ‰©å±•è¿‡äº†ï¼‰
            # ä½¿ç”¨ -inf å¡«å……è¶Šç•Œæ—¶é—´æ­¥ï¼Œè¡¨ç¤ºè¿™äº›æ—¶é—´æ­¥ä¸å¯é¢„æµ‹
            step_logits_batch = torch.full((B, max_T), float('-inf'), device=device, dtype=step_logits_list[0].dtype)
            for i, step_logits in enumerate(step_logits_list):
                T_i = step_logits.shape[0]
                y_step_i = y_step[i].item() if y_step.numel() > i else -1
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šé™åˆ¶å¤åˆ¶é•¿åº¦ä¸º min(T_i, max_T)ï¼Œé˜²æ­¢ç´¢å¼•è¶Šç•Œ
                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ step_logits ä¸ä¸ºç©ºä¸”ç´¢å¼•æœ‰æ•ˆ
                if T_i > 0:
                    copy_len = min(T_i, max_T, step_logits.shape[0])
                    if copy_len > 0 and i < step_logits_batch.shape[0]:
                        step_logits_batch[i, :copy_len] = step_logits[:copy_len]
                
                # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ y_step æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if y_step_i >= 0:
                    if y_step_i >= max_T:
                        error_msg = f"[ERROR] Step Alignment: Sample {i} has y_step={y_step_i} >= max_T={max_T}!"
                        if logger:
                            logger.log(error_msg, to_terminal=True)
                        else:
                            print(error_msg)
                        error_msg = f"  step_logits shape: {step_logits.shape}, T_i={T_i}"
                        if logger:
                            logger.log(error_msg, to_terminal=True)
                        else:
                            print(error_msg)
                        error_msg = f"  This should have been caught earlier - max_T should have been extended!"
                        if logger:
                            logger.log(error_msg, to_terminal=True)
                        else:
                            print(error_msg)
                    elif y_step_i >= T_i:
                        # y_step è¶…å‡ºäº†å®é™…çš„ step_logits é•¿åº¦ï¼Œä½†ä»åœ¨ max_T èŒƒå›´å†…
                        # è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸º max_T å¯èƒ½è¢«æ‰©å±•äº†
                        # Debug prints removed - è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°æ–‡ä»¶
                        pass
                # æ³¨æ„ï¼šå¦‚æœ y_step è¶Šç•Œï¼Œstep_logits åœ¨è¶Šç•Œä½ç½®ä¿æŒ -inf
                # ä½† output_seq_mask å·²ç»æ ‡è®°ä¸º Trueï¼ŒæŸå¤±å‡½æ•°ä¼šæ­£ç¡®å¤„ç†
        else:
            # ğŸ”¥ ä¸¥é‡è­¦å‘Šï¼šæ¨¡å‹æ²¡æœ‰è¿”å› step_logits
            missing_count = sum(1 for out in batch_outputs if 'step_logits' not in out)
            if missing_count > 0:
                error_msg = f"\n[ERROR] âš ï¸  {missing_count}/{B} samples missing 'step_logits' in model output!"
                if logger:
                    logger.log(error_msg, to_terminal=True)
                else:
                    print(error_msg)
                error_msg = f"  Available keys in first output: {list(batch_outputs[0].keys())}"
                if logger:
                    logger.log(error_msg, to_terminal=True)
                else:
                    print(error_msg)
                error_msg = f"  This is a CRITICAL error - the model's forward() method must return 'step_logits'!"
                if logger:
                    logger.log(error_msg, to_terminal=True)
                else:
                    print(error_msg)
                # åˆ›å»º fallback step_logitsï¼ˆå…¨ -infï¼Œè¡¨ç¤ºæ— é¢„æµ‹ï¼‰
                step_logits_batch = torch.full((B, max_T), float('-inf'), device=device, dtype=torch.float32)
            else:
                step_logits_batch = None

        # ğŸ”¥ ASTRA-CL: æå– agent_embeddingsï¼ˆå¦‚æœå­˜åœ¨ï¼‰- åœ¨æˆªæ–­ä¹‹å‰æå–
        has_agent_embeddings = all('agent_embeddings' in out for out in batch_outputs)
        agent_emb_batch = None  # åˆå§‹åŒ–ä¸º Noneï¼Œç¡®ä¿å˜é‡åœ¨ä½œç”¨åŸŸå†…
        if has_agent_embeddings:
            agent_emb_list = [out['agent_embeddings'] for out in batch_outputs]
            # agent_embeddings å½¢çŠ¶æ˜¯ [T, N, D]ï¼Œéœ€è¦å¯¹é½åˆ° [B, max_T, max_N, D]
            agent_emb_batch = torch.zeros(B, max_T, max_N, agent_emb_list[0].shape[2], 
                                         device=device, dtype=agent_emb_list[0].dtype)
            for i, emb in enumerate(agent_emb_list):
                T_i, N_i, D_i = emb.shape
                agent_emb_batch[i, :min(T_i, max_T), :min(N_i, max_N), :] = emb[:min(T_i, max_T), :min(N_i, max_N), :]

        # æ„å»ºç›®æ ‡å­—å…¸
        targets = {
            'y_agent': y_agent,
            'y_step': y_step
        }

        # æ„å»ºæ©ç å­—å…¸
        # æ³¨æ„ï¼šæŸå¤±å‡½æ•°æœŸæœ› agent_mask æ˜¯ [B, N]ï¼Œå…¶ä¸­ N æ˜¯ Agent æ•°é‡
        # ç”±äºè¾“å‡º logits æ˜¯ [B, max_T, max_N, num_classes]ï¼Œè€Œæ ‡ç­¾æ˜¯ [B, max_agents]
        # æˆ‘ä»¬éœ€è¦ç¡®ä¿ max_N >= max_agentsï¼Œæˆ–è€…å¯¹è¾“å‡ºè¿›è¡Œæˆªæ–­/å¯¹é½

        # è·å–å®é™…çš„ Agent æ•°é‡ï¼ˆä»è¾“å‡ºä¸­ï¼‰
        actual_max_N = max_N  # è¾“å‡ºä¸­çš„æœ€å¤§ Agent æ•°

        # å¦‚æœè¾“å‡ºçš„ Agent æ•°é‡ä¸è¾“å…¥ä¸åŒ¹é…ï¼Œéœ€è¦å¯¹é½
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾ max_N >= max_agentsï¼ˆå› ä¸ºæ¨¡å‹è¾“å‡ºå¯èƒ½åŒ…å«æ›´å¤š Agentï¼‰
        # å¦‚æœ max_N < max_agentsï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•è¾“å‡ºï¼ˆä½†è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼‰

        # å¯¹äºæŸå¤±è®¡ç®—ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å…¥æ©ç ï¼ˆå› ä¸ºæ ‡ç­¾æ˜¯åŸºäºè¾“å…¥çš„ï¼‰
        # ä½†éœ€è¦ç¡®ä¿è¾“å‡º logits çš„ Agent ç»´åº¦ä¸æ ‡ç­¾åŒ¹é…
        target_agent_dim = agent_mask.shape[1]  # ç›®æ ‡ Agent ç»´åº¦ï¼ˆæ¥è‡ªæ ‡ç­¾ï¼‰

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ­¥æˆªæ–­ agent_embeddingsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if max_N > target_agent_dim:
            # è¾“å‡ºæœ‰æ›´å¤š Agentï¼Œæˆªæ–­åˆ°è¾“å…¥çš„æ•°é‡
            logits_batch = logits_batch[:, :, :target_agent_dim, :]
            alpha_batch = alpha_batch[:, :, :target_agent_dim, :]
            gate_weights_batch = gate_weights_batch[:, :, :target_agent_dim, :]
            output_agent_mask = output_agent_mask[:, :, :target_agent_dim]
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ­¥æˆªæ–­ agent_embeddings
            if agent_emb_batch is not None:
                agent_emb_batch = agent_emb_batch[:, :, :target_agent_dim, :]
            max_N = target_agent_dim  # æ›´æ–° max_N
        elif max_N < target_agent_dim:
            # è¾“å‡ºæœ‰æ›´å°‘ Agentï¼Œéœ€è¦ paddingï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
            pad_size = target_agent_dim - max_N
            logits_batch = F.pad(logits_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
            alpha_batch = F.pad(alpha_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
            gate_weights_batch = F.pad(gate_weights_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
            output_agent_mask = F.pad(output_agent_mask, (0, pad_size, 0, 0, 0, 0), value=False)
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ­¥ padding agent_embeddings
            if agent_emb_batch is not None:
                agent_emb_batch = F.pad(agent_emb_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
            max_N = target_agent_dim  # æ›´æ–° max_N

        # ç¡®ä¿ y_agent çš„ç»´åº¦ä¸å¯¹é½åçš„ logits åŒ¹é…
        if y_agent.shape[1] != max_N:
            if y_agent.shape[1] > max_N:
                # æˆªæ–­ y_agent
                y_agent = y_agent[:, :max_N]
                agent_mask = agent_mask[:, :max_N]
            else:
                # æ‰©å±• y_agentï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                pad_size = max_N - y_agent.shape[1]
                y_agent = F.pad(y_agent, (0, pad_size, 0, 0), value=0)
                agent_mask = F.pad(agent_mask, (0, pad_size, 0, 0), value=False)

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ update è€Œä¸æ˜¯é‡æ–°èµ‹å€¼ï¼Œé¿å…è¦†ç›–å·²æœ‰å­—æ®µ
        # å…ˆæ„å»ºåŸºç¡€è¾“å‡ºå­—å…¸
        model_outputs = {
            'logits': logits_batch,
            'alpha': alpha_batch,
            'gate_weights': gate_weights_batch,
            'load': load_batch
        }
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ update æ·»åŠ å…¶ä»–å­—æ®µï¼Œè€Œä¸æ˜¯é‡æ–°èµ‹å€¼
        # å¦‚æœå­˜åœ¨ step_logitsï¼Œæ·»åŠ åˆ°è¾“å‡ºå­—å…¸
        if step_logits_batch is not None:
            model_outputs['step_logits'] = step_logits_batch
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ agent_embeddingsï¼ˆç”¨äº ASTRA-CL å¯¹æ¯”å­¦ä¹ ï¼‰
        # æ³¨æ„ï¼šagent_emb_batch å·²ç»åœ¨ä¸Šé¢åŒæ­¥æˆªæ–­/å¯¹é½äº†ï¼Œè¿™é‡Œç›´æ¥æ·»åŠ 
        if has_agent_embeddings and agent_emb_batch is not None:
            # ç¡®ä¿ç»´åº¦åŒ¹é…ï¼ˆåŒé‡æ£€æŸ¥ï¼Œè™½ç„¶ä¸Šé¢å·²ç»å¤„ç†è¿‡äº†ï¼‰
            if agent_emb_batch.shape[2] != max_N:
                if agent_emb_batch.shape[2] > max_N:
                    agent_emb_batch = agent_emb_batch[:, :, :max_N, :]
                elif agent_emb_batch.shape[2] < max_N:
                    pad_size = max_N - agent_emb_batch.shape[2]
                    agent_emb_batch = F.pad(agent_emb_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
            model_outputs['agent_embeddings'] = agent_emb_batch
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ global_featï¼ˆç”¨äº SupConLoss å¯¹æ¯”å­¦ä¹ ï¼‰
        if has_global_feat and global_feat_batch is not None:
            model_outputs['global_feat'] = global_feat_batch
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ state_valueï¼ˆç”¨äºå¼ºåŒ–å­¦ä¹  Criticï¼‰
        if has_state_value and state_value_batch is not None:
            model_outputs['state_value'] = state_value_batch
        
        # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šåœ¨ä¼ é€’ç»™ loss å‡½æ•°ä¹‹å‰éªŒè¯ step_logits
        if batch_idx == 0 and epoch == 0:
            print(f"\n[DEBUG] Before Loss Calculation:")
            print(f"  model_outputs keys: {list(model_outputs.keys())}")
            if 'step_logits' in model_outputs:
                print(f"  âœ… step_logits shape: {model_outputs['step_logits'].shape}")
                print(f"  step_logits dtype: {model_outputs['step_logits'].dtype}")
            else:
                print(f"  âŒ step_logits MISSING in model_outputs!")
                print(f"  step_logits_batch is None: {step_logits_batch is None}")

        # æ„å»ºæ©ç å­—å…¸ï¼ˆæŸå¤±å‡½æ•°æœŸæœ›çš„æ ¼å¼ï¼‰
        masks = {
            'agent_mask': agent_mask,  # [B, max_agents] ç”¨äºæŸå¤±è®¡ç®—
            'seq_mask': output_seq_mask,  # [B, max_T] è¾“å‡ºåºåˆ—æ©ç 
        }

        # === 1. è®¡ç®—ç›‘ç£æŸå¤± (Supervised Loss) ===
        loss_dict = loss_fn(model_outputs, targets, masks)
        sup_loss = loss_dict['total_loss']
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯ç›‘ç£æŸå¤±æ˜¯å¦ä¸º NaNï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡è¯¥ batch æˆ–ä½¿ç”¨å¤‡ç”¨æŸå¤±
        if torch.isnan(sup_loss) or torch.isinf(sup_loss):
            error_msg = f"[CRITICAL ERROR] Supervised loss is NaN/Inf at batch {batch_idx}, epoch {epoch}!"
            if logger:
                logger.log(error_msg, to_terminal=True)
            else:
                print(error_msg)
            error_msg = f"  Agent loss: {loss_dict['agent_loss'].item():.6f}"
            if logger:
                logger.log(error_msg, to_terminal=True)
            else:
                print(error_msg)
            error_msg = f"  Step loss: {loss_dict['step_loss'].item():.6f}"
            if logger:
                logger.log(error_msg, to_terminal=True)
            else:
                print(error_msg)
            error_msg = f"  Aux loss: {loss_dict['aux_loss'].item():.6f}"
            if logger:
                logger.log(error_msg, to_terminal=True)
            else:
                print(error_msg)
            error_msg = f"  Skipping this batch to prevent NaN propagation."
            if logger:
                logger.log(error_msg, to_terminal=True)
            else:
                print(error_msg)
            # ä½¿ç”¨ä»… Agent Loss ä½œä¸ºå¤‡ç”¨ï¼ˆå‡è®¾ Agent Loss æ˜¯ç¨³å®šçš„ï¼‰
            if not (torch.isnan(loss_dict['agent_loss']) or torch.isinf(loss_dict['agent_loss'])):
                sup_loss = loss_fn.w_agent * loss_dict['agent_loss']
                print(f"  Using agent_loss only as fallback: {sup_loss.item():.6f}")
            else:
                print(f"  âŒ All loss components are NaN/Inf, cannot proceed with this batch.")
                continue  # è·³è¿‡è¿™ä¸ª batch
        
        # === 2. è®¡ç®—å¯¹æ¯”æŸå¤± (Contrastive Loss) ===
        # ğŸ”¥ ASTRA-CL: Counterfactual Node-Level Contrast Loss
        cl_loss = torch.tensor(0.0).to(device)
        
        if has_healed and 'agent_embeddings' in model_outputs:
            # æå– Mutated å›¾çš„ Agent embeddings
            emb_mut = model_outputs['agent_embeddings']  # [B, max_T, max_N, D]
            
            # å¤„ç† Healed å›¾ï¼šéœ€è¦å•ç‹¬å‰å‘ä¼ æ’­
            healed_graph_list = batch['healed_graph_list']
            healed_outputs = []
            
            for healed_graphs in healed_graph_list:
                # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                healed_graphs_device = [graph.to(device) for graph in healed_graphs]
                # å¯¹æ¯ä¸ª Healed å›¾åºåˆ—è¿›è¡Œå‰å‘ä¼ æ’­
                with torch.set_grad_enabled(True):  # éœ€è¦æ¢¯åº¦ç”¨äºå¯¹æ¯”å­¦ä¹ 
                    healed_out = model(healed_graphs_device)
                    healed_outputs.append(healed_out)
            
            # å¯¹é½ Healed embeddingsï¼ˆä¸ Mutated ç›¸åŒçš„å¯¹é½é€»è¾‘ï¼‰
            if healed_outputs and 'agent_embeddings' in healed_outputs[0]:
                # å¯¹é½åˆ°ç›¸åŒçš„å½¢çŠ¶ [B, max_T, max_N, D]
                B_healed = len(healed_outputs)
                emb_heal_batch = torch.zeros(B_healed, max_T, max_N, emb_mut.shape[3], 
                                            device=emb_mut.device, dtype=emb_mut.dtype)
                
                for i, out in enumerate(healed_outputs):
                    emb_heal = out['agent_embeddings']  # [T, N, D]
                    T_h, N_h, D_h = emb_heal.shape
                    emb_heal_batch[i, :min(T_h, max_T), :min(N_h, max_N), :] = emb_heal[:min(T_h, max_T), :min(N_h, max_N), :]
                
                # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ embeddings
                emb_mut_final = emb_mut[:, -1, :, :]  # [B, max_N, D]
                emb_heal_final = emb_heal_batch[:, -1, :, :]  # [B_healed, max_N, D]
                
                # ç¡®ä¿ batch size åŒ¹é…
                if B_healed == B:
                    # è®¡ç®—å¯¹æ¯”æŸå¤±
                    # éœ€è¦å°† y_agent è½¬æ¢ä¸ºç´¢å¼•æ ¼å¼
                    mistake_agent_idx = y_agent.argmax(dim=1)  # [B]
                    
                    cl_loss = astra_cl_criterion(
                        emb_mut_final, 
                        emb_heal_final, 
                        mistake_agent_idx,
                        agent_mask
                    )
        
        # å¦‚æœæ²¡æœ‰é…å¯¹æ•°æ®ï¼Œä½¿ç”¨åŸæœ‰çš„ SupConLossï¼ˆåŸºäº global_featï¼‰
        elif 'global_feat' in model_outputs:
            global_feat = model_outputs['global_feat']  # [B, d_model] (æ‰¹å¤„ç†å¯¹é½å)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šBatch Size æ£€æŸ¥ï¼ˆé˜²æ­¢å° batch å´©æºƒï¼‰
            if global_feat.shape[0] < 2:
                cl_loss = torch.tensor(0.0).to(device)
            else:
                # è·å–æ ‡ç­¾
                if 'mistake_type' in batch:
                    cl_labels = batch['mistake_type'].to(device)  # [B]
                else:
                    # ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨ y_agent + y_step ç»„åˆä½œä¸ºä¼ªæ ‡ç­¾
                    # è¿™æ ·å¯ä»¥åŒºåˆ†"åŒä¸€ Agent åœ¨ä¸åŒæ—¶é—´æ­¥çš„æ•…éšœ"ï¼ˆæ›´ç»†ç²’åº¦ï¼‰
                    # ä½¿ç”¨å“ˆå¸Œå‡½æ•°å°† (agent_id, step) æ˜ å°„åˆ°ç±»åˆ« ID
                    true_agent_idx = y_agent.argmax(dim=1)  # [B]
                    # åˆ›å»ºç»„åˆæ ‡ç­¾ï¼šagent_id * max_step + step_id
                    # å‡è®¾ max_step ä¸è¶…è¿‡ 1000ï¼Œè¿™æ ·ç»„åˆæ˜¯å”¯ä¸€çš„
                    max_step_for_hash = 1000
                    cl_labels = true_agent_idx * max_step_for_hash + y_step  # [B]
                
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if global_feat.shape[0] == B and cl_labels.shape[0] == B:
                    cl_loss = contrastive_criterion(global_feat, cl_labels)
                else:
                    cl_loss = torch.tensor(0.0).to(device)
        
        # === 3. è®¡ç®— MAPPO å¼ºåŒ–å­¦ä¹ æŸå¤± (RL Loss) ===
        # A. è·å– Action (å³æ¨¡å‹çš„é¢„æµ‹)
        # Agent Action: é€‰å“ªä¸ª Agent
        # ğŸ”¥ ä¿®å¤ï¼šé€‚åº” num_classes=1ï¼Œç›´æ¥ä½¿ç”¨åˆ†æ•°
        logits_last = model_outputs['logits'][:, -1, :, :]  # [B, N, 1]
        scores = logits_last.squeeze(-1)  # [B, N] - æ¯ä¸ª Agent çš„æ•…éšœåˆ†æ•°
        
        # å¯¹é½ç»´åº¦
        B_act, N_act = scores.shape
        target_N_act = y_agent.shape[1]
        valid_N_act = min(N_act, target_N_act)
        
        scores = scores[:, :valid_N_act]
        agent_mask_act = agent_mask[:, :valid_N_act]
        
        # åº”ç”¨æ©ç ï¼šå°†æ— æ•ˆ Agent çš„åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©·
        scores_masked = scores.clone()
        scores_masked[~agent_mask_act.bool()] = -1e9
        
        # ä½¿ç”¨ softmax å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        agent_probs = F.softmax(scores_masked, dim=-1)  # [B, valid_N_act]
        
        # å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ Agent
        dist_agent = torch.distributions.Categorical(probs=agent_probs)
        action_agent = dist_agent.sample()  # [B] é‡‡æ ·åŠ¨ä½œ
        log_prob_agent = dist_agent.log_prob(action_agent)  # [B]
        
        # Step Action: é€‰å“ªä¸€æ­¥
        if 'step_logits' in model_outputs:
            step_logits_act = model_outputs['step_logits']  # [B, T]
            # å°† -inf æ›¿æ¢ä¸ºå¾ˆå°çš„å€¼ï¼Œé¿å… softmax é—®é¢˜
            step_logits_safe = step_logits_act.clone()
            step_logits_safe[step_logits_safe == float('-inf')] = -1e9
            step_probs = F.softmax(step_logits_safe, dim=-1)  # [B, T]
            dist_step = torch.distributions.Categorical(probs=step_probs)
            action_step = dist_step.sample()  # [B]
            log_prob_step = dist_step.log_prob(action_step)  # [B]
        else:
            # Fallback: å¦‚æœæ²¡æœ‰ step_logitsï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            action_step = torch.zeros(B, dtype=torch.long, device=device)
            log_prob_step = torch.zeros(B, device=device)
        
        # B. è®¡ç®— Reward (å¥–åŠ±) - ä¼˜åŒ–ç‰ˆï¼šæ·»åŠ  Shaped Reward
        # è§„åˆ™ï¼šAgent å¯¹ç»™ +0.5, Step å¯¹ç»™ +0.5, å…¨å¯¹é¢å¤– +1.0
        # æ–°å¢ï¼šè·ç¦»å¥–åŠ±ï¼ˆå¦‚æœé¢„æµ‹æ­¥éª¤åœ¨çœŸå®æ­¥éª¤çš„å‰å 1 æ­¥èŒƒå›´å†…ï¼Œç»™ 0.2 åˆ†ï¼‰
        # å¿…é¡» detachï¼Œä¸éœ€è¦æ¢¯åº¦
        with torch.no_grad():
            true_agent = y_agent.argmax(dim=1)  # [B]
            true_step = y_step  # [B]
            
            # åŸºç¡€å¥–åŠ±
            r_agent = (action_agent == true_agent).float() * 0.5  # [B]
            r_step = (action_step == true_step).float() * 0.5  # [B]
            
            # å®Œç¾å¥–åŠ±
            r_bonus = ((action_agent == true_agent) & (action_step == true_step)).float() * 1.0  # [B]
            
            # ğŸ”¥ æ–°å¢ï¼šè·ç¦»å¥–åŠ± (Shaped Reward) - å‡å°‘ç¨€ç–æ€§
            # å¦‚æœé¢„æµ‹æ­¥éª¤åœ¨çœŸå®æ­¥éª¤çš„å‰å 1 æ­¥èŒƒå›´å†…ï¼Œç»™ 0.2 åˆ†
            step_diff = torch.abs(action_step - true_step)  # [B]
            r_proximity = (step_diff <= 1).float() * 0.2  # [B]
            
            rewards = r_agent + r_step + r_bonus + r_proximity  # [B]
            
            # è®¡ç®— Advantage (ä¼˜åŠ¿) = Reward - Critic_Value
            # ç®€å•çš„å•æ­¥ PPOï¼ŒAdvantage = R - V(s)
            if 'state_value' in model_outputs:
                values = model_outputs['state_value'].squeeze(-1)  # [B] (æ‰¹å¤„ç†å¯¹é½å)
                if values.shape[0] != B:
                    # ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨é›¶å€¼
                    values = torch.zeros(B, device=device)
                advantages = rewards - values  # [B]
            else:
                # å¦‚æœæ²¡æœ‰ state_valueï¼Œä½¿ç”¨é›¶å€¼
                values = torch.zeros(B, device=device)
                advantages = rewards  # [B]
        
        # C. è®¡ç®— PPO Loss (Actor Loss + Critic Loss)
        # Critic Loss: MSE(Value, Reward)
        if 'state_value' in model_outputs:
            state_value_act = model_outputs['state_value'].squeeze(-1)  # [B] (æ‰¹å¤„ç†å¯¹é½å)
            if state_value_act.shape[0] != B:
                state_value_act = torch.zeros(B, device=device)
            critic_loss = F.mse_loss(state_value_act, rewards)
        else:
            critic_loss = torch.tensor(0.0).to(device)
        
        # Actor Loss: -log_prob * advantage
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä¸ä¿ç•™æ—§ç­–ç•¥ (Approximate PPO)
        pg_loss = -(log_prob_agent + log_prob_step) * advantages.detach()  # [B]
        pg_loss = pg_loss.mean()  # æ ‡é‡
        
        rl_loss = pg_loss + 0.5 * critic_loss
        
        # === 4. æ€»æŸå¤±ä¸åå‘ä¼ æ’­ ===
        total_loss = W_SUP * sup_loss + W_CL * cl_loss + W_RL * rl_loss

        # ğŸ”¥ å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼šåœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆä¿å­˜åˆ°æ—¥å¿—ï¼‰
        if batch_idx == 0:
            debug_msg = f"\n[DEBUG] Loss Calculation - Batch {batch_idx} (Epoch {epoch}):"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Loss Weights: W_SUP={W_SUP}, W_CL={W_CL}, W_RL={W_RL} {'(Warm-up: RL disabled)' if W_RL == 0.0 else '(RL enabled)'}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Max T: {max_T}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Seq Mask Sums: {output_seq_mask.sum(dim=1).tolist()}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  Y_Step: {y_step.tolist()}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            
            # æ£€æŸ¥ step_logits æ˜¯å¦å­˜åœ¨
            if 'step_logits' in model_outputs:
                step_logits_shape = model_outputs['step_logits'].shape
                debug_msg = f"  âœ… Step Logits Shape: {step_logits_shape}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                debug_msg = f"  Step Logits dtype: {model_outputs['step_logits'].dtype}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                debug_msg = f"  Step Logits device: {model_outputs['step_logits'].device}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                
                # æ£€æŸ¥ step_logits çš„å€¼èŒƒå›´
                sl = model_outputs['step_logits']
                debug_msg = f"  Step Logits stats: min={sl.min().item():.4f}, max={sl.max().item():.4f}, mean={sl.mean().item():.4f}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                debug_msg = f"  Step Logits -inf count: {(sl == float('-inf')).sum().item()}/{sl.numel()}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                
                # æ£€æŸ¥æ¯ä¸ªæ ·æœ¬çš„ step_logits åœ¨æ ‡ç­¾ä½ç½®çš„å€¼
                for i in range(B):
                    y_step_i = y_step[i].item()
                    if y_step_i >= 0 and y_step_i < step_logits_shape[1]:
                        logit_at_label = model_outputs['step_logits'][i, y_step_i].item()
                        print(f"    Sample {i}: y_step={y_step_i}, logit_at_label={logit_at_label:.4f}")
                    else:
                        print(f"    Sample {i}: y_step={y_step_i} (out of range, max_T={max_T})")
            else:
                print("  âŒ Step Logits NOT in model_outputs!")
                print(f"  Available keys: {list(model_outputs.keys())}")
                raise RuntimeError("CRITICAL: step_logits missing in model_outputs after batch alignment!")
            
            print(f"  Step Loss: {loss_dict['step_loss'].item():.6f}")
            # æ£€æŸ¥æ¯ä¸ªæ ·æœ¬çš„æ©ç çŠ¶æ€
            for i in range(B):
                y_step_i = y_step[i].item()
                mask_sum = output_seq_mask[i].sum().item()
                mask_at_label = output_seq_mask[i, y_step_i].item() if y_step_i >= 0 and y_step_i < max_T else False
                print(f"    Sample {i}: y_step={y_step_i}, mask_sum={mask_sum}, mask_at_label={mask_at_label}")

        # åå‘ä¼ æ’­ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
        # ç¼©æ”¾æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯æ—¶ï¼‰
        scaled_loss = total_loss / gradient_accumulation_steps
        scaled_loss.backward()
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯ Critic ç½‘ç»œçš„æ¢¯åº¦å›ä¼ 
        # æ£€æŸ¥ Critic å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦ï¼ˆç¡®ä¿æ¢¯åº¦æ­£ç¡®å›ä¼ ï¼‰
        if hasattr(model, 'critic') and 'state_value' in model_outputs:
            critic_has_grad = False
            critic_grad_norm = 0.0
            for param in model.critic.parameters():
                if param.grad is not None:
                    critic_has_grad = True
                    critic_grad_norm += param.grad.norm().item() ** 2
            critic_grad_norm = critic_grad_norm ** 0.5
            
            # åœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°æ¢¯åº¦ä¿¡æ¯å’Œ Reward ç»Ÿè®¡
            if batch_idx == 0 and epoch == 0:
                print(f"\n[DEBUG] Critic Gradient Check:")
                print(f"  Critic has gradient: {critic_has_grad}")
                if critic_has_grad:
                    print(f"  Critic gradient norm: {critic_grad_norm:.6f}")
                else:
                    print(f"  âš ï¸  WARNING: Critic has NO gradient! This may indicate a problem with gradient flow.")
                
                # ğŸ”¥ æ–°å¢ï¼šæ‰“å° Reward ç»Ÿè®¡ä¿¡æ¯ï¼ˆç›‘æ§ Shaped Reward æ•ˆæœï¼‰
                print(f"\n[DEBUG] Reward Statistics:")
                print(f"  Reward components (mean over batch):")
                print(f"    r_agent: {r_agent.mean().item():.4f} (Agent correct reward)")
                print(f"    r_step: {r_step.mean().item():.4f} (Step correct reward)")
                print(f"    r_bonus: {r_bonus.mean().item():.4f} (Perfect match bonus)")
                print(f"    r_proximity: {r_proximity.mean().item():.4f} (Distance reward - NEW)")
                print(f"    Total reward: {rewards.mean().item():.4f}")
                print(f"  Reward range: [{rewards.min().item():.4f}, {rewards.max().item():.4f}]")
                print(f"  Advantage range: [{advantages.min().item():.4f}, {advantages.max().item():.4f}]")
        
        # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥éª¤è¾¾åˆ°æ—¶æ‰æ›´æ–°å‚æ•°
        if (batch_idx + 1) % gradient_accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
        elif (batch_idx + 1) == len(dataloader):
            # æœ€åä¸€ä¸ªbatchï¼Œå³ä½¿æ²¡è¾¾åˆ°ç´¯ç§¯æ­¥æ•°ä¹Ÿè¦æ›´æ–°
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()

        # ç´¯è®¡æŸå¤±
        total_loss_val = total_loss.item()
        total_loss += total_loss_val
        total_agent_loss += loss_dict['agent_loss'].item()
        total_step_loss += loss_dict['step_loss'].item()
        total_aux_loss += loss_dict['aux_loss'].item()
        total_cl_loss += cl_loss.item()
        total_rl_loss += rl_loss.item()

        # è®¡ç®—æŒ‡æ ‡
        metrics = compute_metrics(model_outputs, targets, masks)
        all_metrics['agent_accuracy'].append(metrics['agent_accuracy'])
        all_metrics['step_accuracy'].append(metrics['step_accuracy'])

        # ç®€åŒ–è¿›åº¦æ¡è¾“å‡ºï¼Œåªæ˜¾ç¤ºæ€»æŸå¤±
        pbar.set_postfix({'Loss': f"{total_loss_val:.4f}"})

    # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
    num_batches = len(dataloader)
    avg_loss = total_loss / num_batches
    avg_agent_loss = total_agent_loss / num_batches
    avg_step_loss = total_step_loss / num_batches
    avg_aux_loss = total_aux_loss / num_batches
    avg_cl_loss = total_cl_loss / num_batches
    avg_rl_loss = total_rl_loss / num_batches

    avg_agent_acc = np.mean(all_metrics['agent_accuracy'])
    avg_step_acc = np.mean(all_metrics['step_accuracy'])

    return {
        'loss': avg_loss,
        'agent_loss': avg_agent_loss,
        'step_loss': avg_step_loss,
        'aux_loss': avg_aux_loss,
        'cl_loss': avg_cl_loss,
        'rl_loss': avg_rl_loss,
        'agent_accuracy': avg_agent_acc,
        'step_accuracy': avg_step_acc
    }


def validate(model: nn.Module,
            dataloader: DataLoader,
            loss_fn: ASTRALoss,
            device: torch.device,
            logger: Optional[TrainingLogger] = None) -> Dict[str, float]:
    """éªŒè¯"""
    model.eval()
    total_loss = 0.0
    all_metrics = {'agent_accuracy': [], 'step_accuracy': []}
    
    # ğŸ”¥ è¾…åŠ©å‡½æ•°ï¼šç»Ÿä¸€å¤„ç† debug ä¿¡æ¯çš„æ‰“å°å’Œæ—¥å¿—ä¿å­˜
    def debug_log(msg: str, to_terminal: bool = True):
        """å°† debug ä¿¡æ¯ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ï¼Œå¯é€‰æ˜¯å¦åŒæ—¶æ‰“å°åˆ°ç»ˆç«¯"""
        if logger:
            logger.log(msg, to_terminal=to_terminal)
        else:
            if to_terminal:
                print(msg, flush=True)

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validating"):
            graph_lists = batch['graph_list']
            y_agent = batch['y_agent'].to(device)
            y_step = batch['y_step'].to(device)
            agent_mask = batch['agent_mask'].to(device)
            seq_mask = batch['seq_mask'].to(device)

            # å‰å‘ä¼ æ’­
            batch_outputs = []
            for graph_list in graph_lists:
                # å°†å›¾æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                graph_list_device = [graph.to(device) for graph in graph_list]
                output = model(graph_list_device)
                batch_outputs.append(output)

            # åˆå¹¶è¾“å‡ºï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ‰¹å¤„ç†é€»è¾‘ï¼‰
            B = len(graph_lists)

            # æ‰¾åˆ°æ‰¹æ¬¡ä¸­çš„æœ€å¤§åºåˆ—é•¿åº¦å’Œ Agent æ•°é‡
            max_T = max(out['logits'].shape[0] for out in batch_outputs)
            max_N = max(out['logits'].shape[1] for out in batch_outputs)
            num_classes = batch_outputs[0]['logits'].shape[2]
            num_experts = batch_outputs[0]['gate_weights'].shape[2]

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨åˆ›å»ºæ©ç ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥ y_step æ˜¯å¦éœ€è¦æ‰©å±• max_T
            # è¿™ç¡®ä¿ output_seq_mask èƒ½å¤Ÿè¦†ç›–æ‰€æœ‰æœ‰æ•ˆçš„ y_step ä½ç½®
            y_step_cpu = batch['y_step']  # è¿˜åœ¨ CPU ä¸Š
            max_y_step = y_step_cpu.max().item() if y_step_cpu.numel() > 0 and y_step_cpu.max() >= 0 else -1
            if max_y_step >= 0 and max_y_step >= max_T:
                # éœ€è¦æ‰©å±• max_T ä»¥åŒ…å«è¶Šç•Œçš„ y_step
                max_T = max_y_step + 1

            # åˆå§‹åŒ–æ‰¹å¤„ç†å¼ é‡ï¼ˆä½¿ç”¨æ‰©å±•åçš„ max_Tï¼‰
            logits_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['logits'].dtype)
            alpha_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['alpha'].dtype)
            gate_weights_batch = torch.zeros(B, max_T, max_N, num_experts, device=device, dtype=batch_outputs[0]['gate_weights'].dtype)

            # åˆ›å»ºè¾“å‡ºæ©ç ï¼ˆç”¨äºæŸå¤±è®¡ç®—æ—¶å¿½ç•¥ paddingï¼‰
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ‰©å±•åçš„ max_T åˆ›å»ºæ©ç 
            output_seq_mask = torch.zeros(B, max_T, dtype=torch.bool, device=device)
            output_agent_mask = torch.zeros(B, max_T, max_N, dtype=torch.bool, device=device)

            # å¡«å……æ¯ä¸ªæ ·æœ¬çš„è¾“å‡º
            for i, out in enumerate(batch_outputs):
                T_i = out['logits'].shape[0]
                N_i = out['logits'].shape[1]

                # å¤åˆ¶å®é™…æ•°æ®
                logits_batch[i, :T_i, :N_i, :] = out['logits']
                alpha_batch[i, :T_i, :N_i, :] = out['alpha']
                gate_weights_batch[i, :T_i, :N_i, :] = out['gate_weights']

                # è®¾ç½®æ©ç ï¼šæ ‡è®°æ¨¡å‹è¾“å‡ºçš„æœ‰æ•ˆæ—¶é—´æ­¥
                output_seq_mask[i, :T_i] = True
                output_agent_mask[i, :T_i, :N_i] = True
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ y_step æ‰€åœ¨çš„ä½ç½®ä¹Ÿè¢«æ ‡è®°ä¸ºæœ‰æ•ˆ
                # å³ä½¿ y_step è¶…å‡ºäº†æ¨¡å‹è¾“å‡ºé•¿åº¦ï¼ˆç”±äº max_T æ‰©å±•ï¼‰ï¼Œä¹Ÿè¦æ ‡è®°ä¸ºæœ‰æ•ˆ
                y_step_i = y_step[i].item() if y_step.numel() > i else -1
                if y_step_i >= 0 and y_step_i < max_T:
                    output_seq_mask[i, y_step_i] = True  # å¼ºåˆ¶æ¿€æ´»æ ‡ç­¾ä½ç½®çš„æ©ç 

            # load éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆå¯èƒ½æ˜¯ [num_experts] æˆ– [T, num_experts]ï¼‰
            load_list = [out['load'] for out in batch_outputs]
            if load_list[0].dim() == 1:
                # [num_experts] -> [B, num_experts]
                load_batch = torch.stack(load_list, dim=0)
            elif load_list[0].dim() == 2:
                # [T, num_experts] -> [B, max_T, num_experts]
                max_T_load = max(load.shape[0] for load in load_list)
                load_batch = torch.zeros(B, max_T_load, num_experts, device=device, dtype=load_list[0].dtype)
                for i, load in enumerate(load_list):
                    T_load = load.shape[0]
                    load_batch[i, :T_load, :] = load
            else:
                load_batch = torch.stack(load_list, dim=0)

            # step_logits å¤„ç†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ‰€æœ‰è¾“å‡ºæ˜¯å¦éƒ½æœ‰ step_logitsï¼Œå¹¶ç»™å‡ºæ˜ç¡®è­¦å‘Š
            has_step_logits = all('step_logits' in out for out in batch_outputs)
            if has_step_logits:
                step_logits_list = [out['step_logits'] for out in batch_outputs]
                # step_logits å½¢çŠ¶æ˜¯ [T]ï¼Œéœ€è¦å¯¹é½åˆ°æ‰©å±•åçš„ max_Tï¼ˆå·²ç»åœ¨ä¸Šé¢æ‰©å±•è¿‡äº†ï¼‰
                # ä½¿ç”¨ -inf å¡«å……è¶Šç•Œæ—¶é—´æ­¥ï¼Œè¡¨ç¤ºè¿™äº›æ—¶é—´æ­¥ä¸å¯é¢„æµ‹
                step_logits_batch = torch.full((B, max_T), float('-inf'), device=device, dtype=step_logits_list[0].dtype)
                for i, step_logits in enumerate(step_logits_list):
                    T_i = step_logits.shape[0]
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šé™åˆ¶å¤åˆ¶é•¿åº¦ä¸º min(T_i, max_T)ï¼Œé˜²æ­¢ç´¢å¼•è¶Šç•Œ
                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ step_logits ä¸ä¸ºç©ºä¸”ç´¢å¼•æœ‰æ•ˆ
                    if T_i > 0:
                        copy_len = min(T_i, max_T, step_logits.shape[0])
                        if copy_len > 0 and i < step_logits_batch.shape[0]:
                            step_logits_batch[i, :copy_len] = step_logits[:copy_len]
                    # æ³¨æ„ï¼šå¦‚æœ y_step è¶Šç•Œï¼Œstep_logits åœ¨è¶Šç•Œä½ç½®ä¿æŒ -inf
                    # ä½† output_seq_mask å·²ç»æ ‡è®°ä¸º Trueï¼ŒæŸå¤±å‡½æ•°ä¼šæ­£ç¡®å¤„ç†
            else:
                # ğŸ”¥ ä¸¥é‡è­¦å‘Šï¼šæ¨¡å‹æ²¡æœ‰è¿”å› step_logits
                missing_count = sum(1 for out in batch_outputs if 'step_logits' not in out)
                if missing_count > 0:
                    print(f"[ERROR] âš ï¸  {missing_count}/{B} samples missing 'step_logits' in model output!")
                    print(f"  Available keys in first output: {list(batch_outputs[0].keys())}")
                    # åˆ›å»º fallback step_logitsï¼ˆå…¨ -infï¼Œè¡¨ç¤ºæ— é¢„æµ‹ï¼‰
                    step_logits_batch = torch.full((B, max_T), float('-inf'), device=device, dtype=torch.float32)
                else:
                    step_logits_batch = None

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨æˆªæ–­ä¹‹å‰æå– agent_embeddingsã€global_featã€state_value
            has_agent_embeddings = all('agent_embeddings' in out for out in batch_outputs)
            agent_emb_batch = None
            if has_agent_embeddings:
                agent_emb_list = [out['agent_embeddings'] for out in batch_outputs]
                # agent_embeddings å½¢çŠ¶æ˜¯ [T, N, D]ï¼Œéœ€è¦å¯¹é½åˆ° [B, max_T, max_N, D]
                agent_emb_batch = torch.zeros(B, max_T, max_N, agent_emb_list[0].shape[2], 
                                             device=device, dtype=agent_emb_list[0].dtype)
                for i, emb in enumerate(agent_emb_list):
                    T_i, N_i, D_i = emb.shape
                    agent_emb_batch[i, :min(T_i, max_T), :min(N_i, max_N), :] = emb[:min(T_i, max_T), :min(N_i, max_N), :]
            
            # æå– global_feat å’Œ state_valueï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            has_global_feat = all('global_feat' in out for out in batch_outputs)
            global_feat_batch = None
            if has_global_feat:
                global_feat_list = [out['global_feat'] for out in batch_outputs]
                global_feat_batch = torch.stack([feat.squeeze(0) if feat.dim() > 1 and feat.shape[0] == 1 else feat 
                                                for feat in global_feat_list], dim=0)
            
            has_state_value = all('state_value' in out for out in batch_outputs)
            state_value_batch = None
            if has_state_value:
                state_value_list = [out['state_value'] for out in batch_outputs]
                state_value_batch = torch.stack([val.squeeze() if val.dim() > 0 else val.unsqueeze(0) 
                                                for val in state_value_list], dim=0)
                if state_value_batch.dim() == 1:
                    state_value_batch = state_value_batch.unsqueeze(-1)

            # å¯¹é½ Agent ç»´åº¦
            target_agent_dim = agent_mask.shape[1]  # ç›®æ ‡ Agent ç»´åº¦ï¼ˆæ¥è‡ªæ ‡ç­¾ï¼‰

            if max_N > target_agent_dim:
                # è¾“å‡ºæœ‰æ›´å¤š Agentï¼Œæˆªæ–­åˆ°è¾“å…¥çš„æ•°é‡
                logits_batch = logits_batch[:, :, :target_agent_dim, :]
                alpha_batch = alpha_batch[:, :, :target_agent_dim, :]
                gate_weights_batch = gate_weights_batch[:, :, :target_agent_dim, :]
                output_agent_mask = output_agent_mask[:, :, :target_agent_dim]
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ­¥æˆªæ–­ agent_embeddings
                if agent_emb_batch is not None:
                    agent_emb_batch = agent_emb_batch[:, :, :target_agent_dim, :]
                max_N = target_agent_dim  # æ›´æ–° max_N
            elif max_N < target_agent_dim:
                # è¾“å‡ºæœ‰æ›´å°‘ Agentï¼Œéœ€è¦ paddingï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
                pad_size = target_agent_dim - max_N
                logits_batch = F.pad(logits_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
                alpha_batch = F.pad(alpha_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
                gate_weights_batch = F.pad(gate_weights_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
                output_agent_mask = F.pad(output_agent_mask, (0, pad_size, 0, 0, 0, 0), value=False)
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ­¥ padding agent_embeddings
                if agent_emb_batch is not None:
                    agent_emb_batch = F.pad(agent_emb_batch, (0, 0, 0, pad_size, 0, 0, 0, 0))
                max_N = target_agent_dim  # æ›´æ–° max_N

            # ç¡®ä¿ y_agent çš„ç»´åº¦ä¸å¯¹é½åçš„ logits åŒ¹é…
            if y_agent.shape[1] != max_N:
                if y_agent.shape[1] > max_N:
                    # æˆªæ–­ y_agent
                    y_agent = y_agent[:, :max_N]
                    agent_mask = agent_mask[:, :max_N]
                else:
                    # æ‰©å±• y_agentï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                    pad_size = max_N - y_agent.shape[1]
                    y_agent = F.pad(y_agent, (0, pad_size, 0, 0), value=0)
                    agent_mask = F.pad(agent_mask, (0, pad_size, 0, 0), value=False)

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ„å»ºè¾“å‡ºå­—å…¸ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½è¢«æ·»åŠ 
            model_outputs = {
                'logits': logits_batch,
                'alpha': alpha_batch,
                'gate_weights': gate_weights_batch,
                'load': load_batch
            }

            # å¦‚æœå­˜åœ¨ step_logitsï¼Œæ·»åŠ åˆ°è¾“å‡ºå­—å…¸
            if step_logits_batch is not None:
                model_outputs['step_logits'] = step_logits_batch
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  agent_embeddingsï¼ˆç”¨äº ASTRA-CL å¯¹æ¯”å­¦ä¹ ï¼‰
            if has_agent_embeddings and agent_emb_batch is not None:
                model_outputs['agent_embeddings'] = agent_emb_batch
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  global_featï¼ˆç”¨äº SupConLoss å¯¹æ¯”å­¦ä¹ ï¼‰
            if has_global_feat and global_feat_batch is not None:
                model_outputs['global_feat'] = global_feat_batch
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  state_valueï¼ˆç”¨äºå¼ºåŒ–å­¦ä¹  Criticï¼‰
            if has_state_value and state_value_batch is not None:
                model_outputs['state_value'] = state_value_batch

            # æ„å»ºç›®æ ‡å­—å…¸
            targets = {'y_agent': y_agent, 'y_step': y_step}
            
            # æ„å»ºæ©ç å­—å…¸ï¼ˆæŸå¤±å‡½æ•°æœŸæœ›çš„æ ¼å¼ï¼‰
            masks = {'agent_mask': agent_mask, 'seq_mask': output_seq_mask}

            loss_dict = loss_fn(model_outputs, targets, masks)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯æŸå¤±æ˜¯å¦ä¸º NaNï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡è¯¥æ ·æœ¬
            val_loss = loss_dict['total_loss']
            if torch.isnan(val_loss) or torch.isinf(val_loss):
                print(f"[WARNING] Validation loss is NaN/Inf, skipping this batch.")
                continue
            
            total_loss += val_loss.item()

            metrics = compute_metrics(model_outputs, targets, masks)
            all_metrics['agent_accuracy'].append(metrics['agent_accuracy'])
            all_metrics['step_accuracy'].append(metrics['step_accuracy'])
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ‰“å°æ¯ä¸ª batch çš„å‡†ç¡®ç‡ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜ï¼ˆä¿å­˜åˆ°æ—¥å¿—ï¼‰
            batch_idx = len(all_metrics['agent_accuracy']) - 1
            if batch_idx < 3:  # æ‰“å°å‰3ä¸ª batch
                debug_msg = f"  [Val Batch {batch_idx}] Agent Acc: {metrics['agent_accuracy']:.6f}, Step Acc: {metrics['step_accuracy']:.6f}"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
            
            # ğŸ”¥ è°ƒè¯•ï¼šåœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°éªŒè¯é›†çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä¿å­˜åˆ°æ—¥å¿—ï¼‰
            if len(all_metrics['agent_accuracy']) == 1:  # ç¬¬ä¸€ä¸ª batch
                debug_msg = f"\n[DEBUG] Validation Batch 0:"
                if logger:
                    logger.log(debug_msg, to_terminal=True)
                else:
                    print(debug_msg)
                debug_log(f"  Batch size: {B}")
                debug_log(f"  y_agent shape: {y_agent.shape}")
                debug_log(f"  y_agent sum per sample: {y_agent.sum(dim=1).tolist()}")
                debug_log(f"  agent_mask shape: {agent_mask.shape}")
                debug_log(f"  agent_mask sum per sample: {agent_mask.sum(dim=1).tolist()}")
                # æ£€æŸ¥æœ‰å¤šå°‘æ ·æœ¬æœ‰æœ‰æ•ˆæ ‡ç­¾
                has_label = y_agent.sum(dim=1) > 0
                debug_log(f"  Samples with valid labels: {has_label.sum().item()}/{B}")
                if has_label.any():
                    # æ‰“å°é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                    logits_val = model_outputs['logits']  # [B, T, N, 1]
                    seq_mask_val = masks['seq_mask']  # [B, T]
                    
                    # ğŸ”¥ ä¿®å¤ï¼šæ ¹æ®æ¯ä¸ªæ ·æœ¬çš„å®é™…åºåˆ—é•¿åº¦æå– scores
                    B_val, T_val, N_val, _ = logits_val.shape
                    scores_val = torch.zeros(B_val, N_val, device=logits_val.device, dtype=logits_val.dtype)
                    
                    for i in range(B_val):
                        valid_steps = seq_mask_val[i].nonzero(as_tuple=True)[0]
                        if valid_steps.numel() > 0:
                            last_step = valid_steps[-1].item()
                            scores_val[i] = logits_val[i, last_step, :, 0]
                        else:
                            scores_val[i] = 0.0
                    
                    # ç»§ç»­åŸæœ‰çš„ç»´åº¦å¯¹é½é€»è¾‘
                    valid_N_val = min(N_val, y_agent.shape[1])
                    scores_val = scores_val[:, :valid_N_val]
                    y_agent_val = y_agent[:, :valid_N_val]
                    mask_val = agent_mask[:, :valid_N_val]
                    
                    # ğŸ”¥ è¯¦ç»†è°ƒè¯•ï¼šæ£€æŸ¥ logits çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿å­˜åˆ°æ—¥å¿—ï¼‰
                    debug_log(f"  Logits shape: {logits_val.shape}")
                    debug_log(f"  Scores shape: {scores_val.shape}")
                    debug_log(f"  Scores (first 3 samples, first 10 agents):")
                    for i in range(min(3, B_val)):
                        debug_log(f"    Sample {i}: {scores_val[i, :10].tolist()}")
                    
                    # åº”ç”¨æ©ç 
                    scores_masked_val = scores_val.clone()
                    scores_masked_val[~mask_val.bool()] = -1e9
                    debug_log(f"  Scores after masking (first 3 samples, first 10 agents):")
                    for i in range(min(3, B_val)):
                        debug_log(f"    Sample {i}: {scores_masked_val[i, :10].tolist()}")
                    
                    # æ£€æŸ¥é¢„æµ‹é€»è¾‘
                    true_idx_val = y_agent_val.argmax(dim=1)  # ğŸ”¥ ä¿®å¤ï¼šå…ˆå®šä¹‰ true_idx_val
                    pred_idx_val = scores_masked_val.argmax(dim=1)
                    debug_log(f"  Predictions (all): {pred_idx_val.tolist()}")
                    debug_log(f"  True labels (all): {true_idx_val.tolist()}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
                    if torch.isnan(scores_val).any():
                        debug_log(f"  [WARNING] Scores contain NaN!")
                    if torch.isinf(scores_val).any():
                        debug_log(f"  [WARNING] Scores contain Inf!")
                    
                    # æ£€æŸ¥logitsçš„ç»Ÿè®¡
                    debug_log(f"  Logits stats: min={logits_val.min().item():.4f}, max={logits_val.max().item():.4f}, mean={logits_val.mean().item():.4f}")
                    debug_log(f"  Scores stats: min={scores_val.min().item():.4f}, max={scores_val.max().item():.4f}, mean={scores_val.mean().item():.4f}")
                    debug_log(f"  Scores statistics:")
                    debug_log(f"    Mean: {scores_val.mean().item():.4f}, Std: {scores_val.std().item():.4f}")
                    debug_log(f"    Min: {scores_val.min().item():.4f}, Max: {scores_val.max().item():.4f}")
                    debug_log(f"  Scores per sample (first 5, first 10 agents):")
                    for i in range(min(5, B_val)):
                        debug_log(f"    Sample {i}: {scores_val[i, :10].tolist()}")
                    debug_log(f"  Scores after masking (first 5, first 10 agents):")
                    scores_masked_debug = scores_val.clone()
                    scores_masked_debug[~mask_val.bool()] = -1e9
                    for i in range(min(5, B_val)):
                        debug_log(f"    Sample {i}: {scores_masked_debug[i, :10].tolist()}")
                    debug_log(f"  Agent mask (first 5, first 10 agents):")
                    for i in range(min(5, B_val)):
                        debug_log(f"    Sample {i}: {mask_val[i, :10].tolist()}")
                    debug_log(f"  Scores stats (first sample): min={scores_val[0].min().item():.4f}, max={scores_val[0].max().item():.4f}, mean={scores_val[0].mean().item():.4f}, std={scores_val[0].std().item():.4f}")
                    debug_log(f"  Scores values (first sample, first 10): {scores_val[0, :10].tolist()}")
                    debug_log(f"  Mask (first sample, first 10): {mask_val[0, :10].tolist()}")
                    
                    scores_masked_val = scores_val.clone()
                    scores_masked_val[~mask_val.bool()] = -1e9
                    
                    # ğŸ”¥ æ£€æŸ¥æ©ç åçš„åˆ†æ•°
                    debug_log(f"  Masked scores (first sample, first 10): {scores_masked_val[0, :10].tolist()}")
                    debug_log(f"  Masked scores max (first sample): {scores_masked_val[0].max().item():.4f} at index {scores_masked_val[0].argmax().item()}")
                    
                    pred_idx_val = scores_masked_val.argmax(dim=1)
                    true_idx_val = y_agent_val.argmax(dim=1)
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè¯¦ç»†åˆ†æé¢„æµ‹åˆ†å¸ƒï¼Œè¯Šæ–­"å…¨ 0 é¢„æµ‹"é—®é¢˜
                    unique_preds, pred_counts = torch.unique(pred_idx_val, return_counts=True)
                    pred_dist = dict(zip(unique_preds.tolist(), pred_counts.tolist()))
                    debug_log(f"  Prediction distribution: {pred_dist}")
                    
                    unique_true, true_counts = torch.unique(true_idx_val, return_counts=True)
                    true_dist = dict(zip(unique_true.tolist(), true_counts.tolist()))
                    debug_log(f"  True label distribution: {true_dist}")
                    
                    # ğŸ”¥ å…³é”®è¯Šæ–­ï¼šæ£€æŸ¥æ˜¯å¦é¢„æµ‹åç¼©åˆ°å•ä¸€å€¼
                    if len(pred_dist) == 1:
                        collapsed_idx = list(pred_dist.keys())[0]
                        collapsed_count = pred_dist[collapsed_idx]
                        debug_log(f"  âš ï¸ [CRITICAL] é¢„æµ‹åç¼©ï¼šæ‰€æœ‰ {collapsed_count} ä¸ªæ ·æœ¬éƒ½é¢„æµ‹ä¸º Agent {collapsed_idx}")
                        debug_log(f"     è¿™é€šå¸¸æ„å‘³ç€æ¨¡å‹é™·å…¥äº†'å¤šæ•°ç±»åç¼©'æˆ–'é»˜è®¤è¾“å‡ºæ¨¡å¼'")
                    
                    # ğŸ”¥ å…³é”®è¯Šæ–­ï¼šæ£€æŸ¥é¢„æµ‹ 0 çš„æ¯”ä¾‹
                    pred_0_count = (pred_idx_val == 0).sum().item()
                    pred_0_ratio = pred_0_count / len(pred_idx_val)
                    debug_log(f"  é¢„æµ‹ä¸º 0 çš„æ¯”ä¾‹: {pred_0_ratio:.2%} ({pred_0_count}/{len(pred_idx_val)})")
                    if pred_0_ratio > 0.8:
                        debug_log(f"  âš ï¸ [WARNING] è¶…è¿‡ 80% çš„æ ·æœ¬é¢„æµ‹ä¸º Agent 0ï¼Œæ¨¡å‹å¯èƒ½åç¼©åˆ°é»˜è®¤è¾“å‡º")
                    
                    debug_log(f"  Predictions (first 10): {pred_idx_val[:10].tolist()}")
                    debug_log(f"  True labels (first 10): {true_idx_val[:10].tolist()}")
                    debug_log(f"  Correct (first 10): {(pred_idx_val[:10] == true_idx_val[:10]).tolist()}")
                    
                    # ğŸ”¥ å…³é”®è¯Šæ–­ï¼šæ£€æŸ¥ logits çš„åˆ†å¸ƒï¼Œçœ‹æ˜¯å¦æœ‰æ˜æ˜¾çš„ bias
                    logits_at_0 = logits_val[:, -1, 0, 0] if logits_val.dim() == 4 else logits_val[:, -1, 0]
                    logits_mean = logits_val[:, -1, :, 0].mean(dim=1) if logits_val.dim() == 4 else logits_val[:, -1, :].mean(dim=1)
                    debug_log(f"  Logits at Agent 0 (mean): {logits_at_0.mean().item():.4f}, std: {logits_at_0.std().item():.4f}")
                    debug_log(f"  Logits mean across all agents: {logits_mean.mean().item():.4f}, std: {logits_mean.std().item():.4f}")
                    if logits_at_0.mean().item() > logits_mean.mean().item() + 0.5:
                        debug_log(f"  âš ï¸ [WARNING] Agent 0 çš„ logits æ˜æ˜¾é«˜äºå¹³å‡å€¼ï¼Œå¯èƒ½å­˜åœ¨ bias")
                    
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ logits å…¨ä¸º 0 æˆ– NaN
                    if torch.isnan(scores_val).any():
                        debug_log(f"  [WARNING] Found NaN in scores!")
                    if (scores_val == 0).all(dim=1).any():
                        debug_log(f"  [WARNING] Some samples have all-zero scores!")
                    if (scores_val.abs() < 1e-6).all(dim=1).any():
                        debug_log(f"  [WARNING] Some samples have near-zero scores!")
                debug_log(f"  Agent accuracy (this batch): {metrics['agent_accuracy']:.6f}")

    num_batches = len(dataloader)
    avg_loss = total_loss / num_batches
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéªŒè¯é›†å‡†ç¡®ç‡è®¡ç®—æ–¹å¼
    # é—®é¢˜ï¼šå¦‚æœæŒ‰ batch å¹³å‡ï¼Œå¯èƒ½æ©ç›–æ¨¡å‹çš„å˜åŒ–
    # è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿å‡†ç¡®ç‡è®¡ç®—æ­£ç¡®ï¼Œå¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if all_metrics['agent_accuracy']:
        avg_agent_acc = np.mean(all_metrics['agent_accuracy'])
        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ª batch çš„å‡†ç¡®ç‡åˆ†å¸ƒï¼ˆä¿å­˜åˆ°æ—¥å¿—ï¼‰
        if len(all_metrics['agent_accuracy']) > 1:
            acc_values = all_metrics['agent_accuracy']
            debug_msg = f"  [Val Debug] Batch accuracies: {[f'{a:.4f}' for a in acc_values[:5]]}... (showing first 5)"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
            debug_msg = f"  [Val Debug] Mean acc: {avg_agent_acc:.6f}, Std: {np.std(acc_values):.6f}"
            if logger:
                logger.log(debug_msg, to_terminal=True)
            else:
                print(debug_msg)
    else:
        avg_agent_acc = 0.0
    
    if all_metrics['step_accuracy']:
        avg_step_acc = np.mean(all_metrics['step_accuracy'])
    else:
        avg_step_acc = 0.0

    return {
        'loss': avg_loss,
        'agent_accuracy': avg_agent_acc,
        'step_accuracy': avg_step_acc
    }


def main():
    SAVE_WINDOW_SIZE = 5  # å®šä¹‰çª—å£å¤§å°
    window_best_acc = 0.0  # åˆå§‹åŒ–å½“å‰çª—å£çš„æœ€ä½³å‡†ç¡®ç‡
    """ä¸»è®­ç»ƒå‡½æ•°"""
    import argparse

    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
    seed_everything(42)

    parser = argparse.ArgumentParser(description='Train ASTRA-MoE model')
    parser.add_argument('--data_dir', type=str, default='outputs', help='Data directory')
    parser.add_argument('--processed_dir', type=str, default='processed_data', help='Processed data directory')
    parser.add_argument('--output_dir', type=str, default='checkpoints_large', help='Output directory for checkpoints')
    parser.add_argument('--max_seq_len', type=int, default=160, help='Maximum sequence length (updated to cover test data max length 130 + 30 margin)')
    parser.add_argument('--max_agents', type=int, default=50, help='Maximum number of agents')  # ğŸ”¥ ä¿®å¤ï¼šä» 10 å¢åŠ åˆ° 50
    parser.add_argument('--batch_size', type=int, default=4, help='Batch size')
    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--d_model', type=int, default=256, help='Model dimension (reduced to prevent overfitting)')
    parser.add_argument('--num_hgt_layers', type=int, default=2, help='Number of HGT layers (deeper to capture 2nd-order neighbor relations)')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='Device')
    parser.add_argument('--force_cpu', action='store_true', help='Force use CPU (for debugging RTX 5070 compatibility issues)')
    parser.add_argument('--resume', type=str, default=None, help='Resume from checkpoint')
    parser.add_argument('--force_reprocess', action='store_true', default=True, help='Force reprocess data (ignore cache)')
    parser.add_argument('--no_force_reprocess', dest='force_reprocess', action='store_false', help='Use cached processed data if available')
    parser.add_argument('--debug_overfit', action='store_true', help='Debug mode: Overfit on single batch (200 epochs, balanced loss weights)')
    # ğŸ”¥ æ·»åŠ æŸå¤±æƒé‡å‚æ•°ï¼ˆç”¨äºæ§åˆ¶å¯¹æ¯”å­¦ä¹ ï¼‰
    parser.add_argument('--w_sup', type=float, default=1.0, help='Supervised loss weight (default: 1.0)')
    parser.add_argument('--w_cl', type=float, default=2.0, help='Contrastive loss weight (default: 2.0, increased to break "all-0 prediction" collapse. Recommended: >= 1.0)')
    parser.add_argument('--w_rl', type=float, default=0.0, help='Reinforcement learning loss weight (default: 0.0, disabled)')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Gradient accumulation steps (default: 1, no accumulation). Effective batch size = batch_size * gradient_accumulation_steps')

    args = parser.parse_args()

    # ğŸ”¥ åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è®­ç»ƒç›®å½•
    base_output_dir = Path(args.output_dir)
    base_output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå¼€å§‹æ—¶é—´æˆ³
    train_start_time = datetime.now()
    timestamp = train_start_time.strftime("%Y%m%d_%H%M%S")
    output_dir = base_output_dir / f"train_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆåœ¨è®­ç»ƒç›®å½•ä¸‹ï¼‰
    log_dir = output_dir / "logs"
    logger = TrainingLogger(log_dir)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè‡ªåŠ¨è®¾ç½® Hand-Crafted è¯Šæ–­æ–‡ä»¶è·¯å¾„åˆ°è®­ç»ƒæ—¥å¿—ç›®å½•
    hc_emb_debug_file = log_dir / "hc_emb_debug.txt"
    hc_match_debug_file = log_dir / "hc_match_debug.txt"
    os.environ['HC_EMB_DEBUG_FILE'] = str(hc_emb_debug_file)
    os.environ['HC_DEBUG_FILE'] = str(hc_match_debug_file)
    
    logger.log(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {train_start_time.strftime('%Y-%m-%d %H:%M:%S')}", to_terminal=True)
    logger.log(f"Checkpoint ä¿å­˜ç›®å½•: {output_dir}", to_terminal=True)
    logger.log(f"è®­ç»ƒæ—¥å¿—ç›®å½•: {log_dir}", to_terminal=True)
    logger.log(f"Hand-Crafted Embedding è¯Šæ–­æ–‡ä»¶: {hc_emb_debug_file}", to_terminal=True)
    logger.log(f"Hand-Crafted åŒ¹é…è¯Šæ–­æ–‡ä»¶: {hc_match_debug_file}", to_terminal=True)

    # è®¾å¤‡ - æ£€æŸ¥ CUDA å…¼å®¹æ€§
    print("=" * 60)
    print("è®¾å¤‡æ£€æŸ¥")
    print("=" * 60)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")

    # æ·±åº¦è¯Šæ–­ GPU çŠ¶æ€
    # æŠ‘åˆ¶ sm_120 è­¦å‘Šï¼ˆå¦‚æœ GPU è®¡ç®—æµ‹è¯•æˆåŠŸï¼Œè­¦å‘Šå¯ä»¥å¿½ç•¥ï¼‰
    import warnings
    warnings.filterwarnings('ignore', category=UserWarning, module='torch.cuda')

    cuda_available = torch.cuda.is_available()
    device_count = 0
    valid_devices = []

    if cuda_available:
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        try:
            device_count = torch.cuda.device_count()
            print(f"GPU æ•°é‡: {device_count}")

            # ç‰¹æ®Šå¤„ç†ï¼šå³ä½¿ device_count ä¸º 0ï¼Œä¹Ÿå°è¯•ç›´æ¥æµ‹è¯• cuda:0
            # å› ä¸ºæŸäº›æƒ…å†µä¸‹ PyTorch å¯èƒ½æŠ¥å‘Š device_count=0 ä½†å®é™…å¯ä»¥ä½¿ç”¨
            if device_count == 0:
                print("âš ï¸  device_count ä¸º 0ï¼Œä½†å°è¯•ç›´æ¥æµ‹è¯• cuda:0...")
                try:
                    # å°è¯•ç›´æ¥åˆ›å»ºå¼ é‡æµ‹è¯•
                    test_tensor = torch.zeros(1).cuda()
                    _ = test_tensor + 1
                    del test_tensor
                    torch.cuda.empty_cache()
                    # å¦‚æœèƒ½æˆåŠŸï¼Œè¯´æ˜ GPU å®é™…ä¸Šå¯ç”¨
                    valid_devices.append(0)
                    device_count = 1
                    print("  âœ“ ç›´æ¥æµ‹è¯•æˆåŠŸï¼GPU å®é™…ä¸Šå¯ç”¨ï¼ˆå¯èƒ½æ˜¯ sm_120 å…¼å®¹æ¨¡å¼ï¼‰")
                    try:
                        gpu_name = torch.cuda.get_device_name(0)
                        capability = torch.cuda.get_device_capability(0)
                        print(f"  GPU è®¾å¤‡: {gpu_name}")
                        print(f"  è®¡ç®—èƒ½åŠ›: {capability}")
                        if capability[0] >= 12:
                            print(f"  âš ï¸  Blackwell æ¶æ„ (sm_{capability[0]}{capability[1]})")
                            print(f"  è™½ç„¶ PyTorch æ˜¾ç¤ºä¸å…¼å®¹ï¼Œä½† GPU è®¡ç®—æµ‹è¯•æˆåŠŸ")
                            print(f"  å°†å°è¯•ä½¿ç”¨ GPUï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
                    except:
                        pass
                except Exception as e:
                    print(f"  âœ— ç›´æ¥æµ‹è¯•å¤±è´¥: {str(e)}")

            # å°è¯•è®¿é—®æ¯ä¸ªè®¾å¤‡ä»¥éªŒè¯æ˜¯å¦çœŸæ­£å¯ç”¨
            for i in range(device_count):
                try:
                    # å°è¯•è·å–è®¾å¤‡å±æ€§
                    gpu_name = torch.cuda.get_device_name(i)
                    capability = torch.cuda.get_device_capability(i)
                    props = torch.cuda.get_device_properties(i)

                    print(f"GPU {i}: {gpu_name}")
                    print(f"  è®¡ç®—èƒ½åŠ›: {capability}")
                    print(f"  æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")

                    # å°è¯•åˆ›å»ºä¸€ä¸ªæµ‹è¯•å¼ é‡éªŒè¯è®¾å¤‡æ˜¯å¦çœŸæ­£å¯ç”¨
                    try:
                        test_tensor = torch.zeros(1, device=f'cuda:{i}')
                        _ = test_tensor + 1
                        del test_tensor
                        torch.cuda.empty_cache()
                        if i not in valid_devices:
                            valid_devices.append(i)
                        print(f"  âœ“ è®¾å¤‡ {i} å¯ç”¨")
                    except RuntimeError as e:
                        print(f"  âœ— è®¾å¤‡ {i} ä¸å¯ç”¨: {str(e)}")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ RTX 50 ç³»åˆ—ï¼ˆsm_120ï¼‰
                    if capability[0] >= 12:
                        print(f"  âš ï¸  æ£€æµ‹åˆ°è®¡ç®—èƒ½åŠ› {capability[0]}.{capability[1]} (Blackwell æ¶æ„)")
                        print(f"  å½“å‰ PyTorch ç‰ˆæœ¬å¯èƒ½æ˜¾ç¤ºä¸å…¼å®¹è­¦å‘Š")
                        print(f"  ä½†å¦‚æœ GPU è®¡ç®—æµ‹è¯•æˆåŠŸï¼Œå°†å°è¯•ä½¿ç”¨ GPU")
                except (AssertionError, RuntimeError, IndexError) as e:
                    print(f"GPU {i}: æ— æ³•è·å–è®¾å¤‡ä¿¡æ¯ ({str(e)})")

            if not valid_devices:
                print("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„ GPU è®¾å¤‡")
                print("å¯èƒ½çš„åŸå› :")
                print("  1. GPU è¢«å…¶ä»–è¿›ç¨‹å ç”¨")
                print("  2. GPU é©±åŠ¨ç‰ˆæœ¬ä¸å…¼å®¹")
                print("  3. CUDA ç‰ˆæœ¬ä¸ PyTorch ä¸åŒ¹é…")
                print("  4. GPU ç¡¬ä»¶æ•…éšœ")
                print("  5. RTX 50 ç³»åˆ— (sm_120) æ¶æ„æš‚ä¸æ”¯æŒ")
        except Exception as e:
            print(f"âš ï¸  è·å– GPU ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            device_count = 0
    else:
        print("CUDA ä¸å¯ç”¨")
        print("å¯èƒ½çš„åŸå› :")
        print("  1. æœªå®‰è£… CUDA")
        print("  2. PyTorch æœªç¼–è¯‘ CUDA æ”¯æŒ")
        print("  3. ç³»ç»Ÿæœªæ£€æµ‹åˆ° NVIDIA GPU")

    print("=" * 60)

    # ğŸ”¥ æ·»åŠ  CPU å¼ºåˆ¶æ¨¡å¼æ”¯æŒ
    if args.force_cpu:
        print("\nâš ï¸  å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆ--force_cpu å·²æŒ‡å®šï¼‰")
        device = torch.device('cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    elif args.device == 'cuda' and cuda_available and len(valid_devices) > 0:
        try:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆè®¾å¤‡
            device_id = valid_devices[0]

            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ device_count ä¸º 0ï¼Œä½¿ç”¨ 'cuda' è€Œä¸æ˜¯ 'cuda:0'
            # å› ä¸ºç´¢å¼•è®¿é—®ä¼šå¤±è´¥ï¼Œä½†é»˜è®¤è®¾å¤‡å¯èƒ½å¯ç”¨
            if device_count == 0:
                # ä½¿ç”¨é»˜è®¤ CUDA è®¾å¤‡ï¼ˆä¸æŒ‡å®šç´¢å¼•ï¼‰
                test_tensor = torch.zeros(1).cuda()
                _ = test_tensor + 1  # æ‰§è¡Œç®€å•è®¡ç®—
                del test_tensor
                torch.cuda.empty_cache()
                device = torch.device('cuda')  # ä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼Œä¸æŒ‡å®šç´¢å¼•
                print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device} (GPU å…¼å®¹æ¨¡å¼ - device_count=0 ä½†è®¡ç®—å¯ç”¨)")
            else:
                # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨æŒ‡å®šè®¾å¤‡ç´¢å¼•
                test_tensor = torch.zeros(1, device=f'cuda:{device_id}')
                _ = test_tensor + 1  # æ‰§è¡Œç®€å•è®¡ç®—
                del test_tensor
                torch.cuda.empty_cache()
                device = torch.device(f'cuda:{device_id}')
                try:
                    gpu_name = torch.cuda.get_device_name(device_id)
                    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device} (GPU: {gpu_name})")
                except:
                    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")
        except RuntimeError as e:
            error_msg = str(e)
            print(f"\nâœ— CUDA æµ‹è¯•å¤±è´¥: {error_msg}")
            print("\n" + "=" * 60)
            print("CUDA å…¼å®¹æ€§é—®é¢˜è¯Šæ–­")
            print("=" * 60)

            # æ£€æŸ¥æ˜¯å¦æ˜¯ sm_120 å…¼å®¹æ€§é—®é¢˜
            if "no kernel image is available" in error_msg.lower() or "not compatible" in error_msg.lower():
                # å®‰å…¨åœ°è·å–è®¾å¤‡èƒ½åŠ›ï¼ˆé¿å…åœ¨è®¾å¤‡æ— æ•ˆæ—¶å´©æºƒï¼‰
                capability = None
                try:
                    if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                        capability = torch.cuda.get_device_capability(0)
                except (AssertionError, RuntimeError):
                    pass  # è®¾å¤‡æ— æ•ˆï¼Œæ— æ³•è·å–èƒ½åŠ›

                if capability and capability[0] >= 12:
                    print("æ£€æµ‹åˆ° RTX 50 ç³»åˆ— GPU (Blackwell æ¶æ„, sm_120)")
                    print("å½“å‰ PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ sm_120 è®¡ç®—èƒ½åŠ›")
                    print("\né‡è¦è¯´æ˜:")
                    print("  RTX 5070 ç­‰ Blackwell æ¶æ„ GPU éœ€è¦ PyTorch ä»æºç ç¼–è¯‘æ”¯æŒ")
                    print("  ç›®å‰å®˜æ–¹å‘å¸ƒçš„ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬ nightlyï¼‰å°šæœªå®Œå…¨æ”¯æŒ sm_120")
                    print("\nä¸´æ—¶è§£å†³æ–¹æ¡ˆ:")
                    print("  1. ä½¿ç”¨ CPU æ¨¡å¼è®­ç»ƒï¼ˆå½“å‰è‡ªåŠ¨å›é€€ï¼‰")
                    print("  2. ç­‰å¾… PyTorch å®˜æ–¹å‘å¸ƒæ”¯æŒ sm_120 çš„ç‰ˆæœ¬")
                    print("  3. ä»æºç ç¼–è¯‘ PyTorchï¼ˆéœ€è¦ CUDA Toolkit å’Œç¼–è¯‘ç¯å¢ƒï¼‰")
                    print("\né•¿æœŸè§£å†³æ–¹æ¡ˆ:")
                    print("  å…³æ³¨ PyTorch GitHub ä»“åº“ï¼Œç­‰å¾…å®˜æ–¹æ”¯æŒ:")
                    print("  https://github.com/pytorch/pytorch/issues")
                    print("\nå½“å‰å°†ä½¿ç”¨ CPU æ¨¡å¼ç»§ç»­è®­ç»ƒ")
                else:
                    print("å¯èƒ½æ˜¯ CUDA ç‰ˆæœ¬ä¸å…¼å®¹æˆ–å…¶ä»–é—®é¢˜")
                    print("å»ºè®®:")
                    print("1. è¿è¡Œ: python check_cuda.py æ£€æŸ¥è¯¦ç»†é”™è¯¯")
                    print("2. è¿è¡Œ: upgrade_pytorch.bat å‡çº§ PyTorch")
            else:
                print("CUDA é”™è¯¯è¯¦æƒ…:", error_msg)
                print("å»ºè®®è¿è¡Œ: python check_cuda.py æ£€æŸ¥è¯¦ç»†é”™è¯¯")

            print("=" * 60)
            print("\nå›é€€åˆ° CPU æ¨¡å¼")
            device = torch.device('cpu')
            print("âš ï¸  è­¦å‘Š: CPU è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä¿®å¤ CUDA é—®é¢˜åä½¿ç”¨ GPU")
        except Exception as e:
            print(f"\nâœ— æ„å¤–çš„ CUDA é”™è¯¯: {e}")
            print("å›é€€åˆ° CPU æ¨¡å¼")
            device = torch.device('cpu')
    else:
        device = torch.device('cpu')
        if args.device == 'cuda':
            if not cuda_available:
                print(f"\nè­¦å‘Š: è¯·æ±‚ä½¿ç”¨ CUDAï¼Œä½† CUDA ä¸å¯ç”¨")
            elif device_count == 0:
                print(f"\nè­¦å‘Š: è¯·æ±‚ä½¿ç”¨ CUDAï¼Œä½†æœªæ£€æµ‹åˆ° GPU è®¾å¤‡")
            elif len(valid_devices) == 0:
                print(f"\nè­¦å‘Š: è¯·æ±‚ä½¿ç”¨ CUDAï¼Œä½†æ‰€æœ‰ GPU è®¾å¤‡éƒ½ä¸å¯ç”¨")
            print("å›é€€åˆ° CPU æ¨¡å¼")
        print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    print()

    # æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    if args.force_reprocess:
        print("âš ï¸  å¼ºåˆ¶é‡æ–°å¤„ç†æ•°æ®ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
        # å¦‚æœå¼ºåˆ¶é‡æ–°å¤„ç†ï¼Œåˆ é™¤æ—§çš„åˆ†ç‰‡ç¼“å­˜ç›®å½•
        # ğŸ”¥ æ³¨æ„ï¼štrain_gpu.sh å·²ç»åˆ é™¤äº†å…¨å±€è¯è¡¨å¹¶é‡æ–°æ„å»º
        # è¿™é‡Œåªåˆ é™¤æ•°æ®ç¼“å­˜ï¼Œä¸åˆ é™¤å…¨å±€è¯è¡¨ï¼ˆå› ä¸º train_gpu.sh å·²ç»é‡æ–°æ„å»ºäº†ï¼‰
        processed_path = Path(args.processed_dir) if args.processed_dir else Path("processed_data")
        cache_dir = processed_path / "cache"
        if cache_dir.exists():
            print(f"  åˆ é™¤æ—§åˆ†ç‰‡ç¼“å­˜ç›®å½•: {cache_dir}")
            import shutil
            shutil.rmtree(cache_dir)
        # ğŸ”¥ æ³¨æ„ï¼šå…¨å±€è¯è¡¨ (converter_state.pt) ç”± train_gpu.sh ç®¡ç†
        # train_gpu.sh ä¼šåœ¨è®­ç»ƒå‰é‡æ–°æ„å»ºå…¨å±€è¯è¡¨ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦åˆ é™¤
    else:
        print("ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰")

    # ================= æ•°æ®åŠ è½½ä¼˜åŒ– =================
    print("\n" + "="*60)
    print("æ•°æ®é›†åˆå§‹åŒ– (è®­ç»ƒ/éªŒè¯åˆ†ç¦»)")
    print("="*60)
    
    # ğŸ”¥ è®­ç»ƒé›†ï¼šå¼€å¯é…å¯¹ (enable_pairing=True)
    # ç”¨äºè®¡ç®— Contrastive Loss
    print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†ï¼ˆé…å¯¹æ¨¡å¼ï¼‰...")
    print("âš ï¸  æ³¨æ„ï¼šæ•°æ®åŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...", flush=True)
    import sys
    sys.stdout.flush()
    
    try:
        full_dataset = WhoWhenDataset(
            data_dir=args.data_dir,
            max_seq_len=args.max_seq_len,
            max_agents=args.max_agents,
            processed_dir=args.processed_dir,
            force_reprocess=args.force_reprocess,
            enable_pairing=True  # ğŸ”¥ é‡ç‚¹ï¼šè®­ç»ƒæ—¶å¼€å¯é…å¯¹
        )
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼æ•°æ®é›†å¤§å°: {len(full_dataset)}", flush=True)
        sys.stdout.flush()
    except KeyboardInterrupt:
        print("\nâŒ æ•°æ®åŠ è½½è¢«ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰", flush=True)
        raise
    except Exception as e:
        print(f"\nâŒ æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()
        raise
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_subset, val_subset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # è®­ç»ƒé›†ï¼šç›´æ¥ä½¿ç”¨Subsetï¼ˆä¿æŒé…å¯¹æ¨¡å¼ï¼‰
    from torch.utils.data import Subset
    train_dataset = train_subset
    
    # ğŸ”¥ éªŒè¯é›†ï¼šåˆ›å»ºåŒ…è£…å™¨ï¼Œå¼ºåˆ¶è¿”å›å•æ ·æœ¬ï¼ˆå³ä½¿åŸdatasetæ˜¯é…å¯¹æ¨¡å¼ï¼‰
    class SingleSampleWrapper:
        """åŒ…è£…å™¨ï¼šå°†é…å¯¹æ•°æ®è½¬æ¢ä¸ºå•æ ·æœ¬æ•°æ®ï¼ˆç”¨äºéªŒè¯/æµ‹è¯•ï¼‰"""
        def __init__(self, subset):
            self.subset = subset
        
        def __len__(self):
            return len(self.subset)
        
        def __getitem__(self, idx):
            sample = self.subset[idx]
            
            # å¦‚æœè¿”å›çš„æ˜¯é…å¯¹æ•°æ®ï¼Œåªè¿”å›mutatedéƒ¨åˆ†
            if isinstance(sample, dict) and 'mutated' in sample:
                return sample['mutated']
            # å¦åˆ™ç›´æ¥è¿”å›ï¼ˆå·²ç»æ˜¯å•æ ·æœ¬ï¼‰
            return sample
    
    val_dataset = SingleSampleWrapper(val_subset)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)} (é…å¯¹æ¨¡å¼)")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)} (å•æ ·æœ¬æ¨¡å¼)")
    print("="*60 + "\n")

    # æ·»åŠ è°ƒè¯•ï¼šæ£€æŸ¥æ ‡ç­¾ç»Ÿè®¡ï¼ˆä»…è®­ç»ƒé›†ï¼‰
    print("\næ£€æŸ¥è®­ç»ƒé›†æ ‡ç­¾ç»Ÿè®¡ï¼ˆå‰10ä¸ªæ ·æœ¬ï¼‰...")
    y_agent_count = 0
    y_step_count = 0
    for i in range(min(10, len(train_dataset))):
        sample = train_dataset[i]
        # è®­ç»ƒé›†åœ¨é…å¯¹æ¨¡å¼ä¸‹è¿”å› {'mutated': ..., 'healed': ...}
        if isinstance(sample, dict) and 'mutated' in sample:
            labels = sample['mutated'].get('labels', {})
        else:
            labels = sample.get('labels', {})
        y_agent = labels.get('y_agent', -1)
        y_step = labels.get('y_step', -1)
        if y_agent >= 0:
            y_agent_count += 1
        if y_step >= 0:
            y_step_count += 1
        if i < 3:  # æ‰“å°å‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            print(f"  æ ·æœ¬ {i}: y_agent={y_agent}, y_step={y_step}, "
                  f"mistake_agent={labels.get('mistake_agent_name', 'N/A')}, "
                  f"mistake_step={labels.get('mistake_step_str', 'N/A')}")
    print(f"  æ ‡ç­¾ç»Ÿè®¡: y_agentæœ‰æ•ˆ={y_agent_count}/10, y_stepæœ‰æ•ˆ={y_step_count}/10")
    if y_agent_count == 0:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ y_agent æ ‡ç­¾ï¼")
    if y_step_count == 0:
        print("  âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ y_step æ ‡ç­¾ï¼")
    print()

    # ğŸ”¥ ä¼˜åŒ–ï¼šæ£€æŸ¥ Batch Size æ˜¯å¦è¶³å¤Ÿå¤§ï¼ˆå¯¹æ¯”å­¦ä¹ éœ€è¦ï¼‰
    print("\n" + "="*60)
    print("Batch Size æ£€æŸ¥ (å¯¹æ¯”å­¦ä¹ ä¾èµ–)")
    print("="*60)
    if args.batch_size < 16:
        print(f"âš ï¸  è­¦å‘Š: å½“å‰ batch_size={args.batch_size} < 16")
        print("  å¯¹æ¯”å­¦ä¹  (SupCon) éœ€è¦è¶³å¤Ÿå¤§çš„ batch size æ‰èƒ½æ‰¾åˆ°æ­£æ ·æœ¬å¯¹")
        print("  å¦‚æœ batch_size å¤ªå°ï¼Œä¸€ä¸ª batch é‡Œå¯èƒ½æ ¹æœ¬æ²¡æœ‰åŒç±»çš„æ­£æ ·æœ¬å¯¹")
        print("  åæœ: mask_positive å…¨ä¸º 0ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å—ç›´æ¥å¤±æ•ˆ")
        print("\n  å»ºè®®:")
        print("    1. å¦‚æœæ˜¾å­˜å…è®¸ï¼Œå°† batch_size å¢åŠ åˆ° >= 16")
        print("    2. å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè€ƒè™‘ä½¿ç”¨ Gradient Accumulation (æ¢¯åº¦ç´¯ç§¯)")
        print("      ä¾‹å¦‚: --batch_size 4 --gradient_accumulation_steps 4 (ç­‰æ•ˆ batch_size=16)")
        print("    3. æˆ–è€…æš‚æ—¶ç¦ç”¨å¯¹æ¯”å­¦ä¹  (è®¾ç½® W_CL=0)")
        print("="*60 + "\n")
    else:
        print(f"âœ“ Batch size={args.batch_size} >= 16ï¼Œæ»¡è¶³å¯¹æ¯”å­¦ä¹ è¦æ±‚")
        print("="*60 + "\n")

    # æ•°æ®åŠ è½½å™¨
    # ğŸ”¥ ASTRA-CL: è®­ç»ƒé›†å¯ç”¨é…å¯¹æ¨¡å¼ï¼ŒéªŒè¯é›†ç¦ç”¨é…å¯¹æ¨¡å¼
    print("\n" + "="*60)
    print("æ•°æ®åŠ è½½å™¨é…ç½®")
    print("="*60)
    # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç¦ç”¨é…å¯¹æ¨¡å¼
    has_pairs = (hasattr(train_dataset, 'pairs') and 
                train_dataset.pairs and 
                len(train_dataset.pairs) > 0)
    
    if has_pairs:
        print("è®­ç»ƒé›†: enable_pairing=True (ç”¨äºå¯¹æ¯”å­¦ä¹ )")
    else:
        print("è®­ç»ƒé›†: enable_pairing=False (æ²¡æœ‰é…å¯¹æ•°æ®ï¼Œå¯¹æ¯”å­¦ä¹ ä¸å¯ç”¨)")
    print("éªŒè¯é›†: enable_pairing=False (æ¨¡æ‹ŸçœŸå®æ¨ç†åœºæ™¯)")
    print("="*60 + "\n")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_fn(b, max_seq_len=args.max_seq_len, max_agents=args.max_agents, is_paired=has_pairs)  # ğŸ”¥ æ ¹æ®æ˜¯å¦æœ‰é…å¯¹æ•°æ®å†³å®š
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=lambda b: collate_fn(b, max_seq_len=args.max_seq_len, max_agents=args.max_agents, is_paired=False)  # ğŸ”¥ éªŒè¯é›†ï¼šå…³é—­é…å¯¹
    )

    # æ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = ASTRAMoE(
        node_feat_dim=8192,  # ğŸ”¥ Qwen3-8B: 4096 (åµŒå…¥) + 4096 (å…ƒæ•°æ®)
        edge_feat_dim=32,
        d_model=256,  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œé»˜è®¤64
        num_heads=4,
        num_hgt_layers=args.num_hgt_layers,  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œé»˜è®¤2
        num_temporal_layers=2,
        num_experts=4,
        num_classes=1,  # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä» args.max_agents æ”¹ä¸º 1ï¼ˆæ¯ä¸ª Agent è¾“å‡ºä¸€ä¸ªæ•…éšœåˆ†æ•°ï¼‰
        dropout=args.dropout,  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œé»˜è®¤0.5
        max_seq_len=args.max_seq_len
    ).to(device)

    # è¯Šæ–­ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨ GPU ä¸Š
    model_device = next(model.parameters()).device
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"æ¨¡å‹å®é™…è®¾å¤‡: {model_device}")
    if model_device.type == 'cuda':
        print(f"âœ“ æ¨¡å‹å·²æˆåŠŸç§»åŠ¨åˆ° GPU: {model_device}")
        # æµ‹è¯• GPU è®¡ç®—
        try:
            test_tensor = torch.randn(10, 10, device=device)
            result = torch.matmul(test_tensor, test_tensor)
            print(f"âœ“ GPU è®¡ç®—æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            print("   å°†å›é€€åˆ° CPU æ¨¡å¼")
            device = torch.device('cpu')
            model = model.to(device)
    else:
        print(f"âš ï¸  æ¨¡å‹åœ¨ CPU ä¸Šï¼Œè®­ç»ƒé€Ÿåº¦ä¼šè¾ƒæ…¢")

    # æŸå¤±å‡½æ•°
    # ğŸ”¥ è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å¹³è¡¡çš„æƒé‡
    if args.debug_overfit:
        print("\n" + "="*60)
        print("ğŸ”§ è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼ (Debug Overfit Mode)")
        print("="*60)
        print("  - Loss æƒé‡: w_agent=1.0, w_step=1.0, w_aux=0.01 (å¹³è¡¡)")
        print("  - å­¦ä¹ ç‡: 5e-4 (å›ºå®šï¼Œæ— è°ƒåº¦å™¨)")
        print("  - è®­ç»ƒ: ä»…ä½¿ç”¨ç¬¬ä¸€ä¸ª batchï¼Œ200 epochs")
        print("  - ç›®æ ‡: Loss åº”é™è‡³ 0.001ï¼ŒAccuracy åº”è¾¾åˆ° 100%")
        print("="*60 + "\n")
        loss_fn = ASTRALoss(
            w_agent=5.0,   # å¹³è¡¡æƒé‡
            w_step=1.0,    # å¹³è¡¡æƒé‡ï¼ˆä¸å†ä¸»å¯¼ï¼‰
            w_aux=0.01,
            focal_alpha=0.25,
            focal_gamma=2.0,
            mask_agent0=True  # ğŸ”¥ å¯ç”¨å»åæœºåˆ¶
        )
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼ï¼šå¹³è¡¡ Loss æƒé‡ï¼Œä¿®å¤æƒé‡å¤±è¡¡é—®é¢˜
        # ğŸ”¥ ä¿®å¤ï¼šStep Loss æ˜¯ Agent Loss çš„ 50 å€ï¼Œå¯¼è‡´æ¨¡å‹åç§‘
        # æ–°ç­–ç•¥ï¼šæé«˜ Agent æƒé‡ï¼Œé™ä½ Step æƒé‡ï¼Œå¼ºåˆ¶å¹³è¡¡
        loss_fn = ASTRALoss(
            w_agent=10.0,  # ğŸ”¥ æ¿€è¿›çš„ Agent æƒé‡ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨ Agent é¢„æµ‹
            w_step=0.1,    # ğŸ”¥ å‹åˆ¶ Step Lossï¼Œé¿å…ä¸»å¯¼è®­ç»ƒ
            w_aux=0.0,
            focal_alpha=0.25,
            focal_gamma=5.0,  # ğŸ”¥ æé«˜çš„ Gammaï¼Œåªå…³æ³¨éš¾æ ·æœ¬ï¼ˆé 0 æ ·æœ¬ï¼‰
            mask_agent0=True  # ğŸ”¥ å¯ç”¨å»åæœºåˆ¶ï¼Œæ‰“ç ´æ¨¡å‹åç¼©
        )
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ‰“å°æŸå¤±å‡½æ•°é…ç½®ï¼Œç¡®ä¿ Focal Loss å‚æ•°æ­£ç¡®
        if logger:
            logger.log(f"\n[Loss Config] Focal Loss gamma={5.0} (æé«˜å€¼ï¼Œåªå…³æ³¨éš¾æ ·æœ¬)", to_terminal=True)
            logger.log(f"[Loss Config] Agent weight={10.0}, Step weight={0.1}, Aux weight={0.0}", to_terminal=True)
            logger.log(f"[Loss Config] Contrastive weight (W_CL)={args.w_cl} (å»ºè®® >= 1.0 ä»¥æ‰“ç ´åç¼©ï¼Œå½“å‰: {args.w_cl})", to_terminal=True)
            logger.log(f"[Loss Config] mask_agent0=True (å¯ç”¨å»åæœºåˆ¶ï¼ŒæŠ‘åˆ¶ Agent 0 é¢„æµ‹)", to_terminal=True)
            if args.w_cl < 1.0:
                logger.log(f"[WARNING] W_CL={args.w_cl} < 1.0ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åç¼©åˆ°å…¨ 0 é¢„æµ‹ï¼å»ºè®®å¢åŠ åˆ° 1.0 æˆ–æ›´é«˜", to_terminal=True)

    # ä¼˜åŒ–å™¨
    if args.debug_overfit:
        # è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡ï¼Œå›ºå®šå­¦ä¹ ç‡ï¼ˆæ— è°ƒåº¦å™¨ï¼‰
        optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)
        scheduler = None  # ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        print(f"  ä¼˜åŒ–å™¨: AdamW, lr=5e-4 (å›ºå®š)")
    else:
        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=1e-3)
        scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)

    # æ¢å¤æ£€æŸ¥ç‚¹
    start_epoch = 0
    best_val_acc = 0.0

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_acc = checkpoint.get('best_val_acc', 0.0)
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: epoch {start_epoch}, æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}")

    # è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ...")
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_dataset) == 0:
        raise RuntimeError("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼æ— æ³•å¼€å§‹è®­ç»ƒã€‚")
    
    print(f"âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)} ä¸ªæ ·æœ¬", flush=True)
    print(f"âœ… éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset)} ä¸ªæ ·æœ¬", flush=True)
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥DataLoader
    if len(train_loader) == 0:
        raise RuntimeError("âŒ è®­ç»ƒDataLoaderä¸ºç©ºï¼æ— æ³•å¼€å§‹è®­ç»ƒã€‚")
    
    print(f"âœ… è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}", flush=True)
    print(f"âœ… éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}", flush=True)
    print("", flush=True)

    # Early Stopping ç›¸å…³å˜é‡
    patience = 25  # ğŸ”¥ å¢åŠ  patienceï¼šä» 10 å¢åŠ åˆ° 25ï¼Œç»™æ¨¡å‹æ›´å¤šå­¦ä¹ æ—¶é—´
    best_val_loss = float('inf')
    patience_counter = 0

    try:
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ª batch çš„ y_step ä¿¡æ¯
        print("\n" + "="*80)
        print("è°ƒè¯•ä¿¡æ¯: æ£€æŸ¥ç¬¬ä¸€ä¸ªè®­ç»ƒ batch çš„ y_step æ ‡ç­¾")
        print("="*80)
        first_batch = next(iter(train_loader))
        y_step_batch = first_batch.get('y_step', torch.tensor([]))
        seq_mask_batch = first_batch.get('seq_mask', torch.tensor([]))
        graph_lists = first_batch.get('graph_list', [])

        print(f"ç¬¬ä¸€ä¸ª batch å¤§å°: {len(graph_lists)}")
        print(f"y_step å€¼: {y_step_batch.tolist()}")
        if seq_mask_batch.numel() > 0:
            seq_lens = seq_mask_batch.sum(dim=1).tolist()
            print(f"æ¯ä¸ªæ ·æœ¬çš„åºåˆ—é•¿åº¦: {seq_lens}")
            print(f"seq_mask å½¢çŠ¶: {seq_mask_batch.shape}")

        # æ£€æŸ¥æ¯ä¸ªæ ·æœ¬
        valid_y_step_count = 0
        for i in range(len(graph_lists)):
            y_step_i = y_step_batch[i].item() if y_step_batch.numel() > i else -1
            seq_len_i = len(graph_lists[i]) if i < len(graph_lists) else 0
            valid_timesteps = seq_mask_batch[i].sum().item() if seq_mask_batch.numel() > 0 and i < seq_mask_batch.shape[0] else 0

            print(f"\n  æ ·æœ¬ {i}:")
            print(f"    y_step: {y_step_i}")
            print(f"    å›¾åºåˆ—é•¿åº¦: {seq_len_i}")
            print(f"    æœ‰æ•ˆæ—¶é—´æ­¥æ•° (seq_mask): {valid_timesteps}")

            if y_step_i >= 0:
                valid_y_step_count += 1
                if y_step_i >= seq_len_i:
                    print(f"    âš ï¸  è­¦å‘Š: y_step ({y_step_i}) >= å›¾åºåˆ—é•¿åº¦ ({seq_len_i})")
                if y_step_i >= valid_timesteps:
                    print(f"    âš ï¸  è­¦å‘Š: y_step ({y_step_i}) >= æœ‰æ•ˆæ—¶é—´æ­¥æ•° ({valid_timesteps})")

        print(f"\næœ‰æ•ˆ y_step æ ‡ç­¾æ•°é‡: {valid_y_step_count}/{len(graph_lists)}")
        if valid_y_step_count == 0:
            print("  âŒ æ‰€æœ‰ y_step æ ‡ç­¾éƒ½æ˜¯ -1ï¼è¿™æ˜¯ Step Loss ä¸º 0 çš„åŸå› ã€‚")
            print("  å»ºè®®:")
            print("    1. è¿è¡Œ: python debug_labels.py æ£€æŸ¥æ ‡ç­¾ç”Ÿæˆè¿‡ç¨‹")
            print("    2. è¿è¡Œ: python check_step_issue.py æ£€æŸ¥æ•°æ®åŠ è½½")
            print("    3. æ£€æŸ¥ data_adapter.py ä¸­çš„ convert æ–¹æ³•æ˜¯å¦æ­£ç¡®å¤„ç† mistake_step")
        print("="*80 + "\n")

        # ğŸ”¥ è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨ç¬¬ä¸€ä¸ª batchï¼Œè®­ç»ƒ 200 ä¸ª epoch
        if args.debug_overfit:
            print("\n" + "="*60)
            print("ğŸš€ å¼€å§‹è¿‡æ‹Ÿåˆæµ‹è¯•ï¼šå• Batch è®­ç»ƒ")
            print("="*60)
            
            # è·å–ç¬¬ä¸€ä¸ª batch
            single_batch = next(iter(train_loader))
            print(f"  ä½¿ç”¨ Batch å¤§å°: {len(single_batch['graph_list'])}")
            print(f"  è®­ç»ƒ Epochs: 200")
            print("="*60 + "\n")
            
            # åˆ›å»ºå• batch çš„ DataLoaderï¼ˆç”¨äº train_epochï¼‰
            # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ train_epoch æ¥æ”¯æŒå• batch æ¨¡å¼ï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨
            # æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ›å»ºä¸€ä¸ªåªåŒ…å«ä¸€ä¸ª batch çš„ DataLoader
            # æ³¨æ„ï¼šDataLoader å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸éœ€è¦å†æ¬¡å¯¼å…¥
            
            # ä¸ºäº†å…¼å®¹ train_epochï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„ DataLoader
            # ä½† train_epoch æœŸæœ›çš„æ˜¯åŒ…å« graph_list ç­‰å­—æ®µçš„ batch
            # æ‰€ä»¥æˆ‘ä»¬ç›´æ¥åœ¨è¿™é‡Œå®ç°å• batch è®­ç»ƒå¾ªç¯
            
            overfit_epochs = 200
            for epoch in range(overfit_epochs):
                # ç›´æ¥åœ¨è¿™ä¸ª batch ä¸Šè®­ç»ƒ
                model.train()
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                graph_lists = single_batch['graph_list']
                y_agent = single_batch['y_agent'].to(device)
                y_step = single_batch['y_step'].to(device)
                agent_mask = single_batch['agent_mask'].to(device)
                seq_mask = single_batch['seq_mask'].to(device)
                
                # å‰å‘ä¼ æ’­
                batch_outputs = []
                for graph_list in graph_lists:
                    graph_list_device = [graph.to(device) for graph in graph_list]
                    output = model(graph_list_device)
                    batch_outputs.append(output)
                
                # åˆå¹¶æ‰¹å¤„ç†è¾“å‡ºï¼ˆå¤ç”¨ train_epoch çš„é€»è¾‘ï¼‰
                B = len(graph_lists)
                max_T = max(out['logits'].shape[0] for out in batch_outputs)
                max_N = max(out['logits'].shape[1] for out in batch_outputs)
                num_classes = batch_outputs[0]['logits'].shape[2]
                num_experts = batch_outputs[0]['gate_weights'].shape[2]
                
                # æ£€æŸ¥ y_step æ˜¯å¦éœ€è¦æ‰©å±• max_T
                y_step_cpu = single_batch['y_step']
                max_y_step = y_step_cpu.max().item() if y_step_cpu.numel() > 0 and y_step_cpu.max() >= 0 else -1
                if max_y_step >= 0 and max_y_step >= max_T:
                    max_T = max_y_step + 1
                
                # åˆå§‹åŒ–æ‰¹å¤„ç†å¼ é‡
                logits_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['logits'].dtype)
                alpha_batch = torch.zeros(B, max_T, max_N, num_classes, device=device, dtype=batch_outputs[0]['alpha'].dtype)
                gate_weights_batch = torch.zeros(B, max_T, max_N, num_experts, device=device, dtype=batch_outputs[0]['gate_weights'].dtype)
                
                output_seq_mask = torch.zeros(B, max_T, dtype=torch.bool, device=device)
                output_agent_mask = torch.zeros(B, max_T, max_N, dtype=torch.bool, device=device)
                
                # å¡«å……æ¯ä¸ªæ ·æœ¬çš„è¾“å‡º
                for i, out in enumerate(batch_outputs):
                    T_i = out['logits'].shape[0]
                    N_i = out['logits'].shape[1]
                    logits_batch[i, :T_i, :N_i, :] = out['logits']
                    alpha_batch[i, :T_i, :N_i, :] = out['alpha']
                    gate_weights_batch[i, :T_i, :N_i, :] = out['gate_weights']
                    output_seq_mask[i, :T_i] = True
                    output_agent_mask[i, :T_i, :N_i] = True
                    y_step_i = y_step[i].item() if y_step.numel() > i else -1
                    if y_step_i >= 0 and y_step_i < max_T:
                        output_seq_mask[i, y_step_i] = True
                
                # load å¤„ç†
                load_list = [out['load'] for out in batch_outputs]
                if load_list[0].dim() == 1:
                    load_batch = torch.stack(load_list, dim=0)
                elif load_list[0].dim() == 2:
                    max_T_load = max(load.shape[0] for load in load_list)
                    load_batch = torch.zeros(B, max_T_load, num_experts, device=device, dtype=load_list[0].dtype)
                    for i, load in enumerate(load_list):
                        T_load = load.shape[0]
                        load_batch[i, :T_load, :] = load
                else:
                    load_batch = torch.stack(load_list, dim=0)
                
                # step_logits å¤„ç†ï¼ˆä¸ train_epoch ä¿æŒä¸€è‡´ï¼‰
                step_logits_batch = None
                if 'step_logits' in batch_outputs[0]:
                    step_logits_list = [out['step_logits'] for out in batch_outputs]
                    # step_logits å½¢çŠ¶æ˜¯ [T]ï¼Œéœ€è¦å¯¹é½åˆ° max_T
                    # ä½¿ç”¨ -inf å¡«å……è¶Šç•Œæ—¶é—´æ­¥ï¼Œè¡¨ç¤ºè¿™äº›æ—¶é—´æ­¥ä¸å¯é¢„æµ‹ï¼ˆä¸ train_epoch ä¿æŒä¸€è‡´ï¼‰
                    step_logits_batch = torch.full((B, max_T), float('-inf'), device=device, dtype=step_logits_list[0].dtype)
                    for i, step_logits in enumerate(step_logits_list):
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®é™…çš„åºåˆ—é•¿åº¦ï¼ˆä» logits è·å–ï¼‰ï¼Œè€Œä¸æ˜¯ step_logits çš„é•¿åº¦
                        # å› ä¸ºæ¨¡å‹è¾“å‡ºçš„ step_logits é•¿åº¦å¯èƒ½ä¸åºåˆ—é•¿åº¦ä¸åŒ¹é…
                        T_i_actual = batch_outputs[i]['logits'].shape[0]  # å®é™…çš„åºåˆ—é•¿åº¦
                        T_i_step = step_logits.shape[0]  # step_logits çš„é•¿åº¦
                        
                        # å¦‚æœ step_logits é•¿åº¦å°äºåºåˆ—é•¿åº¦ï¼Œéœ€è¦å¡«å……
                        if T_i_step < T_i_actual:
                            # å¡«å……åˆ°åºåˆ—é•¿åº¦
                            padding = torch.full((T_i_actual - T_i_step,), float('-inf'), device=device, dtype=step_logits.dtype)
                            step_logits = torch.cat([step_logits, padding], dim=0)
                            T_i_step = T_i_actual
                        elif T_i_step > T_i_actual:
                            # æˆªæ–­åˆ°åºåˆ—é•¿åº¦
                            step_logits = step_logits[:T_i_actual]
                            T_i_step = T_i_actual
                        
                        # å¤åˆ¶åˆ°æ‰¹å¤„ç†å¼ é‡ï¼ˆæœ€å¤šåˆ° max_Tï¼‰
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                        copy_len = min(T_i_step, max_T, step_logits.shape[0])
                        if copy_len > 0:
                            step_logits_batch[i, :copy_len] = step_logits[:copy_len]
                        # å¦‚æœ y_step è¶Šç•Œï¼Œç¡®ä¿è¯¥ä½ç½®çš„æ©ç ä¹Ÿè¢«æ­£ç¡®è®¾ç½®ï¼ˆå·²åœ¨å‰é¢å¤„ç†ï¼‰
                
                # Agent ç»´åº¦å¯¹é½
                if y_agent.shape[1] > max_N:
                    y_agent = y_agent[:, :max_N]
                    agent_mask = agent_mask[:, :max_N]
                elif y_agent.shape[1] < max_N:
                    pad_size = max_N - y_agent.shape[1]
                    y_agent = F.pad(y_agent, (0, pad_size, 0, 0), value=0)
                    agent_mask = F.pad(agent_mask, (0, pad_size, 0, 0), value=False)
                
                # æ›´æ–°æ¨¡å‹è¾“å‡º
                model_outputs = {
                    'logits': logits_batch,
                    'alpha': alpha_batch,
                    'gate_weights': gate_weights_batch,
                    'load': load_batch
                }
                if step_logits_batch is not None:
                    model_outputs['step_logits'] = step_logits_batch
                
                masks = {
                    'agent_mask': agent_mask,
                    'seq_mask': output_seq_mask,
                }
                
                targets = {
                    'y_agent': y_agent,
                    'y_step': y_step,
                }
                
                # è®¡ç®—æŸå¤±
                loss_dict = loss_fn(model_outputs, targets, masks)
                loss = loss_dict['total_loss']
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä½¿ç”¨ä¸ train_epoch ç›¸åŒçš„ compute_metrics å‡½æ•°ï¼‰
                with torch.no_grad():
                    # æ„å»º masksï¼ˆcompute_metrics æœŸæœ› agent_mask æ˜¯ [B, N]ï¼‰
                    # ä½¿ç”¨åŸå§‹çš„ agent_maskï¼ˆä» batch ä¸­è·å–ï¼Œå·²ç»æ˜¯ [B, N]ï¼‰
                    metrics_masks = {
                        'agent_mask': agent_mask,  # [B, N]
                        'seq_mask': output_seq_mask,  # [B, T]
                    }
                    
                    metrics = compute_metrics(model_outputs, targets, metrics_masks)
                    agent_acc = metrics['agent_accuracy']
                    step_acc = metrics['step_accuracy']
                
                # æ‰“å°æŒ‡æ ‡ï¼ˆæ¯ 10 ä¸ª epoch æ‰“å°ä¸€æ¬¡ï¼Œæˆ–å‰ 20 ä¸ª epoch æ¯æ¬¡éƒ½æ‰“å°ï¼‰
                if (epoch + 1) % 10 == 0 or epoch < 20:
                    print(f"Epoch {epoch+1:3d}/200 | Loss: {loss.item():.6f} | "
                          f"L_agent: {loss_dict['agent_loss'].item():.6f} | "
                          f"L_step: {loss_dict['step_loss'].item():.6f} | "
                          f"Agent Acc: {agent_acc:.4f} | Step Acc: {step_acc:.4f}")
                
                # å¦‚æœè¾¾åˆ°å®Œç¾å‡†ç¡®ç‡ï¼Œæå‰ç»“æŸ
                if agent_acc >= 1.0 and step_acc >= 1.0 and loss.item() < 0.001:
                    print(f"\nâœ… è¿‡æ‹ŸåˆæˆåŠŸï¼Epoch {epoch+1}: Loss={loss.item():.6f}, Agent Acc={agent_acc:.4f}, Step Acc={step_acc:.4f}")
                    break
            
            print("\n" + "="*60)
            print("è¿‡æ‹Ÿåˆæµ‹è¯•å®Œæˆ")
            print("="*60)
            print(f"æœ€ç»ˆ Loss: {loss.item():.6f}")
            print(f"æœ€ç»ˆ Agent Acc: {agent_acc:.4f}")
            print(f"æœ€ç»ˆ Step Acc: {step_acc:.4f}")
            if agent_acc >= 0.95 and step_acc >= 0.95:
                print("âœ… æ¨¡å‹å…·å¤‡å­¦ä¹ èƒ½åŠ›ï¼ˆä»£ç é€»è¾‘æ­£ç¡®ï¼‰")
            else:
                print("âš ï¸  æ¨¡å‹æœªèƒ½å®Œå…¨è¿‡æ‹Ÿåˆï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ä»£ç é€»è¾‘")
            print("="*60 + "\n")
            
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            for epoch in range(start_epoch, args.epochs):
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯ä¸ªepochå¼€å§‹æ—¶æ‰“å°ç¡®è®¤
                print(f"\n{'='*60}", flush=True)
                print(f"Epoch {epoch+1}/{args.epochs}", flush=True)
                print(f"{'='*60}", flush=True)
                import sys
                sys.stdout.flush()
                
                # è®­ç»ƒ
                try:
                    train_metrics = train_epoch(model, train_loader, loss_fn, optimizer, device, epoch, logger,
                                               w_sup=args.w_sup, w_cl=args.w_cl, w_rl=args.w_rl,
                                               gradient_accumulation_steps=args.gradient_accumulation_steps)
                except Exception as e:
                    error_msg = f"âŒ è®­ç»ƒEpoch {epoch+1} å¤±è´¥: {type(e).__name__}: {str(e)}"
                    print(error_msg, flush=True)
                    import traceback
                    traceback.print_exc()
                    if logger:
                        logger.log(error_msg, to_terminal=True)
                        logger.log(f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}", to_terminal=False)
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œåœæ­¢è®­ç»ƒ

                # éªŒè¯
                try:
                    val_metrics = validate(model, val_loader, loss_fn, device, logger)
                except Exception as e:
                    error_msg = f"âŒ éªŒè¯Epoch {epoch+1} å¤±è´¥: {type(e).__name__}: {str(e)}"
                    print(error_msg, flush=True)
                    import traceback
                    traceback.print_exc()
                    if logger:
                        logger.log(error_msg, to_terminal=True)
                        logger.log(f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}", to_terminal=False)
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œåœæ­¢è®­ç»ƒ

                # æ›´æ–°å­¦ä¹ ç‡
                current_lr = None
                if scheduler is not None:
                    scheduler.step()
                    current_lr = optimizer.param_groups[0]['lr']

                # ä½¿ç”¨loggerè®°å½•epochæŒ‡æ ‡ï¼ˆç»ˆç«¯æ˜¾ç¤ºç®€æ´ç‰ˆï¼Œæ–‡ä»¶ä¿å­˜è¯¦ç»†ç‰ˆï¼‰
                logger.log_epoch_metrics(epoch, args.epochs, train_metrics, val_metrics, current_lr)

                # Early Stopping æ£€æŸ¥
                current_val_loss = val_metrics['loss']
                if current_val_loss < best_val_loss:
                    best_val_loss = current_val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    logger.log(f"éªŒè¯æŸå¤±æœªä¸‹é™ (è¿ç»­ {patience_counter}/{patience} ä¸ª epoch)", to_terminal=False)

                # ä¿å­˜æ£€æŸ¥ç‚¹å­—å…¸ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼‰
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
                    'best_val_acc': best_val_acc,
                    'train_metrics': train_metrics,
                    'val_metrics': val_metrics,
                    'config': {
                        'data_dir': args.data_dir,
                        'max_seq_len': args.max_seq_len,
                        'max_agents': args.max_agents,
                        'batch_size': args.batch_size,
                        'lr': args.lr,
                        'num_epochs': args.epochs,
                        'train_start_time': timestamp,
                        # ğŸ”¥ ä¿å­˜æ¨¡å‹ç»“æ„å‚æ•°ï¼Œç¡®ä¿è¯„ä¼°æ—¶èƒ½æ­£ç¡®åŠ è½½
                        'model_config': {
                            'node_feat_dim': 8192,  # ğŸ”¥ Qwen3-8B: 4096 (åµŒå…¥) + 4096 (å…ƒæ•°æ®)
                            'edge_feat_dim': 32,  # å›ºå®šå€¼
                            'd_model': args.d_model,
                            'num_heads': 4,  # å½“å‰ç¡¬ç¼–ç å€¼
                            'num_hgt_layers': args.num_hgt_layers,
                            'num_temporal_layers': 2,  # å½“å‰ç¡¬ç¼–ç å€¼
                            'num_experts': 4,  # å½“å‰ç¡¬ç¼–ç å€¼
                            'num_classes': 1,  # ğŸ”¥ ä¿®å¤ï¼šæ”¹ä¸º 1ï¼ˆæ¯ä¸ª Agent è¾“å‡ºä¸€ä¸ªæ•…éšœåˆ†æ•°ï¼‰
                            'dropout': args.dropout,
                            'max_seq_len': args.max_seq_len
                        }
                    }
                }

                # 1. å§‹ç»ˆä¿å­˜æœ€æ–°çš„ (Latest) - æ–¹ä¾¿æ„å¤–ä¸­æ–­æ¢å¤
                data_name = Path(args.data_dir).name
                latest_file = output_dir / f'latest_{data_name}_{timestamp}.pt'
                # ğŸ”¥ ä¿®å¤ï¼šå¼ºåˆ¶åˆ›å»ºç›®å½•ï¼ˆé˜²æ­¢ä¿å­˜å¤±è´¥ï¼‰
                latest_file.parent.mkdir(parents=True, exist_ok=True)
                torch.save(checkpoint, latest_file)

                # 2. å§‹ç»ˆä¿å­˜å…¨å±€æœ€å¥½çš„ (Global Best)
                if val_metrics['agent_accuracy'] > best_val_acc:
                    best_val_acc = val_metrics['agent_accuracy']
                    checkpoint['best_val_acc'] = best_val_acc
                    best_file = output_dir / f'best_global_acc{best_val_acc:.4f}_{timestamp}.pt'
                    # ğŸ”¥ ä¿®å¤ï¼šå¼ºåˆ¶åˆ›å»ºç›®å½•ï¼ˆé˜²æ­¢ä¿å­˜å¤±è´¥ï¼‰
                    best_file.parent.mkdir(parents=True, exist_ok=True)
                    torch.save(checkpoint, best_file)
                    logger.log(f"  ğŸŒŸ å…¨å±€æœ€ä½³æ›´æ–°: {best_file.name} (Acc: {best_val_acc:.4f})", to_terminal=True)

                # 3. ä¿å­˜æ¯5ä¸ªEpoché‡Œçš„å±€éƒ¨æœ€ä¼˜ (Local Best within Window)
                current_acc = val_metrics['agent_accuracy']
                window_idx = (epoch) // SAVE_WINDOW_SIZE  # ç¬¬å‡ ä¸ªçª—å£ (0, 1, 2...)

                # å¦‚æœå½“å‰ Acc æ¯”å½“å‰çª—å£è®°å½•çš„æœ€å¥½å€¼è¿˜é«˜ï¼Œå°±è¦†ç›–ä¿å­˜
                if current_acc > window_best_acc:
                    window_best_acc = current_acc
                    # è¦†ç›–å½“å‰çª—å£çš„æœ€ä½³æ–‡ä»¶ï¼ˆæ–‡ä»¶ååŒ…å«æ—¶é—´æˆ³å’Œaccï¼‰
                    window_file = output_dir / f'best_epoch{window_idx * SAVE_WINDOW_SIZE + 1}to{(window_idx + 1) * SAVE_WINDOW_SIZE}_acc{window_best_acc:.4f}_{timestamp}.pt'
                    # ğŸ”¥ ä¿®å¤ï¼šå¼ºåˆ¶åˆ›å»ºç›®å½•ï¼ˆé˜²æ­¢ä¿å­˜å¤±è´¥ï¼‰
                    window_file.parent.mkdir(parents=True, exist_ok=True)
                    torch.save(checkpoint, window_file)
                    logger.log(f"  ğŸ’¾ çª—å£({window_idx * 5 + 1}-{window_idx * 5 + 5})æœ€ä½³æ›´æ–°: {window_file.name} (Acc: {window_best_acc:.4f})", to_terminal=False)

                # å¦‚æœå½“å‰æ˜¯ä¸€ä¸ªçª—å£çš„ç»“æŸ (ä¾‹å¦‚ç¬¬ 5, 10, 15... ä¸ª epoch)
                # é‡ç½®çª—å£æœ€ä½³ Accï¼Œä¸ºä¸‹ä¸€ä¸ªçª—å£åšå‡†å¤‡
                if (epoch + 1) % SAVE_WINDOW_SIZE == 0:
                    window_best_acc = 0.0  # é‡ç½®



                # Early Stopping
                if patience_counter >= patience:
                    logger.log(f"\næ—©åœè§¦å‘: éªŒè¯æŸå¤±è¿ç»­ {patience} ä¸ª epoch æœªä¸‹é™", to_terminal=True)
                    logger.log(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}", to_terminal=True)
                    logger.log(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}", to_terminal=True)
                    break
            
            # è®­ç»ƒç»“æŸï¼Œå…³é—­logger
            logger.close()

    except RuntimeError as e:
        if "CUDA" in str(e) or "cuda" in str(e).lower():
            print("\n" + "="*60)
            print("CUDA é”™è¯¯: GPU è®¡ç®—ä¸å…¼å®¹")
            print("="*60)
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. ä½¿ç”¨ CPU è®­ç»ƒï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰:")
            print("   python train.py --device cpu --data_dir outputs --output_dir checkpoints --epochs 50")
            print("\n2. æ£€æŸ¥ CUDA å…¼å®¹æ€§:")
            print("   - è¿è¡Œ: nvidia-smi æŸ¥çœ‹ GPU ä¿¡æ¯")
            print("   - æ£€æŸ¥ PyTorch CUDA ç‰ˆæœ¬: python -c \"import torch; print(torch.version.cuda)\"")
            print("   - æ£€æŸ¥ç³»ç»Ÿ CUDA ç‰ˆæœ¬: nvcc --version")
            print("\n3. é‡æ–°å®‰è£…å…¼å®¹çš„ PyTorch:")
            print("   è®¿é—® https://pytorch.org/get-started/locally/ è·å–æ­£ç¡®çš„å®‰è£…å‘½ä»¤")
            print("="*60)
        else:
            raise
    else:
        print("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()

